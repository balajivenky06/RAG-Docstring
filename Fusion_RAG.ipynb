{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ab57c53-3661-49d6-bf78-3068936598a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Fusion RAG Implementation for Python Docstring Generation\n",
    "# -----------------------------------------------------------------------------\n",
    "# This script implements \"Fusion RAG\" using Reciprocal Rank Fusion (RRF).\n",
    "# It works by:\n",
    "# 1. Running two parallel searches for a given code snippet:\n",
    "#    a. Semantic Search: Using a \"Code-Aware\" query with Pinecone.\n",
    "#    b. Keyword Search: Using a simple keyword query with an in-memory\n",
    "#       BM25 index.\n",
    "# 2. Fusing the ranked results from both searches using the RRF algorithm\n",
    "#    to create a single, superior ranked list.\n",
    "# 3. Selecting the top-ranked document from the fused list as context.\n",
    "# 4. Generating a docstring using a two-stage LLM process with this best context.\n",
    "#\n",
    "# Prerequisites:\n",
    "# - Python 3.7+\n",
    "# - Pinecone account (API Key and Environment)\n",
    "# - Ollama installed and running locally\n",
    "# - Ollama models pulled:\n",
    "#   - ollama pull qwen2.5-coder:0.5b\n",
    "#   - ollama pull qwen2.5-coder:1.5b\n",
    "# - Python libraries installed:\n",
    "#   - pip install pinecone-client sentence-transformers requests beautifulsoup4 ollama rank-bm25\n",
    "# -----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b19f1fd6-5fe0-47bb-9c55-6e3e7de76b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pinecone import Pinecone, ServerlessSpec, PodSpec\n",
    "import ollama\n",
    "import os\n",
    "import uuid\n",
    "import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import re\n",
    "import pandas as pd\n",
    "from rouge import Rouge\n",
    "import pandas as pd\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import nltk\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import openpyxl\n",
    "from bert_score import score\n",
    "import itertools\n",
    "import hf_xet\n",
    "import zlib\n",
    "import subprocess\n",
    "import tempfile\n",
    "import traceback\n",
    "from rank_bm25 import BM25Okapi\n",
    "BM25_ENABLED = True\n",
    "BM25_CORPUS_URLS = [\n",
    "    \"https://peps.python.org/pep-0257/\",\n",
    "    \"https://google.github.io/styleguide/pyguide.html\",\n",
    "    \"https://www.programiz.com/python-programming/docstrings\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7175889-71d9-4e6f-aee6-900b49f8071e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_API_KEY = \"pcsk_71bnuL_HGU1YACobTvL5gJNzHsZG1NMNx3RGmz1ohyC7xMiUYoWnuZpEn5SuvWpuTxnuzm\"\n",
    "PINECONE_ENVIRONMENT = \"us-east-1\"\n",
    "\n",
    "# --- Constants ---\n",
    "INDEX_NAME = \"fusion-rag-docstring\"\n",
    "EMBEDDING_MODEL = 'all-MiniLM-L6-v2' # HuggingFace sentence transformer\n",
    "OLLAMA_GENERATOR_MODEL = 'deepseek-coder:6.7b' # Local Ollama model name (Ensure this is pulled: `ollama pull qwen2.5-coder:1.5b`)\n",
    "OLLAMA_HELPER_MODEL = 'deepseek-r1:1.5b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a418fd1c-d9ba-4ad0-bae2-f40a3fe46e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing services...\n",
      "Embedding model loaded.\n",
      "Pinecone initialized.\n",
      "Ollama client initialized. Attempting to use model: deepseek-coder:6.7b\n",
      "Ensure 'deepseek-coder:6.7b' is available locally in Ollama (`ollama pull deepseek-coder:6.7b`).\n"
     ]
    }
   ],
   "source": [
    "TARGET_URL = [\n",
    "    \"https://peps.python.org/pep-0257/\",\n",
    "    \"https://www.kaggle.com/code/hagzilla/what-are-docstrings\",\n",
    "    \"https://github.com/keleshev/pep257/blob/master/pep257.py\",\n",
    "    \"https://github.com/chadrik/doc484\",\n",
    "    \"https://zerotomastery.io/blog/python-docstring/\",\n",
    "    \"https://google.github.io/styleguide/pyguide.html\",\n",
    "    \"https://www.geeksforgeeks.org/python-docstrings/\",\n",
    "    \"https://pandas.pydata.org/docs/development/contributing_docstring.html\",\n",
    "    \"https://www.coding-guidelines.lftechnology.com/docs/python/docstrings/\",\n",
    "    \"https://realpython.com/python-pep8/\",\n",
    "    \"https://pypi.org/project/AIDocStringGenerator/\",\n",
    "    \"https://www.geeksforgeeks.org/pep-8-coding-style-guide-python/\",\n",
    "    \"https://llego.dev/posts/write-python-docstrings-guide-documenting-functions/\",\n",
    "    \"https://www.datacamp.com/tutorial/pep8-tutorial-python-code\",\n",
    "    \"https://www.programiz.com/python-programming/docstrings\",\n",
    "    \"https://marketplace.visualstudio.com/items?itemName=ShanthoshS.docstring-generator-ext\",\n",
    "    \"https://stackoverflow.com/questions/3898572/what-are-the-most-common-python-docstring-formats\",\n",
    "    \"https://stackoverflow.com/questions/78753860/what-is-the-proper-way-of-including-examples-in-python-docstrings\",\n",
    "    \"https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html\",\n",
    "    \"https://www.dataquest.io/blog/documenting-in-python-with-docstrings/\",\n",
    "    \"https://www.tutorialspoint.com/python/python_docstrings.htm\"\n",
    "]\n",
    "VECTOR_DIMENSION = 384 # Dimension for all-MiniLM-L6-v2\n",
    "METRIC = \"cosine\"\n",
    "CLOUD = \"aws\"\n",
    "REGION = \"us-east-1\"\n",
    "\n",
    "print(\"Initializing services...\")\n",
    "try:\n",
    "    model = SentenceTransformer(EMBEDDING_MODEL)\n",
    "    print(\"Embedding model loaded.\")\n",
    "\n",
    "    # Pinecone\n",
    "    pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "    print(f\"Pinecone initialized.\") # Environment info is handled internally\n",
    "\n",
    "    # Ollama Client\n",
    "    ollama_client = ollama.Client()\n",
    "    print(f\"Ollama client initialized. Attempting to use model: {OLLAMA_GENERATOR_MODEL}\")\n",
    "    print(f\"Ensure '{OLLAMA_GENERATOR_MODEL}' is available locally in Ollama (`ollama pull {OLLAMA_GENERATOR_MODEL}`).\")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing services: {e}\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0839df2e-2db9-4d34-8dcd-29e74364c3a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Pinecone indexes: ['fusion-rag-docstring', 'self-rag-docstring', 'rag-docstring', 'corrective-rag-docstring', 'code-aware-rag-docstring']\n",
      "Connecting to existing index 'fusion-rag-docstring'.\n",
      "Successfully connected to index 'fusion-rag-docstring'. Stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'metric': 'cosine',\n",
      " 'namespaces': {'': {'vector_count': 14}},\n",
      " 'total_vector_count': 14,\n",
      " 'vector_type': 'dense'}\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Initialize Pinecone ---\n",
    "pinecone_index = None\n",
    "if not PINECONE_API_KEY:\n",
    "    print(\"ERROR: Pinecone API key not found in environment variables.\")\n",
    "    exit(1)\n",
    "try:\n",
    "    pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "    existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]\n",
    "    print(f\"Available Pinecone indexes: {existing_indexes}\")\n",
    "\n",
    "    if INDEX_NAME not in existing_indexes:\n",
    "        print(f\"Index '{INDEX_NAME}' not found. Creating new index...\")\n",
    "        pc.create_index(\n",
    "            name=INDEX_NAME, dimension=VECTOR_DIMENSION, metric=METRIC,\n",
    "            spec=ServerlessSpec(cloud=CLOUD, region=REGION)\n",
    "        )\n",
    "        while not pc.describe_index(INDEX_NAME).status[\"ready\"]:\n",
    "            print(f\"Waiting for index '{INDEX_NAME}' to become ready...\")\n",
    "            time.sleep(5)\n",
    "        print(f\"Index '{INDEX_NAME}' created and ready.\")\n",
    "    else:\n",
    "        print(f\"Connecting to existing index '{INDEX_NAME}'.\")\n",
    "        # Optional: Clear index if you want to re-index fresh\n",
    "        # print(f\"WARNING: Deleting all vectors from existing index '{INDEX_NAME}'...\")\n",
    "        # index_to_clear = pc.Index(INDEX_NAME)\n",
    "        # index_to_clear.delete(delete_all=True)\n",
    "        # print(f\"All vectors deleted from '{INDEX_NAME}'.\")\n",
    "\n",
    "    pinecone_index = pc.Index(INDEX_NAME)\n",
    "    print(f\"Successfully connected to index '{INDEX_NAME}'. Stats: {pinecone_index.describe_index_stats()}\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Failed to initialize or connect to Pinecone index '{INDEX_NAME}': {e}\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c59e8199-7b1b-4add-b843-18940cc99cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Index already contains 14 vectors. Skipping data loading.\n"
     ]
    }
   ],
   "source": [
    "# --- Load Data into Pinecone (Only if index is empty) ---\n",
    "index_stats = pinecone_index.describe_index_stats()\n",
    "if index_stats.total_vector_count == 0:\n",
    "    total_docs_loaded = 0\n",
    "    # Loop through each URL in the list\n",
    "    for url in TARGET_URL:\n",
    "        print(f\"\\nProcessing URL: {url}\")\n",
    "        try:\n",
    "            # Fetch URL content\n",
    "            response = requests.get(url, timeout=30) # Use timeout\n",
    "            response.raise_for_status() # Check for HTTP errors\n",
    "\n",
    "            # Parse HTML\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            main_content = soup.find('main') or soup.find('article') or soup.find('body')\n",
    "            page_text = \"\"\n",
    "            if main_content:\n",
    "                page_text = main_content.get_text(separator='\\n', strip=True)\n",
    "            else:\n",
    "                page_text = soup.get_text(separator='\\n', strip=True) # Fallback\n",
    "\n",
    "            if not page_text or len(page_text) < 50: # Basic check for meaningful content\n",
    "                print(f\" -> Warning: Could not extract sufficient text content from {url}. Skipping.\")\n",
    "                continue # Skip to the next URL\n",
    "\n",
    "            print(f\" -> Extracted text length: {len(page_text)} characters.\")\n",
    "\n",
    "            # Generate embedding\n",
    "            # Note: Encoding large pages as a single vector might lose detail.\n",
    "            # Chunking the text into smaller parts is better for real applications.\n",
    "            embedding = model.encode(page_text).tolist()\n",
    "\n",
    "            # Prepare and upsert data\n",
    "            doc_id = str(uuid.uuid4())\n",
    "            metadata = {\"text\": page_text, \"source\": url} # Store the specific URL as source\n",
    "\n",
    "            pinecone_index.upsert(vectors=[(doc_id, embedding, metadata)])\n",
    "            print(f\" -> Data from {url} loaded into Pinecone with ID: {doc_id}\")\n",
    "            total_docs_loaded += 1\n",
    "            time.sleep(0.5) # Small delay to be polite to the server\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            # Handle errors fetching specific URL, continue with the next\n",
    "            print(f\" -> Error fetching URL {url}: {e}\")\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            # Handle other errors during processing/upserting for this URL\n",
    "            print(f\" -> Error processing or upserting data for {url}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if total_docs_loaded > 0:\n",
    "        print(\"Waiting a moment for indexing...\")\n",
    "        time.sleep(2)\n",
    "        print(pinecone_index.describe_index_stats()) # Show final stats\n",
    "    else:\n",
    "        print(\"Warning: No documents were loaded into the index.\")\n",
    "\n",
    "else:\n",
    "    print(f\"\\nIndex already contains {index_stats.total_vector_count} vectors. Skipping data loading.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4eee31b6-412d-49e4-9101-24cebd058b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Get User Input ---\n",
    "user_code = (\"\"\"\n",
    "class Conv(Layer):\n",
    "    def __init__(self, rank, filters, kernel_size, strides=1, padding='valid', data_format=None, dilation_rate=1, groups=1, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, trainable=True, name=None, conv_op=None, **kwargs):\n",
    "        super(Conv, self).__init__(trainable=trainable, name=name, activity_regularizer=regularizers.get(activity_regularizer), **kwargs)\n",
    "        self.rank = rank\n",
    "        if isinstance(filters, float):\n",
    "            filters = int(filters)\n",
    "        if filters is not None and filters < 0:\n",
    "            raise ValueError(f'Received a negative value for `filters`.Was expecting a positive value, got {filters}.')\n",
    "        self.filters = filters\n",
    "        self.groups = groups or 1\n",
    "        self.kernel_size = conv_utils.normalize_tuple(kernel_size, rank, 'kernel_size')\n",
    "        self.strides = conv_utils.normalize_tuple(strides, rank, 'strides')\n",
    "        self.padding = conv_utils.normalize_padding(padding)\n",
    "        self.data_format = conv_utils.normalize_data_format(data_format)\n",
    "        self.dilation_rate = conv_utils.normalize_tuple(dilation_rate, rank, 'dilation_rate')\n",
    "        self.activation = activations.get(activation)\n",
    "        self.use_bias = use_bias\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
    "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
    "        self.bias_constraint = constraints.get(bias_constraint)\n",
    "        self.input_spec = InputSpec(min_ndim=self.rank + 2)\n",
    "        self._validate_init()\n",
    "        self._is_causal = self.padding == 'causal'\n",
    "        self._channels_first = self.data_format == 'channels_first'\n",
    "        self._tf_data_format = conv_utils.convert_data_format(self.data_format, self.rank + 2)\n",
    "\n",
    "    def _validate_init(self):\n",
    "        if self.filters is not None and self.filters % self.groups != 0:\n",
    "            raise ValueError('The number of filters must be evenly divisible by the number of groups. Received: groups={}, filters={}'.format(self.groups, self.filters))\n",
    "        if not all(self.kernel_size):\n",
    "            raise ValueError('The argument `kernel_size` cannot contain 0(s). Received: %s' % (self.kernel_size,))\n",
    "        if not all(self.strides):\n",
    "            raise ValueError('The argument `strides` cannot contains 0(s). Received: %s' % (self.strides,))\n",
    "        if self.padding == 'causal' and (not isinstance(self, (Conv1D, SeparableConv1D))):\n",
    "            raise ValueError('Causal padding is only supported for `Conv1D`and `SeparableConv1D`.')\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        input_shape = tensor_shape.TensorShape(input_shape)\n",
    "        input_channel = self._get_input_channel(input_shape)\n",
    "        if input_channel % self.groups != 0:\n",
    "            raise ValueError('The number of input channels must be evenly divisible by the number of groups. Received groups={}, but the input has {} channels (full input shape is {}).'.format(self.groups, input_channel, input_shape))\n",
    "        kernel_shape = self.kernel_size + (input_channel // self.groups, self.filters)\n",
    "        self.kernel = self.add_weight(name='kernel', shape=kernel_shape, initializer=self.kernel_initializer, regularizer=self.kernel_regularizer, constraint=self.kernel_constraint, trainable=True, dtype=self.dtype)\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(name='bias', shape=(self.filters,), initializer=self.bias_initializer, regularizer=self.bias_regularizer, constraint=self.bias_constraint, trainable=True, dtype=self.dtype)\n",
    "        else:\n",
    "            self.bias = None\n",
    "        channel_axis = self._get_channel_axis()\n",
    "        self.input_spec = InputSpec(min_ndim=self.rank + 2, axes={channel_axis: input_channel})\n",
    "        if self.padding == 'causal':\n",
    "            tf_padding = 'VALID'\n",
    "        elif isinstance(self.padding, str):\n",
    "            tf_padding = self.padding.upper()\n",
    "        else:\n",
    "            tf_padding = self.padding\n",
    "        tf_dilations = list(self.dilation_rate)\n",
    "        tf_strides = list(self.strides)\n",
    "        tf_op_name = self.__class__.__name__\n",
    "        if tf_op_name == 'Conv1D':\n",
    "            tf_op_name = 'conv1d'\n",
    "        self._convolution_op = functools.partial(nn_ops.convolution_v2, strides=tf_strides, padding=tf_padding, dilations=tf_dilations, data_format=self._tf_data_format, name=tf_op_name)\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_shape = inputs.shape\n",
    "        if self._is_causal:\n",
    "            inputs = array_ops.pad(inputs, self._compute_causal_padding(inputs))\n",
    "        outputs = self._convolution_op(inputs, self.kernel)\n",
    "        if self.use_bias:\n",
    "            output_rank = outputs.shape.rank\n",
    "            if self.rank == 1 and self._channels_first:\n",
    "                bias = array_ops.reshape(self.bias, (1, self.filters, 1))\n",
    "                outputs += bias\n",
    "            elif output_rank is not None and output_rank > 2 + self.rank:\n",
    "\n",
    "                def _apply_fn(o):\n",
    "                    return nn.bias_add(o, self.bias, data_format=self._tf_data_format)\n",
    "                outputs = conv_utils.squeeze_batch_dims(outputs, _apply_fn, inner_rank=self.rank + 1)\n",
    "            else:\n",
    "                outputs = nn.bias_add(outputs, self.bias, data_format=self._tf_data_format)\n",
    "        if not context.executing_eagerly():\n",
    "            out_shape = self.compute_output_shape(input_shape)\n",
    "            outputs.set_shape(out_shape)\n",
    "        if self.activation is not None:\n",
    "            return self.activation(outputs)\n",
    "        return outputs\n",
    "\n",
    "    def _spatial_output_shape(self, spatial_input_shape):\n",
    "        return [conv_utils.conv_output_length(length, self.kernel_size[i], padding=self.padding, stride=self.strides[i], dilation=self.dilation_rate[i]) for i, length in enumerate(spatial_input_shape)]\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        input_shape = tensor_shape.TensorShape(input_shape).as_list()\n",
    "        batch_rank = len(input_shape) - self.rank - 1\n",
    "        if self.data_format == 'channels_last':\n",
    "            return tensor_shape.TensorShape(input_shape[:batch_rank] + self._spatial_output_shape(input_shape[batch_rank:-1]) + [self.filters])\n",
    "        else:\n",
    "            return tensor_shape.TensorShape(input_shape[:batch_rank] + [self.filters] + self._spatial_output_shape(input_shape[batch_rank + 1:]))\n",
    "\n",
    "    def _recreate_conv_op(self, inputs):\n",
    "        return False\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'filters': self.filters, 'kernel_size': self.kernel_size, 'strides': self.strides, 'padding': self.padding, 'data_format': self.data_format, 'dilation_rate': self.dilation_rate, 'groups': self.groups, 'activation': activations.serialize(self.activation), 'use_bias': self.use_bias, 'kernel_initializer': initializers.serialize(self.kernel_initializer), 'bias_initializer': initializers.serialize(self.bias_initializer), 'kernel_regularizer': regularizers.serialize(self.kernel_regularizer), 'bias_regularizer': regularizers.serialize(self.bias_regularizer), 'activity_regularizer': regularizers.serialize(self.activity_regularizer), 'kernel_constraint': constraints.serialize(self.kernel_constraint), 'bias_constraint': constraints.serialize(self.bias_constraint)}\n",
    "        base_config = super(Conv, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def _compute_causal_padding(self, inputs):\n",
    "        left_pad = self.dilation_rate[0] * (self.kernel_size[0] - 1)\n",
    "        if getattr(inputs.shape, 'ndims', None) is None:\n",
    "            batch_rank = 1\n",
    "        else:\n",
    "            batch_rank = len(inputs.shape) - 2\n",
    "        if self.data_format == 'channels_last':\n",
    "            causal_padding = [[0, 0]] * batch_rank + [[left_pad, 0], [0, 0]]\n",
    "        else:\n",
    "            causal_padding = [[0, 0]] * batch_rank + [[0, 0], [left_pad, 0]]\n",
    "        return causal_padding\n",
    "\n",
    "    def _get_channel_axis(self):\n",
    "        if self.data_format == 'channels_first':\n",
    "            return -1 - self.rank\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "    def _get_input_channel(self, input_shape):\n",
    "        channel_axis = self._get_channel_axis()\n",
    "        if input_shape.dims[channel_axis].value is None:\n",
    "            raise ValueError('The channel dimension of the inputs should be defined. Found `None`.')\n",
    "        return int(input_shape[channel_axis])\n",
    "\n",
    "    def _get_padding_op(self):\n",
    "        if self.padding == 'causal':\n",
    "            op_padding = 'valid'\n",
    "        else:\n",
    "            op_padding = self.padding\n",
    "        if not isinstance(op_padding, (list, tuple)):\n",
    "            op_padding = op_padding.upper()\n",
    "        return op_padding\n",
    "\"\"\")\n",
    "if not user_code.strip():\n",
    "    print(\"No code provided. Exiting.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d084de0a-0db2-4929-83c9-f1dfaf693e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bm25_searcher(urls):\n",
    "    \"\"\"Fetches content from URLs and builds an in-memory BM25 search index.\"\"\"\n",
    "    if not BM25_ENABLED: return None, {}\n",
    "    \n",
    "    #print(\"\\nBuilding in-memory BM25 index for keyword search...\")\n",
    "    corpus = []\n",
    "    doc_map = {} # Maps index to URL source\n",
    "    for i, url in enumerate(urls):\n",
    "        try:\n",
    "            r = requests.get(url, timeout=15, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "            r.raise_for_status()\n",
    "            soup = BeautifulSoup(r.content, 'html.parser')\n",
    "            text = (soup.find('main') or soup.find('article') or soup.body).get_text(strip=True)\n",
    "            if text:\n",
    "                corpus.append(text)\n",
    "                doc_map[i] = {\"text\": text, \"source\": url}\n",
    "        except Exception as e:\n",
    "            print(f\"  -> Failed to fetch or parse {url} for BM25 corpus: {e}\")\n",
    "            \n",
    "    if not corpus:\n",
    "        print(\"  -> Could not build BM25 corpus.\")\n",
    "        return None, {}\n",
    "        \n",
    "    tokenized_corpus = [doc.lower().split(\" \") for doc in corpus]\n",
    "    bm25 = BM25Okapi(tokenized_corpus)\n",
    "    #print(f\"BM25 index built with {len(corpus)} documents.\")\n",
    "    return bm25, doc_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4489af28-6b7a-40ee-8e5a-8522afc0f0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reciprocal_rank_fusion(ranked_lists, k=60):\n",
    "    \"\"\"\n",
    "    Performs Reciprocal Rank Fusion on multiple ranked lists of document IDs.\n",
    "    ranked_lists: A list where each element is another list of document IDs in ranked order.\n",
    "    \"\"\"\n",
    "    fused_scores = {}\n",
    "    \n",
    "    for rank_list in ranked_lists:\n",
    "        for i, doc_id in enumerate(rank_list):\n",
    "            if doc_id not in fused_scores:\n",
    "                fused_scores[doc_id] = 0\n",
    "            # Add reciprocal rank score\n",
    "            fused_scores[doc_id] += 1 / (k + i + 1) # i is 0-indexed, so rank is i+1\n",
    "            \n",
    "    # Sort documents by their fused score in descending order\n",
    "    sorted_fused_list = sorted(fused_scores.items(), key=lambda item: item[1], reverse=True)\n",
    "    return sorted_fused_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bfbf5f94-5783-4cf2-906a-eaa49c3cc751",
   "metadata": {},
   "outputs": [],
   "source": [
    "def revised_prompt(user_code, helper_model_name):\n",
    "    #context = ctx\n",
    "    OLLAMA_REWRITER_MODEL = helper_model_name\n",
    "    rewritten_request = None\n",
    "    rewriter_prompt = f\"\"\"\n",
    "    You are an helpful assistant that refines prompts. Given the following python code: {user_code} generate an optimized prompt for another AI whose sole task is to create a Python docstring for the code and your output.\n",
    "    The optimized prompt should clearly state the task, and subtly incorporate hints from the context if relevant, without necessarily repeating the entire context.\n",
    "    Focus on creating a self-contained, clear instruction for the next AI.\n",
    "    \n",
    "    Generate only the optimized context prompt text for the docstring generation AI.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        rewriter_response = ollama_client.generate(\n",
    "            model=OLLAMA_REWRITER_MODEL,\n",
    "            prompt=rewriter_prompt,\n",
    "            #options={'temperature': 0.3} # Lower temperature for more focused rewriting\n",
    "        )\n",
    "        rewritten_request = rewriter_response.get('response', '').strip()\n",
    "    except Exception as e:\n",
    "        rewritten_request = rewriter_prompt # Ensure it's None on error\n",
    "    return rewritten_request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5829af99-652d-4a6d-b845-ec51ba2f96ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_docstring(user_code, ollama_cli, helper_model, generator_model):\n",
    "    \"\"\"Generates a docstring using a two-stage (rewrite->generate) LLM process.\"\"\"\n",
    "    rewritten_request = None\n",
    "    raw_docstring = f\"# ERROR: Generation failed unexpectedly.\"\n",
    "    \n",
    "    rewritten_request = revised_prompt(user_code, helper_model)\n",
    "    try:\n",
    "        res_gen = ollama_cli.generate(model=generator_model, prompt=rewritten_request)\n",
    "        raw_docstring = res_gen.get('response', '').strip()\n",
    "    except Exception as e_gen: print(f\"  LLM Generator Error: {e_gen}\"); raw_docstring = f\"# ERROR: LLM Generation failed: {e_gen}\"\n",
    "  \n",
    "    return raw_docstring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2f79150b-d1ca-427f-a327-3391c7c38618",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_content_generation(context, user_code, rewritten_req, ollama_client, OLLAMA_MODEL):\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': 'You are an expert Python programmer tasked with generating docstrings. You will receive context (if found), and the code to document in the final message. Use the context only if it is directly relevant to explaining the provided code. Return only the docstring and dont include the given python code in the output.'}\n",
    "    ]\n",
    "    \n",
    "    # Add the retrieved context as a separate user message, if it exists\n",
    "    if context:\n",
    "        # Include source information in the context message for clarity\n",
    "        messages.append({'role': 'user', 'content': f\"Here is potentially relevant context retrieved from python\\n{user_code}\\n and content :\\n---\\n{rewritten_req}\\n---\"})\n",
    "    else:\n",
    "        # Explicitly state if no context was found or provided\n",
    "        messages.append({'role': 'user', 'content': \"No specific context was retrieved or provided for this request.\"})\n",
    "    \n",
    "    # Add the final user message with the code and the explicit request\n",
    "    messages.append({'role': 'user', 'content': f\"Based on any relevant context provided earlier, generate the Python docstring for the following code:\\n```python\\n{user_code}\\n```\\n\\nOutput *only* the complete docstring content itself, starting with triple quotes. Dont include python codes in the output. You need to generate a single docstring as whole for given python class code.\"})\n",
    "    \n",
    "    try:\n",
    "        response = ollama_client.chat(\n",
    "            model=OLLAMA_MODEL,\n",
    "            messages=messages\n",
    "        )\n",
    "    \n",
    "        generated_docstring = response.get('message', {}).get('content', '').strip()\n",
    "        if generated_docstring.startswith(\"```python\"):\n",
    "            generated_docstring = generated_docstring[len(\"```python\"):].strip()\n",
    "        elif generated_docstring.startswith(\"```\"):\n",
    "             generated_docstring = generated_docstring[len(\"```\"):].strip()\n",
    "    \n",
    "        if generated_docstring.endswith(\"```\"):\n",
    "            generated_docstring = generated_docstring[:-len(\"```\")].strip()\n",
    "    \n",
    "        # Ensure it starts with triple quotes if possible, otherwise print as is\n",
    "        if not (generated_docstring.startswith('\"\"\"') or generated_docstring.startswith(\"'''\")):\n",
    "             print(\"(Model might not have generated a perfectly formatted docstring)\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error communicating with Ollama chat endpoint: {e}\")\n",
    "    #print(generated_docstring)\n",
    "    return generated_docstring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5618e142-8045-45c4-9de2-9b11b9e053b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_fusion_rag_pipeline(user_code, pinecone_index, bm25_searcher, bm25_doc_map, emb_model, ollama_cli, helper_model, generator_model):\n",
    "    \"\"\"Executes the Fusion RAG pipeline.\"\"\"\n",
    "    #print(\"--- Running Fusion RAG pipeline ---\")\n",
    "    entities = re.search(r\"class\\s+(\\w+)\", user_code)\n",
    "    class_name = entities.group(1) if entities else \"\"\n",
    "    methods = re.findall(r\"def\\s+(_?\\w+)\\s*\\(\", user_code)\n",
    "    public_methods = [m for m in methods if not m.startswith('__') or m == '__init__']\n",
    "    semantic_query = f\"python docstring conventions for class {class_name} with methods {', '.join(public_methods[:3])}\"\n",
    "    keyword_query = f\"{class_name} {' '.join(public_methods)}\"\n",
    "\n",
    "    pinecone_matches, pinecone_q = [], 0\n",
    "    if pinecone_index:\n",
    "        try:\n",
    "            embedding = emb_model.encode(semantic_query).tolist()\n",
    "            results = pinecone_index.query(vector=embedding, top_k=5, include_metadata=True)\n",
    "            pinecone_matches = results.matches\n",
    "            pinecone_q = 1\n",
    "            #print(f\"    -> Pinecone returned {len(pinecone_matches)} candidates.\")\n",
    "        except Exception as e:\n",
    "            print(f\"    -> Pinecone retrieval error: {e}\")\n",
    "\n",
    "    bm25_docs = []\n",
    "    if bm25_searcher:\n",
    "        tokenized_query = keyword_query.lower().split()\n",
    "        doc_scores = bm25_searcher.get_scores(tokenized_query)\n",
    "        # Get top 5 document indices and scores\n",
    "        top_n_indices = sorted(range(len(doc_scores)), key=lambda i: doc_scores[i], reverse=True)[:5]\n",
    "        bm25_docs = [{\"id\": str(i), \"score\": doc_scores[i]} for i in top_n_indices if doc_scores[i] > 0]\n",
    "        \n",
    "    pinecone_ranked_ids = [match.id for match in pinecone_matches]\n",
    "    bm25_ranked_ids = [doc[\"id\"] for doc in bm25_docs]\n",
    "    \n",
    "    fused_results = reciprocal_rank_fusion([pinecone_ranked_ids, bm25_ranked_ids])\n",
    "    rewritten_request = revised_prompt(user_code, helper_model)\n",
    "    \n",
    "    if not fused_results:\n",
    "        raw_doc = final_content_generation('', user_code, rewritten_request, ollama_cli, generator_model)\n",
    "        return raw_doc\n",
    "    \n",
    "    # --- 4. Select Best Context ---\n",
    "    top_fused_id = fused_results[0][0]\n",
    "    #print(f\" -> Step 4: Selected best fused document (ID: {top_fused_id}).\")\n",
    "    \n",
    "    # Retrieve the content for the top fused ID\n",
    "    # In this demo, BM25 IDs are simple indices, Pinecone IDs are UUIDs.\n",
    "    # This requires a mapping or fetching from the source. We'll use the BM25 map as a proxy.\n",
    "    retrieved_ctx = \"\"\n",
    "    try:\n",
    "        if top_fused_id in [m.id for m in pinecone_matches]: # If it came from Pinecone\n",
    "            match = next((m for m in pinecone_matches if m.id == top_fused_id), None)\n",
    "            if match:\n",
    "                retrieved_ctx = match.metadata.get('text', '')\n",
    "                source_desc = match.metadata.get('source', 'Pinecone')\n",
    "        elif int(top_fused_id) in bm25_doc_map: # If it came from BM25\n",
    "             retrieved_ctx = bm25_doc_map[int(top_fused_id)][\"text\"]\n",
    "             source_desc = bm25_doc_map[int(top_fused_id)][\"source\"]\n",
    "    except (ValueError, KeyError):\n",
    "        print(\"   -> Could not map fused ID back to content. Proceeding without context.\")\n",
    "        retrieved_ctx = \"\"\n",
    "\n",
    "    # --- 5. Generate Final Docstring ---\n",
    "    retrieved_contexts_list.append(retrieved_ctx)\n",
    "    raw_doc = final_content_generation(fused_results, user_code, rewritten_request, ollama_cli, generator_model)\n",
    "    \n",
    "    print(\"--- Pipeline Finished ---\")\n",
    "    return raw_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "111c73a2-2a78-423b-aed5-d69589eea726",
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw_doc = generate_docstring(user_code,  ollama_client, OLLAMA_HELPER_MODEL, OLLAMA_GENERATOR_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e67273b5-8e45-4fe4-9b71-b887ec5674da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9f29ea62-77bb-4686-8d25-9b8f2a4a5755",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_searcher, bm25_doc_map = build_bm25_searcher(BM25_CORPUS_URLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c0da6ba3-a763-47b1-b255-710b2b0038eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_files_df = pd.read_pickle('class_files_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "23d1fb7c-4ea6-481a-b962-240465cc59a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = class_files_df[\"Comments\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "644c98e7-2a7e-4620-94c8-d64d9992ca57",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_docstrings_list = []\n",
    "retrieved_contexts_list = []\n",
    "rewritten_contexts_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "840ba2f6-c714-4168-8a3f-efc320e8b3a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Pipeline Finished ---\n",
      "--- Pipeline Finished ---\n",
      "--- Pipeline Finished ---\n",
      "--- Pipeline Finished ---\n",
      "--- Pipeline Finished ---\n",
      "(Model might not have generated a perfectly formatted docstring)\n",
      "--- Pipeline Finished ---\n",
      "--- Pipeline Finished ---\n",
      "--- Pipeline Finished ---\n",
      "--- Pipeline Finished ---\n",
      "--- Pipeline Finished ---\n",
      "--- Pipeline Finished ---\n",
      "--- Pipeline Finished ---\n",
      "--- Pipeline Finished ---\n",
      "--- Pipeline Finished ---\n",
      "--- Pipeline Finished ---\n",
      "--- Pipeline Finished ---\n",
      "--- Pipeline Finished ---\n",
      "--- Pipeline Finished ---\n",
      "--- Pipeline Finished ---\n",
      "(Model might not have generated a perfectly formatted docstring)\n",
      "--- Pipeline Finished ---\n",
      "--- Pipeline Finished ---\n",
      "--- Pipeline Finished ---\n",
      "--- Pipeline Finished ---\n",
      "--- Pipeline Finished ---\n",
      "--- Pipeline Finished ---\n",
      "--- Pipeline Finished ---\n",
      "--- Pipeline Finished ---\n",
      "--- Pipeline Finished ---\n",
      "--- Pipeline Finished ---\n",
      "--- Pipeline Finished ---\n",
      "--- Pipeline Finished ---\n",
      "--- Pipeline Finished ---\n",
      "--- Pipeline Finished ---\n",
      "--- Pipeline Finished ---\n",
      "--- Pipeline Finished ---\n",
      "--- Pipeline Finished ---\n",
      "--- Pipeline Finished ---\n",
      "--- Pipeline Finished ---\n",
      "--- Pipeline Finished ---\n",
      "--- Pipeline Finished ---\n",
      "--- Pipeline Finished ---\n",
      "--- Pipeline Finished ---\n",
      "--- Pipeline Finished ---\n",
      "--- Pipeline Finished ---\n",
      "--- Pipeline Finished ---\n",
      "--- Pipeline Finished ---\n",
      "--- Pipeline Finished ---\n",
      "--- Pipeline Finished ---\n",
      "--- Pipeline Finished ---\n",
      "--- Pipeline Finished ---\n",
      "--- Pipeline Finished ---\n",
      "    -> Pinecone retrieval error: Failed to connect; did you specify the correct index name?\n",
      "--- Pipeline Finished ---\n",
      "--- Pipeline Finished ---\n",
      "--- Pipeline Finished ---\n",
      "--- Pipeline Finished ---\n",
      "--- Pipeline Finished ---\n",
      "--- Pipeline Finished ---\n",
      "--- Pipeline Finished ---\n",
      "--- Pipeline Finished ---\n",
      "--- Pipeline Finished ---\n",
      "--- Pipeline Finished ---\n",
      "--- Pipeline Finished ---\n",
      "--- Pipeline Finished ---\n",
      "--- Pipeline Finished ---\n",
      "--- Pipeline Finished ---\n",
      "--- Pipeline Finished ---\n",
      "--- Pipeline Finished ---\n"
     ]
    }
   ],
   "source": [
    "for i, row in class_files_df.iterrows():\n",
    "    user_code = row[\"Code_without_comments\"]\n",
    "    output = run_fusion_rag_pipeline(\n",
    "        user_code=user_code,\n",
    "        pinecone_index=pinecone_index,\n",
    "        bm25_searcher=bm25_searcher,\n",
    "        bm25_doc_map=bm25_doc_map,\n",
    "        emb_model=model,\n",
    "        ollama_cli=ollama_client,\n",
    "        helper_model=OLLAMA_HELPER_MODEL,\n",
    "        generator_model=OLLAMA_GENERATOR_MODEL\n",
    "    )\n",
    "    generated_docstrings_list.append(output)\n",
    "class_files_df[\"RAG_Docstring\"] = generated_docstrings_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "925c2636-ce75-4488-ae32-795a4bf3f27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_rag_docstring(docstring_text):\n",
    "    if not isinstance(docstring_text, str):\n",
    "        return docstring_text\n",
    "\n",
    "    if docstring_text.startswith(\"# ERROR:\") or docstring_text.startswith(\"# SKIPPED:\"):\n",
    "        return docstring_text\n",
    "\n",
    "    text = docstring_text.strip()\n",
    "\n",
    "    if text.startswith(\"```python\"):\n",
    "        text = text[len(\"```python\"):].strip()\n",
    "    elif text.startswith(\"```\"):\n",
    "        text = text[len(\"```\"):].strip()\n",
    "    if text.endswith(\"```\"):\n",
    "        text = text[:-len(\"```\")].strip()\n",
    "\n",
    "    content_inside_quotes = None\n",
    "    first_double_quotes = text.find('\"\"\"')\n",
    "    if first_double_quotes != -1:\n",
    "        last_double_quotes = text.rfind('\"\"\"')\n",
    "        if last_double_quotes > first_double_quotes and (last_double_quotes + 3) <= len(text):\n",
    "            content_inside_quotes = text[first_double_quotes + 3 : last_double_quotes].strip()\n",
    "\n",
    "    if content_inside_quotes is None or not content_inside_quotes.strip():\n",
    "        first_single_quotes = text.find(\"'''\")\n",
    "        if first_single_quotes != -1:\n",
    "            last_single_quotes = text.rfind(\"'''\")\n",
    "            if last_single_quotes > first_single_quotes and (last_single_quotes + 3) <= len(text):\n",
    "                content_inside_quotes = text[first_single_quotes + 3 : last_single_quotes].strip()\n",
    "    \n",
    "    if content_inside_quotes is not None and content_inside_quotes.strip():\n",
    "        final_text_to_clean = content_inside_quotes\n",
    "    else:\n",
    "        final_text_to_clean = text\n",
    "        if final_text_to_clean.startswith('\"\"\"') and final_text_to_clean.endswith('\"\"\"') and len(final_text_to_clean) >= 6:\n",
    "            final_text_to_clean = final_text_to_clean[3:-3].strip()\n",
    "        elif final_text_to_clean.startswith(\"'''\") and final_text_to_clean.endswith(\"'''\") and len(final_text_to_clean) >= 6:\n",
    "            final_text_to_clean = final_text_to_clean[3:-3].strip()\n",
    "\n",
    "    final_text_to_clean = re.sub(r\"(?i)^class\\s+\\w+:\\s*\\n?\", \"\", final_text_to_clean).strip()\n",
    "    \n",
    "    return final_text_to_clean\n",
    "\n",
    "class_files_df[\"RAG_Docstring\"] = class_files_df[\"RAG_Docstring\"].astype(str).apply(clean_rag_docstring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2ad84afc-40a9-4606-80b3-6ec7e5dfa8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rouge(df, reference_column, hypothesis_column):\n",
    "    rouge = Rouge()\n",
    "\n",
    "    def calculate_score(row):\n",
    "        scores = rouge.get_scores(row[hypothesis_column].lower(), row[reference_column].lower())\n",
    "        return scores[0]['rouge-1']['f']\n",
    "\n",
    "    df['ROUGE-1 ' + reference_column] = df.apply(calculate_score, axis=1)\n",
    "    return df\n",
    "\n",
    "# Calculate ROUGE-1 scores\n",
    "data_1 = calculate_rouge(class_files_df, 'Comments', 'RAG_Docstring')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2f1163b7-1a5f-4498-88f9-864555f1407b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bleu(df, reference_column, hypothesis_column):\n",
    "    nltk.download('punkt')\n",
    "\n",
    "    def calculate_score(row):\n",
    "        reference = [row[reference_column].lower().split()]\n",
    "        hypothesis = row[hypothesis_column].lower().split()\n",
    "        score = sentence_bleu(reference, hypothesis, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "        return score\n",
    "\n",
    "    df['BLEU Score ' + reference_column] = df.apply(calculate_score, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aaf38775-7ee8-473a-a07e-4ea181ebf911",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/balajivenktesh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Calculate BLEU scores\n",
    "data_1 = calculate_bleu(data_1, 'Comments', 'RAG_Docstring')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bc233525-8c33-46e2-9007-342bca36270b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate BERT encoding score, using cosine similarity\n",
    "def calculate_bert_score(ground_truth, generated):\n",
    "    # Calculate BERT score\n",
    "    _, _, bert_score_f1 = score([ground_truth], [generated], lang='en', model_type='bert-base-uncased')\n",
    "\n",
    "    return bert_score_f1.item()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e7d020e1-27d6-46e6-b597-b6b17c4c2302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate BLEU scores\n",
    "list_append_1 = []\n",
    "for index, row in data_1.iterrows():\n",
    "    list_append_1.append(calculate_bert_score(str(row[\"Comments\"]), str(row[\"RAG_Docstring\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1025429f-7981-4222-ae7a-d27cb543e7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1[\"Accuracy\"] = list_append_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3050ae3e-33cd-4395-bcfe-cc9ac2e415b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate number of syllables in docstring\n",
    "def count_syllables(word):\n",
    "    # Remove punctuation\n",
    "    word = re.sub(r'[^a-zA-Z]', '', word)\n",
    "    \n",
    "    # Vowel count\n",
    "    vowels = 'aeiouy'\n",
    "    syllables = 0\n",
    "    last_was_vowel = False\n",
    "    for char in word:\n",
    "        if char.lower() in vowels:\n",
    "            if not last_was_vowel:\n",
    "                syllables += 1\n",
    "            last_was_vowel = True\n",
    "        else:\n",
    "            last_was_vowel = False\n",
    "    \n",
    "    # Adjust syllable count for words ending in 'e'\n",
    "    if word.endswith(('e', 'es', 'ed')):\n",
    "        syllables -= 1\n",
    "    \n",
    "    # Adjust syllable count for words with no vowels\n",
    "    if syllables == 0:\n",
    "        syllables = 1\n",
    "    \n",
    "    return syllables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "814c0165-a9a0-4e8a-8e24-95f64055fc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Flesch reading score\n",
    "def flesch_reading_ease(text):\n",
    "    sentences = text.count('.') + text.count('!') + text.count('?') + 1\n",
    "    words = len(re.findall(r'\\b\\w+\\b', text))\n",
    "    syllables = sum(count_syllables(word) for word in text.split())\n",
    "    \n",
    "    # Calculate Flesch Reading Ease score\n",
    "    score = 206.835 - 1.015 * (words / sentences) - 84.6 * (syllables / words)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8b8b4cd0-f960-4e1e-9738-e63e03cde27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Easy scores\n",
    "list_append_2 = []\n",
    "for index, row in data_1.iterrows():\n",
    "    list_append_2.append(flesch_reading_ease(str(row[\"RAG_Docstring\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bd341024-f250-4c18-8016-f3f9523d8fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1[\"Ease\"] = list_append_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "af32ae61-39a1-4e0a-a5c5-30dd27982b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "def compress(input):\n",
    "\treturn zlib.compress(input.encode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ebcce976-99cc-47fb-9eca-4704c74698c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conciness(ground_truth, generated):\n",
    "    comp1 = compress(ground_truth)\n",
    "    comp2 = compress(generated)\n",
    "    return sys.getsizeof(comp2) / sys.getsizeof(comp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "124e78ed-8168-432d-87cd-5f156c8cbc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Conciseness scores\n",
    "list_append_3 = []\n",
    "for index, row in data_1.iterrows():\n",
    "    list_append_3.append(conciness(str(row[\"Comments\"]), str(row[\"RAG_Docstring\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "54c842e8-0389-4c2b-8beb-efcd2f7e628d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1[\"Conciseness\"] = list_append_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fc2fefd2-c2db-4a97-b345-32ae02740333",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_parameter_coverage(code_str, docstring_str):\n",
    "    \"\"\"\n",
    "    Calculates the proportion of function/method parameters mentioned in the docstring.\n",
    "    Returns a float (0.0 to 1.0) or None if no parameters are found in the code.\n",
    "    \"\"\"        \n",
    "    match = re.search(r\"def\\s+\\w+\\s*\\((.*?)\\):\", code_str)\n",
    "    if not match:\n",
    "        match = re.search(r\"async\\s+def\\s+\\w+\\s*\\((.*?)\\):\", code_str) \n",
    "\n",
    "    if not match:\n",
    "        return None \n",
    "\n",
    "    params_str = match.group(1)\n",
    "    if not params_str.strip(): \n",
    "        return 1.0 \n",
    "\n",
    "    potential_params = [p.strip().split('=')[0].split(':')[0].strip() for p in params_str.split(',')]\n",
    "    actual_params = [p for p in potential_params if p and p not in ('self', 'cls') and not p.startswith('*')]\n",
    "\n",
    "    if not actual_params:\n",
    "        return 1.0 \n",
    "\n",
    "    covered_params = 0\n",
    "    docstring_lower = docstring_str.lower()\n",
    "    for param_name in actual_params:\n",
    "        if re.search(r\"\\b\" + re.escape(param_name.lower()) + r\"\\b\", docstring_lower):\n",
    "            covered_params += 1\n",
    "        elif f\"{param_name.lower()}:\" in docstring_lower or f\"parameter {param_name.lower()}\" in docstring_lower:\n",
    "             covered_params += 1\n",
    "    return covered_params / len(actual_params) if actual_params else 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b4013306-ebc5-4edb-b3ec-56828ee2c035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Return Value Coverage Calculation Function ---\n",
    "def calculate_return_coverage(code_str, docstring_str):\n",
    "    \"\"\"\n",
    "    Checks if the docstring mentions a return value if the code seems to have one.\n",
    "    Returns 1 if covered/not applicable, 0 if potentially missing, None on error.\n",
    "    \"\"\"\n",
    "    has_return_statement = False\n",
    "    for line in code_str.splitlines():\n",
    "        stripped_line = line.strip()\n",
    "        if stripped_line.startswith(\"return \") and not stripped_line.endswith(\"return None\") and len(stripped_line) > len(\"return \"):\n",
    "            has_return_statement = True\n",
    "            break\n",
    "    \n",
    "    if not has_return_statement:\n",
    "        return 1.0 \n",
    "\n",
    "    docstring_lower = docstring_str.lower()\n",
    "    return_keywords = [\"return\", \"returns\", \"yield\", \"yields\"] \n",
    "    if any(keyword in docstring_lower for keyword in return_keywords):\n",
    "        return 1.0\n",
    "    else:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "87f09055-8d04-424b-88d5-3f00b5ee2b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Basic Faithfulness Metric Function ---\n",
    "def calculate_basic_faithfulness(generated_docstring, retrieved_context_text):\n",
    "    \"\"\"\n",
    "    Calculates a basic faithfulness score based on token overlap.\n",
    "    This is a crude proxy for actual faithfulness.\n",
    "    Returns a float (0.0 to 1.0) or None.\n",
    "    \"\"\"\n",
    "    # Simple tokenization and stopword removal\n",
    "    stop_words = set([\"a\", \"an\", \"the\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"do\", \"does\", \"did\", \"will\", \"would\", \"should\", \"can\", \"could\", \"may\", \"might\", \"must\", \"and\", \"or\", \"but\", \"if\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"to\", \"in\", \"on\", \"this\", \"that\", \"it\", \"its\", \"you\", \"your\", \"i\", \"me\", \"my\", \"he\", \"she\", \"him\", \"her\", \"they\", \"them\", \"their\"])\n",
    "    \n",
    "    try:\n",
    "        gen_tokens = set(token.lower() for token in re.findall(r'\\b\\w+\\b', generated_docstring) if token.lower() not in stop_words)\n",
    "        ctx_tokens = set(token.lower() for token in re.findall(r'\\b\\w+\\b', retrieved_context_text) if token.lower() not in stop_words)\n",
    "    except Exception as e:\n",
    "        print(f\"Error tokenizing for faithfulness: {e}\")\n",
    "        return None\n",
    "\n",
    "    if not gen_tokens: # If generated docstring has no valid tokens after filtering\n",
    "        return 0.0 \n",
    "\n",
    "    overlapping_tokens = gen_tokens.intersection(ctx_tokens)\n",
    "    \n",
    "    return len(overlapping_tokens) / len(gen_tokens) if gen_tokens else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5f4cf016-2830-4a74-b75f-d85ed5c2b637",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_exception_coverage(code_str, docstring_str):\n",
    "    if not all(isinstance(s, str) for s in [code_str, docstring_str]) or not docstring_str.strip() or docstring_str.startswith((\"# ERROR:\", \"# SKIPPED:\")): return None\n",
    "    raised_exceptions = set(re.findall(r\"raise\\s+(\\w+)\", code_str)) # Basic: finds exception names\n",
    "    if not raised_exceptions: return 1.0 # No exceptions to cover\n",
    "    \n",
    "    docstring_lower = docstring_str.lower()\n",
    "    mentions_raises_section = \"raises:\" in docstring_lower\n",
    "    covered_exceptions = 0\n",
    "    for exc_name in raised_exceptions:\n",
    "        if re.search(r\"\\b\" + re.escape(exc_name.lower()) + r\"\\b\", docstring_lower):\n",
    "            covered_exceptions += 1\n",
    "            \n",
    "    # If a \"Raises:\" section exists, it's good, even if not all specific exceptions are named (simple check)\n",
    "    if mentions_raises_section and raised_exceptions: return 1.0 \n",
    "    if not raised_exceptions: return 1.0 # Should have been caught above\n",
    "    return covered_exceptions / len(raised_exceptions) if raised_exceptions else 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "92619449-aabb-46e2-957d-e005d0e77bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Adherence to Docstring Conventions (Pydocstyle) ---\n",
    "PYDOCSTYLE_ENABLED = True\n",
    "def check_docstring_adherence_pydocstyle(code_str, generated_docstring_content):\n",
    "    \"\"\"\n",
    "    Checks adherence of a generated docstring to PEP 257 using pydocstyle.\n",
    "    The generated_docstring_content should be the *content* of the docstring,\n",
    "    not including the triple quotes.\n",
    "    Returns:\n",
    "        float: A score from 0.0 to 1.0 (1.0 means no errors, 0.0 means many errors).\n",
    "               Returns None if pydocstyle is not enabled or an error occurs.\n",
    "    \"\"\"\n",
    "    # Sanitize content for embedding within triple quotes\n",
    "    safe_content = generated_docstring_content.replace('\\\\', '\\\\\\\\') # Escape backslashes\n",
    "    safe_content = safe_content.replace('\"\"\"', '\\\\\"\\\\\"\\\\\"') # Escape internal triple-double-quotes\n",
    "    safe_content = safe_content.replace(\"'''\", \"\\\\'\\\\'\\\\'\") # Escape internal triple-single-quotes\n",
    "    \n",
    "    # Prepare the content for insertion, ensuring correct indentation for multi-line docstrings\n",
    "    lines = safe_content.split('\\n')\n",
    "    if len(lines) == 1:\n",
    "        # Single line docstring content, no special indentation needed beyond the initial one\n",
    "        indented_docstring_body = lines[0]\n",
    "    else:\n",
    "        # Multi-line: first line as is, subsequent lines indented with 4 spaces\n",
    "        # This assumes the docstring will be placed with an initial 4-space indent.\n",
    "        indented_docstring_body = lines[0] + '\\n' + '\\n'.join(['    ' + line for line in lines[1:]])\n",
    "\n",
    "\n",
    "    # Construct a minimal, valid Python snippet for pydocstyle\n",
    "    # Try to place the docstring correctly within a class or function if identifiable\n",
    "    code_prefix = \"\"\n",
    "    code_suffix = \"\\n    pass\" # Default suffix\n",
    "\n",
    "    class_match = re.search(r\"^(.*\\bclass\\s+\\w+\\s*\\(?.*\\)?:)\", code_str, re.MULTILINE)\n",
    "    func_match = re.search(r\"^(.*\\b(async\\s+)?def\\s+\\w+\\s*\\(?.*\\)?:)\", code_str, re.MULTILINE)\n",
    "\n",
    "    if class_match:\n",
    "        header = class_match.group(1)\n",
    "        # Find the end of the header line to insert the docstring\n",
    "        code_prefix = code_str[:class_match.end()] + f'\\n    \"\"\"{indented_docstring_body}\"\"\"'\n",
    "        code_suffix = code_str[class_match.end():] # The rest of the original class code\n",
    "        # Ensure there's at least a 'pass' or some body if the original was just a header\n",
    "        if not code_suffix.strip() or code_suffix.strip().startswith(\"#\"):\n",
    "            code_suffix = \"\\n    pass\" + code_suffix \n",
    "        code_for_pydocstyle_check = code_prefix + code_suffix\n",
    "\n",
    "    elif func_match:\n",
    "        header = func_match.group(1)\n",
    "        code_prefix = code_str[:func_match.end()] + f'\\n    \"\"\"{indented_docstring_body}\"\"\"'\n",
    "        code_suffix = code_str[func_match.end():]\n",
    "        if not code_suffix.strip() or code_suffix.strip().startswith(\"#\"):\n",
    "            code_suffix = \"\\n    pass\" + code_suffix\n",
    "        code_for_pydocstyle_check = code_prefix + code_suffix\n",
    "    else:\n",
    "        # Fallback: treat as module-level docstring if no class/def found\n",
    "        # This is less ideal as the original code_str might not be a full module\n",
    "        code_for_pydocstyle_check = f'\"\"\"{generated_docstring_content}\"\"\"\\n{code_str}'\n",
    "\n",
    "\n",
    "    errors_count = 0\n",
    "    filtered_errors_count = 0\n",
    "    tmp_file_path = None \n",
    "    try:\n",
    "        with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False, encoding='utf-8') as tmp_file:\n",
    "            tmp_file.write(code_for_pydocstyle_check)\n",
    "            tmp_file_path = tmp_file.name\n",
    "        \n",
    "        command = ['pydocstyle', tmp_file_path]\n",
    "        process = subprocess.run(command, capture_output=True, text=True, encoding='utf-8')\n",
    "        \n",
    "        output = process.stdout.strip()\n",
    "        print\n",
    "        if output:\n",
    "            all_errors = output.splitlines()\n",
    "            errors_count = len(all_errors)\n",
    "            # Filter out D100 (Missing docstring in public module) as it's an artifact\n",
    "            # and D101, D102, D103 if we are only checking the first docstring.\n",
    "            # For now, just D100 as the dummy structure is a module.\n",
    "            filtered_errors = [err for err in all_errors if not err.strip().endswith(\"D100: Missing docstring in public module\")]\n",
    "            filtered_errors_count = len(filtered_errors)\n",
    "        \n",
    "        if process.stderr:\n",
    "            if \"Cannot parse file\" in process.stderr or \"unexpected EOF while parsing\" in process.stderr or \"invalid syntax\" in process.stderr :\n",
    "                 print(f\"Pydocstyle CRITICAL PARSE ERROR for temp file {tmp_file_path}: {process.stderr}\")\n",
    "                 print(\"--- Content written to temp file that failed parsing: ---\")\n",
    "                 print(code_for_pydocstyle_check)\n",
    "                 print(\"--------------------------------------------------------\")\n",
    "                 return 0.0 # Penalize heavily for parse error\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An exception occurred during pydocstyle check: {str(e)}\")\n",
    "        if tmp_file_path and os.path.exists(tmp_file_path): # Check if tmp_file_path was assigned\n",
    "             try:\n",
    "                with open(tmp_file_path, 'r', encoding='utf-8') as f_err:\n",
    "                    print(f\"Content of temp file '{tmp_file_path}' that caused exception:\\n{f_err.read()}\")\n",
    "             except Exception as read_err:\n",
    "                print(f\"Could not read temp file {tmp_file_path}: {read_err}\")\n",
    "        return None # Error during check\n",
    "    finally:\n",
    "        if tmp_file_path and os.path.exists(tmp_file_path):\n",
    "            os.remove(tmp_file_path)\n",
    "    \n",
    "    # Normalize score based on filtered errors.\n",
    "    # Using 10 as the denominator makes the score less harsh than 5.\n",
    "    return max(0.0, 1.0 - (filtered_errors_count / 10.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9b91728e-606d-49b3-b48b-37b704058d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_coverage_list = []\n",
    "return_coverage_list = []\n",
    "faithfulness_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0b05eea9-2c34-43fe-86f6-a50aa85763cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in data_1.iterrows():\n",
    "    param_coverage_list.append(calculate_parameter_coverage(str(row[\"Code_without_comments\"]), str(row[\"RAG_Docstring\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "088cefc0-c7b3-40fc-89a2-b5cda93088cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1[\"Parameter_Coverage\"] = param_coverage_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f44b6e67-cb5a-4621-ab81-d2c0bad5d3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in data_1.iterrows():\n",
    "    return_coverage_list.append(calculate_return_coverage(str(row[\"Code_without_comments\"]), str(row[\"RAG_Docstring\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "47fdaffc-7b2c-4907-b2fc-1b28f1152bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1[\"Return_Coverage\"] = return_coverage_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "302a4755-237e-4792-98a4-27208871ca76",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1[\"Retrieved_Contexts\"] = retrieved_contexts_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4ba16445-3552-459b-b7b6-de2868c12453",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in data_1.iterrows():\n",
    "    faithfulness_list.append(calculate_basic_faithfulness(str(row[\"RAG_Docstring\"]), str(row[\"Retrieved_Contexts\"])))\n",
    "    #faithfulness_list.append(faithfulness_score)\n",
    "#if faithfulness_score is not None: print(f\"    -> Basic Faithfulness: {faithfulness_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "defab78f-f26d-4a87-816b-d4ce71f4aae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1[\"Faithfulness_Score\"] = faithfulness_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "62b2a3d6-5820-4d9f-a5da-03892fcf4bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pydocstyle_adherence_list_1 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c455c9f1-31c0-43f1-b1be-9265744d2522",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "for index, row in data_1.iterrows():\n",
    "    pydocstyle_adherence_list_1.append(check_docstring_adherence_pydocstyle(str(row[\"Code_without_comments\"]), str(row[\"RAG_Docstring\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2b93f321-ca0d-4924-9de2-813a4edd90b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1[\"PythonStyle_Adherence\"] = pydocstyle_adherence_list_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1387b0f6-056a-4354-a38f-c05c6b5c63f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "exception_coverage_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9eccc021-8301-4155-bae5-27a311c8f841",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in data_1.iterrows():\n",
    "    exception_coverage_list.append(calculate_exception_coverage(str(row[\"Code_without_comments\"]), str(row[\"RAG_Docstring\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7ecd033a-5c16-4ce0-a01f-ee4d44f1a6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1[\"Exception_Coverage\"] = exception_coverage_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "759c17d3-a22b-4aad-9d3c-1bcdc98f282c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1.to_excel('./deepseek/Fusion_RAG.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a48e8428-85f2-4a6f-acf8-917643174a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1.to_pickle('./deepseek/Fusion_RAG.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc9490e-0088-48c5-85da-b39cb34f9268",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phd_env",
   "language": "python",
   "name": "phd_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
