{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4133899a-6dff-4071-b6bf-a4c7aab8099d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing services...\n",
      "Embedding model loaded.\n",
      "Pinecone initialized.\n",
      "Ollama client initialized. Attempting to use model: deepseek-coder:6.7b\n",
      "Ensure 'deepseek-coder:6.7b' is available locally in Ollama (`ollama pull deepseek-coder:6.7b`).\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pinecone import Pinecone, ServerlessSpec, PodSpec\n",
    "import ollama\n",
    "import os\n",
    "import uuid\n",
    "import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import re\n",
    "import pandas as pd\n",
    "from rouge import Rouge\n",
    "import pandas as pd\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import nltk\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import openpyxl\n",
    "from bert_score import score\n",
    "import itertools\n",
    "import hf_xet\n",
    "import zlib\n",
    "import subprocess\n",
    "import tempfile\n",
    "import traceback\n",
    "from googlesearch import search as google_search_func # Renamed to avoid conflict\n",
    "GOOGLE_SEARCH_ENABLED = True\n",
    "\n",
    "PINECONE_API_KEY = \"pcsk_71bnuL_HGU1YACobTvL5gJNzHsZG1NMNx3RGmz1ohyC7xMiUYoWnuZpEn5SuvWpuTxnuzm\"\n",
    "PINECONE_ENVIRONMENT = \"us-east-1\"\n",
    "\n",
    "# --- Constants ---\n",
    "INDEX_NAME = \"self-rag-docstring\"\n",
    "EMBEDDING_MODEL = 'all-MiniLM-L6-v2' # HuggingFace sentence transformer\n",
    "OLLAMA_MODEL = 'deepseek-coder:6.7b' # Local Ollama model name (Ensure this is pulled: `ollama pull qwen2.5-coder:1.5b`)\n",
    "OLLAMA_REWRITER_MODEL = 'deepseek-r1:1.5b'\n",
    "TARGET_URL = [\n",
    "    \"https://peps.python.org/pep-0257/\",\n",
    "    \"https://www.kaggle.com/code/hagzilla/what-are-docstrings\",\n",
    "    \"https://github.com/keleshev/pep257/blob/master/pep257.py\",\n",
    "    \"https://github.com/chadrik/doc484\",\n",
    "    \"https://zerotomastery.io/blog/python-docstring/\",\n",
    "    \"https://google.github.io/styleguide/pyguide.html\",\n",
    "    \"https://www.geeksforgeeks.org/python-docstrings/\",\n",
    "    \"https://pandas.pydata.org/docs/development/contributing_docstring.html\",\n",
    "    \"https://www.coding-guidelines.lftechnology.com/docs/python/docstrings/\",\n",
    "    \"https://realpython.com/python-pep8/\",\n",
    "    \"https://pypi.org/project/AIDocStringGenerator/\",\n",
    "    \"https://www.geeksforgeeks.org/pep-8-coding-style-guide-python/\",\n",
    "    \"https://llego.dev/posts/write-python-docstrings-guide-documenting-functions/\",\n",
    "    \"https://www.datacamp.com/tutorial/pep8-tutorial-python-code\",\n",
    "    \"https://www.programiz.com/python-programming/docstrings\",\n",
    "    \"https://marketplace.visualstudio.com/items?itemName=ShanthoshS.docstring-generator-ext\",\n",
    "    \"https://stackoverflow.com/questions/3898572/what-are-the-most-common-python-docstring-formats\",\n",
    "    \"https://stackoverflow.com/questions/78753860/what-is-the-proper-way-of-including-examples-in-python-docstrings\",\n",
    "    \"https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html\",\n",
    "    \"https://www.dataquest.io/blog/documenting-in-python-with-docstrings/\",\n",
    "    \"https://www.tutorialspoint.com/python/python_docstrings.htm\"\n",
    "]\n",
    "VECTOR_DIMENSION = 384 # Dimension for all-MiniLM-L6-v2\n",
    "METRIC = \"cosine\"\n",
    "CLOUD = \"aws\"\n",
    "REGION = \"us-east-1\"\n",
    "\n",
    "print(\"Initializing services...\")\n",
    "try:\n",
    "    model = SentenceTransformer(EMBEDDING_MODEL)\n",
    "    print(\"Embedding model loaded.\")\n",
    "\n",
    "    # Pinecone\n",
    "    pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "    print(f\"Pinecone initialized.\") # Environment info is handled internally\n",
    "\n",
    "    # Ollama Client\n",
    "    ollama_client = ollama.Client()\n",
    "    print(f\"Ollama client initialized. Attempting to use model: {OLLAMA_MODEL}\")\n",
    "    print(f\"Ensure '{OLLAMA_MODEL}' is available locally in Ollama (`ollama pull {OLLAMA_MODEL}`).\")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing services: {e}\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f187a4a9-2143-49fd-b493-04652ac7bff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Pinecone indexes: ['fusion-rag-docstring', 'self-rag-docstring', 'rag-docstring', 'corrective-rag-docstring', 'code-aware-rag-docstring']\n",
      "Connecting to existing index 'self-rag-docstring'.\n",
      "Successfully connected to index 'self-rag-docstring'. Stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'metric': 'cosine',\n",
      " 'namespaces': {'': {'vector_count': 14}},\n",
      " 'total_vector_count': 14,\n",
      " 'vector_type': 'dense'}\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Initialize Pinecone ---\n",
    "pinecone_index = None\n",
    "if not PINECONE_API_KEY:\n",
    "    print(\"ERROR: Pinecone API key not found in environment variables.\")\n",
    "    exit(1)\n",
    "try:\n",
    "    pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "    existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]\n",
    "    print(f\"Available Pinecone indexes: {existing_indexes}\")\n",
    "\n",
    "    if INDEX_NAME not in existing_indexes:\n",
    "        print(f\"Index '{INDEX_NAME}' not found. Creating new index...\")\n",
    "        pc.create_index(\n",
    "            name=INDEX_NAME, dimension=VECTOR_DIMENSION, metric=METRIC,\n",
    "            spec=ServerlessSpec(cloud=CLOUD, region=REGION)\n",
    "        )\n",
    "        while not pc.describe_index(INDEX_NAME).status[\"ready\"]:\n",
    "            print(f\"Waiting for index '{INDEX_NAME}' to become ready...\")\n",
    "            time.sleep(5)\n",
    "        print(f\"Index '{INDEX_NAME}' created and ready.\")\n",
    "    else:\n",
    "        print(f\"Connecting to existing index '{INDEX_NAME}'.\")\n",
    "        # Optional: Clear index if you want to re-index fresh\n",
    "        # print(f\"WARNING: Deleting all vectors from existing index '{INDEX_NAME}'...\")\n",
    "        # index_to_clear = pc.Index(INDEX_NAME)\n",
    "        # index_to_clear.delete(delete_all=True)\n",
    "        # print(f\"All vectors deleted from '{INDEX_NAME}'.\")\n",
    "\n",
    "    pinecone_index = pc.Index(INDEX_NAME)\n",
    "    print(f\"Successfully connected to index '{INDEX_NAME}'. Stats: {pinecone_index.describe_index_stats()}\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Failed to initialize or connect to Pinecone index '{INDEX_NAME}': {e}\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4254cb35-0a80-41f6-81cf-f7efdc601f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Index already contains 14 vectors. Skipping data loading.\n"
     ]
    }
   ],
   "source": [
    "# --- Load Data into Pinecone (Only if index is empty) ---\n",
    "index_stats = pinecone_index.describe_index_stats()\n",
    "if index_stats.total_vector_count == 0:\n",
    "    total_docs_loaded = 0\n",
    "    # Loop through each URL in the list\n",
    "    for url in TARGET_URL:\n",
    "        print(f\"\\nProcessing URL: {url}\")\n",
    "        try:\n",
    "            # Fetch URL content\n",
    "            response = requests.get(url, timeout=30) # Use timeout\n",
    "            response.raise_for_status() # Check for HTTP errors\n",
    "\n",
    "            # Parse HTML\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            main_content = soup.find('main') or soup.find('article') or soup.find('body')\n",
    "            page_text = \"\"\n",
    "            if main_content:\n",
    "                page_text = main_content.get_text(separator='\\n', strip=True)\n",
    "            else:\n",
    "                page_text = soup.get_text(separator='\\n', strip=True) # Fallback\n",
    "\n",
    "            if not page_text or len(page_text) < 50: # Basic check for meaningful content\n",
    "                print(f\" -> Warning: Could not extract sufficient text content from {url}. Skipping.\")\n",
    "                continue # Skip to the next URL\n",
    "\n",
    "            print(f\" -> Extracted text length: {len(page_text)} characters.\")\n",
    "\n",
    "            # Generate embedding\n",
    "            # Note: Encoding large pages as a single vector might lose detail.\n",
    "            # Chunking the text into smaller parts is better for real applications.\n",
    "            embedding = model.encode(page_text).tolist()\n",
    "\n",
    "            # Prepare and upsert data\n",
    "            doc_id = str(uuid.uuid4())\n",
    "            metadata = {\"text\": page_text, \"source\": url} # Store the specific URL as source\n",
    "\n",
    "            pinecone_index.upsert(vectors=[(doc_id, embedding, metadata)])\n",
    "            print(f\" -> Data from {url} loaded into Pinecone with ID: {doc_id}\")\n",
    "            total_docs_loaded += 1\n",
    "            time.sleep(0.5) # Small delay to be polite to the server\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            # Handle errors fetching specific URL, continue with the next\n",
    "            print(f\" -> Error fetching URL {url}: {e}\")\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            # Handle other errors during processing/upserting for this URL\n",
    "            print(f\" -> Error processing or upserting data for {url}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if total_docs_loaded > 0:\n",
    "        print(\"Waiting a moment for indexing...\")\n",
    "        time.sleep(2)\n",
    "        print(pinecone_index.describe_index_stats()) # Show final stats\n",
    "    else:\n",
    "        print(\"Warning: No documents were loaded into the index.\")\n",
    "\n",
    "else:\n",
    "    print(f\"\\nIndex already contains {index_stats.total_vector_count} vectors. Skipping data loading.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79eda663-c1f6-4910-8f11-85d23c98c02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** ADDED: Get separate query for context retrieval ***\n",
    "def context_qry(user_code):\n",
    "    context_query = (f\"\"\"\n",
    "    Provide clear, concise, informative, and accurate docstrings for the given python code following PEP 257 conventions and standards, \n",
    "    to generate the content for a Python docstring based on the provided code snippet and relevant PEP contexts.\n",
    "    \n",
    "    **Instructions:**\n",
    "    1.  Start with a concise summary line explaining the function/method's purpose.\n",
    "    2.  If applicable, add a blank line and then more detailed explanation.\n",
    "    3.  Use the 'Args:' section to describe each parameter, its type, and what it represents.\n",
    "    4.  Use the 'Returns:' section to describe the return value and its type.\n",
    "    5.  Use the 'Raises:' section to list any exceptions explicitly raised by the code.\n",
    "    6.  Adhere strictly to PEP 257 formatting.\n",
    "    7.  Base the docstring primarily on the 'Code Snippet to Document'. Use the 'Relevant Context' for \n",
    "    clarification or examples if needed.\n",
    "    \n",
    "    Also, check relevant content for the user given input code: {user_code}\n",
    "    \"\"\")\n",
    "    return context_query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb6a2a4-8441-4b78-9037-ef21b8ca6551",
   "metadata": {},
   "source": [
    "# --- Get User Input ---\n",
    "user_code = (\"\"\"\n",
    "class LabelBinarizer(TransformerMixin, BaseEstimator, auto_wrap_output_keys=None):\n",
    "    _parameter_constraints: dict = {'neg_label': [Integral], 'pos_label': [Integral], 'sparse_output': ['boolean']}\n",
    "\n",
    "    def __init__(self, *, neg_label=0, pos_label=1, sparse_output=False):\n",
    "        self.neg_label = neg_label\n",
    "        self.pos_label = pos_label\n",
    "        self.sparse_output = sparse_output\n",
    "\n",
    "    @_fit_context(prefer_skip_nested_validation=True)\n",
    "    def fit(self, y):\n",
    "        if self.neg_label >= self.pos_label:\n",
    "            raise ValueError(f'neg_label={self.neg_label} must be strictly less than pos_label={self.pos_label}.')\n",
    "        if self.sparse_output and (self.pos_label == 0 or self.neg_label != 0):\n",
    "            raise ValueError(f'Sparse binarization is only supported with non zero pos_label and zero neg_label, got pos_label={self.pos_label} and neg_label={self.neg_label}')\n",
    "        self.y_type_ = type_of_target(y, input_name='y')\n",
    "        if 'multioutput' in self.y_type_:\n",
    "            raise ValueError('Multioutput target data is not supported with label binarization')\n",
    "        if _num_samples(y) == 0:\n",
    "            raise ValueError('y has 0 samples: %r' % y)\n",
    "        self.sparse_input_ = sp.issparse(y)\n",
    "        self.classes_ = unique_labels(y)\n",
    "        return self\n",
    "\n",
    "    def fit_transform(self, y):\n",
    "        return self.fit(y).transform(y)\n",
    "\n",
    "    def transform(self, y):\n",
    "        check_is_fitted(self)\n",
    "        y_is_multilabel = type_of_target(y).startswith('multilabel')\n",
    "        if y_is_multilabel and (not self.y_type_.startswith('multilabel')):\n",
    "            raise ValueError('The object was not fitted with multilabel input.')\n",
    "        return label_binarize(y, classes=self.classes_, pos_label=self.pos_label, neg_label=self.neg_label, sparse_output=self.sparse_output)\n",
    "\n",
    "    def inverse_transform(self, Y, threshold=None):\n",
    "        check_is_fitted(self)\n",
    "        if threshold is None:\n",
    "            threshold = (self.pos_label + self.neg_label) / 2.0\n",
    "        if self.y_type_ == 'multiclass':\n",
    "            y_inv = _inverse_binarize_multiclass(Y, self.classes_)\n",
    "        else:\n",
    "            y_inv = _inverse_binarize_thresholding(Y, self.y_type_, self.classes_, threshold)\n",
    "        if self.sparse_input_:\n",
    "            y_inv = sp.csr_matrix(y_inv)\n",
    "        elif sp.issparse(y_inv):\n",
    "            y_inv = y_inv.toarray()\n",
    "        return y_inv\n",
    "\n",
    "    def _more_tags(self):\n",
    "        return {'X_types': ['1dlabels']}\n",
    "\"\"\")\n",
    "if not user_code.strip():\n",
    "    print(\"No code provided. Exiting.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f48659d7-babc-448e-b961-d2da0b377605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: Initial Generation Attempt (No RAG) ---\n",
    "def initial_doc_without_rag(user_code, OLLAMA_GENERATOR_MODEL):\n",
    "    initial_docstring = \"\"\n",
    "    try:\n",
    "        initial_prompt = f\"\"\"\n",
    "        Generate a concise and accurate Python docstring for the following code. Focus only on the code provided.\n",
    "        Return only the docstring for the given code, dont give or return the code.\n",
    "        Python Code:\n",
    "        ---\n",
    "        {user_code}\n",
    "        ---\n",
    "    \n",
    "        Generate only the docstring content, formatted appropriately:\n",
    "        \"\"\"\n",
    "        response = ollama_client.generate(\n",
    "            model=OLLAMA_GENERATOR_MODEL,\n",
    "            prompt=initial_prompt\n",
    "        )\n",
    "        initial_docstring = response.get('response', '').strip()\n",
    "    except Exception as e:\n",
    "        print(f\" -> ERROR during initial generation: {e}\")\n",
    "        initial_docstring = \"\" # Ensure empty on error\n",
    "    return initial_docstring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "acf3c314-0d00-47ec-b71d-93cda22f55ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 2: Self-Critique (Simulated) ---\n",
    "def self_critique(initial_docstring, OLLAMA_HELPER_MODEL):\n",
    "    critique_passed = False\n",
    "    if not initial_docstring:\n",
    "        #print(\" -> Skipping critique: No initial docstring was generated.\")\n",
    "        needs_improvement = True # Treat empty generation as needing improvement\n",
    "    else:\n",
    "        try:\n",
    "            # Extract parameter names from user code for critique prompt\n",
    "            param_names = []\n",
    "            param_match = re.search(r'def\\s+\\w+\\s*\\((.*?)\\):', user_code)\n",
    "            if param_match:\n",
    "                params_str = param_match.group(1)\n",
    "                # Basic parsing, handles simple cases, might need refinement for complex signatures\n",
    "                params = [p.strip().split(':')[0].split('=')[0].strip() for p in params_str.split(',') if p.strip() and p.strip() not in ['self', 'cls']]\n",
    "                param_names = [p for p in params if p and p != '*'] # Filter out empty strings or just '*'\n",
    "    \n",
    "            critique_prompt = f\"\"\"\n",
    "            Task: Evaluate the quality of the generated Python Docstring based *only* on the original Python Code. Check if the docstring provides a reasonable summary and mentions key elements like parameters (if any).\n",
    "            Answer ONLY with GOOD or NEEDS_IMPROVEMENT.\n",
    "    \n",
    "            Original Python Code:\n",
    "            ---\n",
    "            {user_code}\n",
    "            ---\n",
    "            Generated Docstring:\n",
    "            ---\n",
    "            {initial_docstring}\n",
    "            ---\n",
    "    \n",
    "            Evaluation Criteria:\n",
    "            - Does the docstring provide a basic summary of what the code might do?\n",
    "            - If the code has parameters ({', '.join(param_names) if param_names else 'None'}), does the docstring attempt to mention or describe them?\n",
    "            - Is the docstring format plausible (starts with triple quotes)?\n",
    "    \n",
    "            Quality Assessment (GOOD or NEEDS_IMPROVEMENT):\n",
    "            \"\"\"\n",
    "            response = ollama_client.generate(\n",
    "                model=OLLAMA_HELPER_MODEL, # Use helper model for critique\n",
    "                prompt=critique_prompt,\n",
    "                options={'temperature': 0.0} # Deterministic critique\n",
    "            )\n",
    "            critique_result = response.get('response', '').strip().upper()\n",
    "            #print(f\" -> Self-critique result: {critique_result}\")\n",
    "    \n",
    "            if \"GOOD\" in critique_result:\n",
    "                critique_passed = True\n",
    "                needs_improvement = False\n",
    "            else: # Assume NEEDS_IMPROVEMENT or other non-GOOD response means improvement needed\n",
    "                needs_improvement = True\n",
    "    \n",
    "        except Exception as e:\n",
    "            print(f\" -> ERROR during self-critique: {e}\")\n",
    "            needs_improvement = True # Assume improvement needed if critique fails\n",
    "    return (critique_result, needs_improvement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f89cdf45-9200-4410-b429-6d3022cac0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 3: Adaptive Retrieval (Trigger RAG if needed) ---\n",
    "def self_RAG(context_query, user_code, needs_improvement, pinecone_index, embedding_model):\n",
    "    final_context = \"\"\n",
    "    final_source_description = \"N/A\"\n",
    "    \n",
    "    if needs_improvement and pinecone_index:\n",
    "        rag_triggered = True # Set flag\n",
    "        #--- RAG Step 3.1: Retrieve Initial Candidate Documents ---\n",
    "        #print(\" -> RAG 3.1: Retrieving initial documents from Pinecone...\")\n",
    "        retrieved_matches = []\n",
    "        initial_docs_for_refinement = []\n",
    "        if context_query.strip():\n",
    "            try:\n",
    "                query_embedding = embedding_model.encode(context_query).tolist()\n",
    "                search_results = pinecone_index.query(vector=query_embedding, top_k=3, include_metadata=True)\n",
    "                retrieved_matches = search_results.matches\n",
    "                initial_docs_for_refinement = [{\"text\": m.metadata.get('text',''), \"source\": m.metadata.get('source','N/A')} for m in retrieved_matches if m.metadata.get('text')]\n",
    "                #print(f\"    -> Retrieved {len(initial_docs_for_refinement)} candidate documents.\")\n",
    "            except Exception as e: print(f\"    -> ERROR querying Pinecone: {e}\")\n",
    "        else: print(\"    -> No context query provided, skipping Pinecone retrieval.\")\n",
    "    \n",
    "        # --- RAG Step 3.2: Corrective Web Search ---\n",
    "        web_context_docs = [] # Store raw web docs for Step 4\n",
    "        if context_query.strip() and GOOGLE_SEARCH_ENABLED:\n",
    "            try:\n",
    "                code_first_line = user_code.split('\\n', 1)[0].strip()\n",
    "                if len(code_first_line) > 50: code_first_line = code_first_line[:50] + \"...\"\n",
    "                web_query = f\"python {context_query} documentation for `{code_first_line}`\"\n",
    "                #print(f\" -> Web search query: '{web_query}'\")\n",
    "                #print(f\" -> Searching Google (asking for num=3 results)...\")\n",
    "                search_urls = list(google_search_func(web_query, lang=\"en\"))\n",
    "        \n",
    "                if search_urls:\n",
    "                    #print(f\" -> Found {len(search_urls)} Google Search URLs. Fetching content...\")\n",
    "                    for i, url in enumerate(search_urls[:3]):\n",
    "                        #print(f\"    -> Fetching content from URL {i+1}: {url}\")\n",
    "                        try:\n",
    "                            headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
    "                            response = requests.get(url, timeout=15, headers=headers, allow_redirects=True)\n",
    "                            response.raise_for_status()\n",
    "                            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                            main_content = soup.find('main') or soup.find('article') or soup.find('body')\n",
    "                            page_text = \"\"\n",
    "                            if main_content: page_text = main_content.get_text(separator='\\n', strip=True)\n",
    "                            else: page_text = soup.get_text(separator='\\n', strip=True)\n",
    "        \n",
    "                            if page_text:\n",
    "                                 #print(f\"       -> Storing snippet (length {len(page_text)}) from: {url} for refinement.\")\n",
    "                                 max_snippet_length = 4000 # Allow longer snippets before chunking\n",
    "                                 web_context_docs.append({\n",
    "                                     \"text\": page_text[:max_snippet_length],\n",
    "                                     \"source\": url,\n",
    "                                     \"type\": \"web\"\n",
    "                                 })\n",
    "                            else: print(f\"       -> No text content extracted from: {url}\")\n",
    "                        except requests.exceptions.Timeout: print(f\"       -> Timeout fetching URL: {url}\")\n",
    "                        except requests.exceptions.RequestException as req_err: print(f\"       -> Failed to fetch/process URL {url}: {req_err}\")\n",
    "                        except Exception as parse_err: print(f\"       -> Failed to parse URL {url}: {parse_err}\")\n",
    "                        time.sleep(1.5)\n",
    "                else: print(\" -> Google Search did not return result URLs.\")\n",
    "            except Exception as e:\n",
    "                print(f\" -> ERROR during Google Search / content fetching process: {e}\")\n",
    "                traceback.print_exc()\n",
    "        elif not context_query.strip(): print(\" -> Skipping web search: No context query was provided.\")\n",
    "        elif not GOOGLE_SEARCH_ENABLED: print(\" -> Skipping web search: 'googlesearch-python' is not installed or failed to import.\")\n",
    "        relevant_chunks = []\n",
    "        combined_sources_list = []\n",
    "        all_potential_docs = initial_docs_for_refinement + web_context_docs\n",
    "    \n",
    "        # --- Helper Function for Rule-Based Grading (Copied from previous CRAG version) ---\n",
    "        def grade_chunk_relevance(chunk_text, user_code, context_query):\n",
    "            if not chunk_text or len(chunk_text.strip()) < 10: return False\n",
    "            chunk_lower = chunk_text.lower()\n",
    "            code_lower = user_code.lower()\n",
    "            query_lower = context_query.lower()\n",
    "            core_keywords = [\"docstring\", \"parameter\", \"argument\", \"return\", \"yield\", \"attribute\", \"class\", \"function\", \"method\", \"pep 257\", \"summary\", \"description\", \"example\", \"usage\", \"type hint\"]\n",
    "            if any(keyword in chunk_lower for keyword in core_keywords): return True\n",
    "            if query_lower:\n",
    "                query_keywords = re.findall(r'\\b\\w{3,}\\b', query_lower)\n",
    "                stop_words = {\"how\", \"what\", \"the\", \"and\", \"for\", \"does\", \"work\", \"python\", \"use\", \"create\"}\n",
    "                query_keywords = [kw for kw in query_keywords if kw not in stop_words]\n",
    "                if query_keywords and any(keyword in chunk_lower for keyword in query_keywords): return True\n",
    "            if (\"def \" in chunk_lower and \"def \" in code_lower) or (\"class \" in chunk_lower and \"class \" in code_lower): return True\n",
    "            param_match = re.search(r'def\\s+\\w+\\s*\\((.*?)\\):', user_code)\n",
    "            if param_match:\n",
    "                params = [p.strip().split('=')[0].strip() for p in param_match.group(1).split(',') if p.strip()]\n",
    "                if any(f\"parameter {p}\" in chunk_lower or f\"{p}:\" in chunk_lower for p in params if p not in ['self', 'cls']): return True\n",
    "            return False\n",
    "        # --- End Helper Function ---\n",
    "    \n",
    "        if not all_potential_docs:\n",
    "            print(\"    -> No documents from Pinecone or Web to refine.\")\n",
    "        else:\n",
    "            #print(f\"    -> Processing {len(all_potential_docs)} documents for chunking...\")\n",
    "            for doc in all_potential_docs:\n",
    "                doc_text = doc.get('text', '')\n",
    "                doc_source = doc.get('source', 'N/A')\n",
    "                doc_type = doc.get('type', 'unknown')\n",
    "                if not doc_text: continue\n",
    "                chunks = [chunk.strip() for chunk in doc_text.split('\\n\\n') if chunk.strip()]\n",
    "                # print(f\"       -> Doc from {doc_source.split('/')[-1]} yielded {len(chunks)} chunks.\") # Verbose\n",
    "                graded_chunk_count = 0\n",
    "                for chunk in chunks:\n",
    "                    if grade_chunk_relevance(chunk, user_code, context_query):\n",
    "                        relevant_chunks.append(chunk)\n",
    "                        graded_chunk_count += 1\n",
    "                        source_tag = f\"{doc_type.capitalize()}:{doc_source.split('/')[-1]}\"\n",
    "                        if source_tag not in combined_sources_list: combined_sources_list.append(source_tag)\n",
    "                # print(f\"          -> Kept {graded_chunk_count} relevant chunks.\") # Verbose\n",
    "    \n",
    "        if relevant_chunks:\n",
    "            final_context = \"\\n\\n\".join(relevant_chunks)\n",
    "            final_source_description = \" | \".join(combined_sources_list)\n",
    "            #print(f\" -> Final refined context compiled from {len(relevant_chunks)} relevant chunks.\")\n",
    "            #print(f\" -> Sources: {final_source_description}\")\n",
    "        else:\n",
    "            print(\" -> No relevant chunks found after refinement.\")\n",
    "            final_context = \"\"\n",
    "    \n",
    "    elif not pinecone_index:\n",
    "         print(\"\\nStep 3: Skipping RAG pipeline as Pinecone index is unavailable.\")\n",
    "    else: # Critique passed\n",
    "        print(\"\\nStep 3: Initial generation passed critique. Skipping RAG pipeline.\")\n",
    "    return final_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a746589-318a-4ccd-bb9a-d506e1cb00b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_content_generation(final_context, user_code, OLLAMA_GENERATOR_MODEL):\n",
    "    messages = [\n",
    "    {'role': 'system', 'content': 'You are an expert Python programmer tasked with generating docstrings. You will receive context (if found) in one message, and the code to document in the final message. Use the context only if it is directly relevant to explaining the provided code. Return only the docstring and dont include the given python code in the output'}\n",
    "]\n",
    "\n",
    "    # Add the retrieved context as a separate user message, if it exists\n",
    "    if final_context:\n",
    "        # Include source information in the context message for clarity\n",
    "        messages.append({'role': 'user', 'content': f\"Here is potentially relevant context retrieved from knowledge base \\n---\\n{final_context} for the python\\n{user_code}\\n :\\n---\"})\n",
    "    else:\n",
    "        # Explicitly state if no context was found or provided\n",
    "        messages.append({'role': 'user', 'content': \"No specific context was retrieved or provided for this request.\"})\n",
    "    \n",
    "    # Add the final user message with the code and the explicit request\n",
    "    messages.append({'role': 'user', 'content': f\"Based on any relevant context provided earlier, generate the Python docstring for the following code:\\n```python\\n{user_code}\\n```\\n\\nOutput *only* the complete docstring content itself, starting with triple quotes. Dont include python codes in the output\"})\n",
    "    \n",
    "    try:\n",
    "        # Use ollama.chat instead of ollama.generate\n",
    "        response = ollama_client.chat(\n",
    "            model=OLLAMA_GENERATOR_MODEL,\n",
    "            messages=messages\n",
    "            # You could add options here if needed, e.g., options={'temperature': 0.5}\n",
    "        )\n",
    "    \n",
    "        # Extract the content from the 'message' dictionary in the response\n",
    "        generated_docstring = response.get('message', {}).get('content', '').strip()\n",
    "    \n",
    "        # --- Print Output ---\n",
    "        #print(\"\\n--- Generated Docstring ---\")\n",
    "    \n",
    "        # Basic cleaning: Chat models might add explanatory text or markdown fences.\n",
    "        # Try to remove common prefixes/suffixes if the model didn't follow instructions precisely.\n",
    "        if generated_docstring.startswith(\"```python\"):\n",
    "            generated_docstring = generated_docstring[len(\"```python\"):].strip()\n",
    "        elif generated_docstring.startswith(\"```\"):\n",
    "             generated_docstring = generated_docstring[len(\"```\"):].strip()\n",
    "    \n",
    "        if generated_docstring.endswith(\"```\"):\n",
    "            generated_docstring = generated_docstring[:-len(\"```\")].strip()\n",
    "    \n",
    "        #print(\"---------------------------\\n\")\n",
    "        #print(generated_docstring)\n",
    "        #print(\"---------------------------\\n\")\n",
    "    \n",
    "    except KeyError:\n",
    "         print(f\"Error: Unexpected response structure from Ollama chat: {response}\")\n",
    "         traceback.print_exc()\n",
    "    except Exception as e:\n",
    "        print(f\"Error communicating with Ollama chat endpoint: {e}\")\n",
    "    return (generated_docstring)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "145f536d-9e1e-438f-8dfe-100dbf476e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_self_rag_pipeline(user_code, pinecone_index, emb_model, ollama_llm_client, generator_model_name, helper_model_name):\n",
    "    user_cd = user_code\n",
    "    context_query = context_qry(user_cd)\n",
    "    doc_without_rag = initial_doc_without_rag(user_cd, generator_model_name)\n",
    "    critique_result, needs_improvement = self_critique(doc_without_rag, helper_model_name)\n",
    "    #print(needs_improvement)\n",
    "    final_context = self_RAG(doc_without_rag, user_cd, needs_improvement, pinecone_index, emb_model)\n",
    "    retrieved_contexts_list.append(final_context)\n",
    "    generated_docstring = final_content_generation(final_context, user_cd, generator_model_name)\n",
    "    return(generated_docstring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7860b038-d833-4c12-89a3-13abb94174df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_files_df = pd.read_pickle('class_files_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "79a8da16-c355-41ae-9b35-425971f79891",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = class_files_df[\"Comments\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8b0edf2a-19db-4f6f-ac80-335b0a2d96ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_docstrings_list = []\n",
    "retrieved_contexts_list = []\n",
    "rewritten_contexts_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f50fdae6-365e-4b48-8579-ed482e508726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 3: Initial generation passed critique. Skipping RAG pipeline.\n",
      "\n",
      "Step 3: Initial generation passed critique. Skipping RAG pipeline.\n",
      "\n",
      "Step 3: Initial generation passed critique. Skipping RAG pipeline.\n",
      "\n",
      "Step 3: Initial generation passed critique. Skipping RAG pipeline.\n",
      "\n",
      "Step 3: Initial generation passed critique. Skipping RAG pipeline.\n",
      "       -> Failed to fetch/process URL /search?num=12: Invalid URL '/search?num=12': No scheme supplied. Perhaps you meant https:///search?num=12?\n",
      "\n",
      "Step 3: Initial generation passed critique. Skipping RAG pipeline.\n",
      "       -> Failed to fetch/process URL https://stackoverflow.com/questions/64879115/how-to-implement-conv1dtranspose-keras: 403 Client Error: Forbidden for url: https://stackoverflow.com/questions/64879115/how-to-implement-conv1dtranspose-keras\n",
      "\n",
      "Step 3: Initial generation passed critique. Skipping RAG pipeline.\n",
      "       -> Failed to fetch/process URL https://stackoverflow.com/questions/61816921/using-conv2dtranspose-to-output-the-double-of-its-input-shape: 403 Client Error: Forbidden for url: https://stackoverflow.com/questions/61816921/using-conv2dtranspose-to-output-the-double-of-its-input-shape\n",
      "\n",
      "Step 3: Initial generation passed critique. Skipping RAG pipeline.\n",
      "\n",
      "Step 3: Initial generation passed critique. Skipping RAG pipeline.\n",
      "\n",
      "Step 3: Initial generation passed critique. Skipping RAG pipeline.\n",
      "\n",
      "Step 3: Initial generation passed critique. Skipping RAG pipeline.\n",
      "\n",
      "Step 3: Initial generation passed critique. Skipping RAG pipeline.\n",
      "\n",
      "Step 3: Initial generation passed critique. Skipping RAG pipeline.\n",
      "\n",
      "Step 3: Initial generation passed critique. Skipping RAG pipeline.\n",
      "\n",
      "Step 3: Initial generation passed critique. Skipping RAG pipeline.\n",
      "       -> Failed to fetch/process URL https://stackoverflow.com/questions/39406539/sklearn-function-transformer-in-pipeline: 403 Client Error: Forbidden for url: https://stackoverflow.com/questions/39406539/sklearn-function-transformer-in-pipeline\n",
      "\n",
      "Step 3: Initial generation passed critique. Skipping RAG pipeline.\n",
      "\n",
      "Step 3: Initial generation passed critique. Skipping RAG pipeline.\n",
      "\n",
      "Step 3: Initial generation passed critique. Skipping RAG pipeline.\n",
      "\n",
      "Step 3: Initial generation passed critique. Skipping RAG pipeline.\n",
      "\n",
      "Step 3: Initial generation passed critique. Skipping RAG pipeline.\n",
      "\n",
      "Step 3: Initial generation passed critique. Skipping RAG pipeline.\n",
      "       -> Failed to fetch/process URL /search?num=12: Invalid URL '/search?num=12': No scheme supplied. Perhaps you meant https:///search?num=12?\n",
      "\n",
      "Step 3: Initial generation passed critique. Skipping RAG pipeline.\n",
      "\n",
      "Step 3: Initial generation passed critique. Skipping RAG pipeline.\n",
      "\n",
      "Step 3: Initial generation passed critique. Skipping RAG pipeline.\n",
      "\n",
      "Step 3: Initial generation passed critique. Skipping RAG pipeline.\n",
      "\n",
      "Step 3: Initial generation passed critique. Skipping RAG pipeline.\n",
      "       -> Failed to fetch/process URL /search?num=12: Invalid URL '/search?num=12': No scheme supplied. Perhaps you meant https:///search?num=12?\n",
      "\n",
      "Step 3: Initial generation passed critique. Skipping RAG pipeline.\n",
      "\n",
      "Step 3: Initial generation passed critique. Skipping RAG pipeline.\n",
      "\n",
      "Step 3: Initial generation passed critique. Skipping RAG pipeline.\n",
      "\n",
      "Step 3: Initial generation passed critique. Skipping RAG pipeline.\n",
      "\n",
      "Step 3: Initial generation passed critique. Skipping RAG pipeline.\n",
      "\n",
      "Step 3: Initial generation passed critique. Skipping RAG pipeline.\n",
      "\n",
      "Step 3: Initial generation passed critique. Skipping RAG pipeline.\n",
      "\n",
      "Step 3: Initial generation passed critique. Skipping RAG pipeline.\n",
      "       -> Failed to fetch/process URL https://stackoverflow.com/questions/59807511/decoding-using-multilabelbinarizer-python: 403 Client Error: Forbidden for url: https://stackoverflow.com/questions/59807511/decoding-using-multilabelbinarizer-python\n",
      "\n",
      "Step 3: Initial generation passed critique. Skipping RAG pipeline.\n",
      "\n",
      "Step 3: Initial generation passed critique. Skipping RAG pipeline.\n",
      "\n",
      "Step 3: Initial generation passed critique. Skipping RAG pipeline.\n",
      "       -> Failed to fetch/process URL /search?num=12: Invalid URL '/search?num=12': No scheme supplied. Perhaps you meant https:///search?num=12?\n",
      "       -> Failed to fetch/process URL https://stackoverflow.com/questions/58721136/how-to-add-a-pooling-layer-on-a-keras-model: 403 Client Error: Forbidden for url: https://stackoverflow.com/questions/58721136/how-to-add-a-pooling-layer-on-a-keras-model\n",
      "       -> Failed to fetch/process URL /search?num=12: Invalid URL '/search?num=12': No scheme supplied. Perhaps you meant https:///search?num=12?\n",
      "       -> Failed to fetch/process URL https://stackoverflow.com/questions/39815518/keras-maxpooling2d-layer-gives-valueerror: 403 Client Error: Forbidden for url: https://stackoverflow.com/questions/39815518/keras-maxpooling2d-layer-gives-valueerror\n",
      "       -> Failed to fetch/process URL /search?num=12: Invalid URL '/search?num=12': No scheme supplied. Perhaps you meant https:///search?num=12?\n",
      "       -> Failed to fetch/process URL /search?num=12: Invalid URL '/search?num=12': No scheme supplied. Perhaps you meant https:///search?num=12?\n",
      "       -> Failed to fetch/process URL https://www.datacamp.com/tutorial/principal-component-analysis-in-python: 403 Client Error: Forbidden for url: https://www.datacamp.com/tutorial/principal-component-analysis-in-python\n",
      "\n",
      "Step 3: Initial generation passed critique. Skipping RAG pipeline.\n",
      "\n",
      "Step 3: Initial generation passed critique. Skipping RAG pipeline.\n",
      "\n",
      "Step 3: Initial generation passed critique. Skipping RAG pipeline.\n",
      "\n",
      "Step 3: Initial generation passed critique. Skipping RAG pipeline.\n",
      "\n",
      "Step 3: Initial generation passed critique. Skipping RAG pipeline.\n",
      "\n",
      "Step 3: Initial generation passed critique. Skipping RAG pipeline.\n",
      "\n",
      "Step 3: Initial generation passed critique. Skipping RAG pipeline.\n",
      "       -> Failed to fetch/process URL https://towardsdatascience.com/softmax-regression-in-python-multi-class-classification-3cb560d90cb2/: 403 Client Error: Forbidden for url: https://towardsdatascience.com/softmax-regression-in-python-multi-class-classification-3cb560d90cb2/\n",
      "       -> Failed to fetch/process URL /search?num=12: Invalid URL '/search?num=12': No scheme supplied. Perhaps you meant https:///search?num=12?\n",
      "       -> Failed to fetch/process URL https://www.blog.trainindata.com/target-encoder-a-powerful-categorical-encoding-method/: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "Step 3: Initial generation passed critique. Skipping RAG pipeline.\n",
      "\n",
      "Step 3: Initial generation passed critique. Skipping RAG pipeline.\n",
      "\n",
      "Step 3: Initial generation passed critique. Skipping RAG pipeline.\n",
      "\n",
      "Step 3: Initial generation passed critique. Skipping RAG pipeline.\n",
      "\n",
      "Step 3: Initial generation passed critique. Skipping RAG pipeline.\n",
      "\n",
      "Step 3: Initial generation passed critique. Skipping RAG pipeline.\n",
      "\n",
      "Step 3: Initial generation passed critique. Skipping RAG pipeline.\n",
      "       -> Failed to fetch/process URL https://www.datacamp.com/tutorial/categorical-data: 403 Client Error: Forbidden for url: https://www.datacamp.com/tutorial/categorical-data\n",
      "       -> Failed to fetch/process URL https://stackoverflow.com/questions/48320396/create-a-custom-sklearn-transformermixin-that-transforms-categorical-variables-c: 403 Client Error: Forbidden for url: https://stackoverflow.com/questions/48320396/create-a-custom-sklearn-transformermixin-that-transforms-categorical-variables-c\n"
     ]
    }
   ],
   "source": [
    "for i, row in class_files_df.iterrows():\n",
    "    user_code = row[\"Code_without_comments\"]\n",
    "    output = run_self_rag_pipeline(user_code, pinecone_index, model, ollama_client, OLLAMA_MODEL, OLLAMA_REWRITER_MODEL)\n",
    "    generated_docstrings_list.append(output)\n",
    "class_files_df[\"RAG_Docstring\"] = generated_docstrings_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0be0eec7-1445-48ac-92a7-43e068283519",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_rag_docstring(docstring_text):\n",
    "    if not isinstance(docstring_text, str):\n",
    "        return docstring_text\n",
    "\n",
    "    if docstring_text.startswith(\"# ERROR:\") or docstring_text.startswith(\"# SKIPPED:\"):\n",
    "        return docstring_text\n",
    "\n",
    "    text = docstring_text.strip()\n",
    "\n",
    "    if text.startswith(\"```python\"):\n",
    "        text = text[len(\"```python\"):].strip()\n",
    "    elif text.startswith(\"```\"):\n",
    "        text = text[len(\"```\"):].strip()\n",
    "    if text.endswith(\"```\"):\n",
    "        text = text[:-len(\"```\")].strip()\n",
    "\n",
    "    content_inside_quotes = None\n",
    "    first_double_quotes = text.find('\"\"\"')\n",
    "    if first_double_quotes != -1:\n",
    "        last_double_quotes = text.rfind('\"\"\"')\n",
    "        if last_double_quotes > first_double_quotes and (last_double_quotes + 3) <= len(text):\n",
    "            content_inside_quotes = text[first_double_quotes + 3 : last_double_quotes].strip()\n",
    "\n",
    "    if content_inside_quotes is None or not content_inside_quotes.strip():\n",
    "        first_single_quotes = text.find(\"'''\")\n",
    "        if first_single_quotes != -1:\n",
    "            last_single_quotes = text.rfind(\"'''\")\n",
    "            if last_single_quotes > first_single_quotes and (last_single_quotes + 3) <= len(text):\n",
    "                content_inside_quotes = text[first_single_quotes + 3 : last_single_quotes].strip()\n",
    "    \n",
    "    if content_inside_quotes is not None and content_inside_quotes.strip():\n",
    "        final_text_to_clean = content_inside_quotes\n",
    "    else:\n",
    "        final_text_to_clean = text\n",
    "        if final_text_to_clean.startswith('\"\"\"') and final_text_to_clean.endswith('\"\"\"') and len(final_text_to_clean) >= 6:\n",
    "            final_text_to_clean = final_text_to_clean[3:-3].strip()\n",
    "        elif final_text_to_clean.startswith(\"'''\") and final_text_to_clean.endswith(\"'''\") and len(final_text_to_clean) >= 6:\n",
    "            final_text_to_clean = final_text_to_clean[3:-3].strip()\n",
    "\n",
    "    final_text_to_clean = re.sub(r\"(?i)^class\\s+\\w+:\\s*\\n?\", \"\", final_text_to_clean).strip()\n",
    "    \n",
    "    return final_text_to_clean\n",
    "\n",
    "class_files_df[\"RAG_Docstring\"] = class_files_df[\"RAG_Docstring\"].astype(str).apply(clean_rag_docstring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9b730a2d-d2a2-46e3-9284-4946d36382fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#class_files_df.to_excel('self_rag.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ea10cc52-c84f-4228-b25e-214ae2b60b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rouge(df, reference_column, hypothesis_column):\n",
    "    rouge = Rouge()\n",
    "\n",
    "    def calculate_score(row):\n",
    "        scores = rouge.get_scores(row[hypothesis_column].lower(), row[reference_column].lower())\n",
    "        return scores[0]['rouge-1']['f']\n",
    "\n",
    "    df['ROUGE-1 ' + reference_column] = df.apply(calculate_score, axis=1)\n",
    "    return df\n",
    "\n",
    "# Calculate ROUGE-1 scores\n",
    "data_1 = calculate_rouge(class_files_df, 'Comments', 'RAG_Docstring')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3ca5e8a9-29d0-42a2-b551-e26aa648fc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bleu(df, reference_column, hypothesis_column):\n",
    "    nltk.download('punkt')\n",
    "\n",
    "    def calculate_score(row):\n",
    "        reference = [row[reference_column].lower().split()]\n",
    "        hypothesis = row[hypothesis_column].lower().split()\n",
    "        score = sentence_bleu(reference, hypothesis, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "        return score\n",
    "\n",
    "    df['BLEU Score ' + reference_column] = df.apply(calculate_score, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8687a372-3199-4d0b-9de0-7860384f0355",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/balajivenktesh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Calculate BLEU scores\n",
    "data_1 = calculate_bleu(data_1, 'Comments', 'RAG_Docstring')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "acd6372b-b54f-4fdf-8953-70df710993a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate BERT encoding score, using cosine similarity\n",
    "def calculate_bert_score(ground_truth, generated):\n",
    "    # Calculate BERT score\n",
    "    _, _, bert_score_f1 = score([ground_truth], [generated], lang='en', model_type='bert-base-uncased')\n",
    "\n",
    "    return bert_score_f1.item()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2f610551-4c41-4bc2-97e8-cd8acf5b99e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate BLEU scores\n",
    "list_append_1 = []\n",
    "for index, row in data_1.iterrows():\n",
    "    list_append_1.append(calculate_bert_score(str(row[\"Comments\"]), str(row[\"RAG_Docstring\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5d763965-99d5-4edc-9cfa-6878a225a412",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1[\"Accuracy\"] = list_append_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5048ec78-c58b-44fa-ad1e-321efc57769f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate number of syllables in docstring\n",
    "def count_syllables(word):\n",
    "    # Remove punctuation\n",
    "    word = re.sub(r'[^a-zA-Z]', '', word)\n",
    "    \n",
    "    # Vowel count\n",
    "    vowels = 'aeiouy'\n",
    "    syllables = 0\n",
    "    last_was_vowel = False\n",
    "    for char in word:\n",
    "        if char.lower() in vowels:\n",
    "            if not last_was_vowel:\n",
    "                syllables += 1\n",
    "            last_was_vowel = True\n",
    "        else:\n",
    "            last_was_vowel = False\n",
    "    \n",
    "    # Adjust syllable count for words ending in 'e'\n",
    "    if word.endswith(('e', 'es', 'ed')):\n",
    "        syllables -= 1\n",
    "    \n",
    "    # Adjust syllable count for words with no vowels\n",
    "    if syllables == 0:\n",
    "        syllables = 1\n",
    "    \n",
    "    return syllables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3df5c382-359d-4d5f-b72d-46414cb85c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Flesch reading score\n",
    "def flesch_reading_ease(text):\n",
    "    sentences = text.count('.') + text.count('!') + text.count('?') + 1\n",
    "    words = len(re.findall(r'\\b\\w+\\b', text))\n",
    "    syllables = sum(count_syllables(word) for word in text.split())\n",
    "    \n",
    "    # Calculate Flesch Reading Ease score\n",
    "    score = 206.835 - 1.015 * (words / sentences) - 84.6 * (syllables / words)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b5b29e8b-0c86-492b-9849-1353322b9cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Easy scores\n",
    "list_append_2 = []\n",
    "for index, row in data_1.iterrows():\n",
    "    list_append_2.append(flesch_reading_ease(str(row[\"RAG_Docstring\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "576b2a81-541a-4c36-a4ff-142a3d022e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1[\"Ease\"] = list_append_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "44356136-5e78-4ebe-93b9-1499c8dadfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "def compress(input):\n",
    "\treturn zlib.compress(input.encode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "53000854-a3ad-47db-8790-e63c72f626d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conciness(ground_truth, generated):\n",
    "    comp1 = compress(ground_truth)\n",
    "    comp2 = compress(generated)\n",
    "    return sys.getsizeof(comp2) / sys.getsizeof(comp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e9735021-3912-4366-9c68-745cf06250bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Conciseness scores\n",
    "list_append_3 = []\n",
    "for index, row in data_1.iterrows():\n",
    "    list_append_3.append(conciness(str(row[\"Comments\"]), str(row[\"RAG_Docstring\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ac3ecdc0-7d42-466e-bb4c-6e4a7acb9f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1[\"Conciseness\"] = list_append_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "df5ee832-7fb9-4aa5-a5d5-e447b2ad08bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_parameter_coverage(code_str, docstring_str):\n",
    "    \"\"\"\n",
    "    Calculates the proportion of function/method parameters mentioned in the docstring.\n",
    "    Returns a float (0.0 to 1.0) or None if no parameters are found in the code.\n",
    "    \"\"\"        \n",
    "    match = re.search(r\"def\\s+\\w+\\s*\\((.*?)\\):\", code_str)\n",
    "    if not match:\n",
    "        match = re.search(r\"async\\s+def\\s+\\w+\\s*\\((.*?)\\):\", code_str) \n",
    "\n",
    "    if not match:\n",
    "        return None \n",
    "\n",
    "    params_str = match.group(1)\n",
    "    if not params_str.strip(): \n",
    "        return 1.0 \n",
    "\n",
    "    potential_params = [p.strip().split('=')[0].split(':')[0].strip() for p in params_str.split(',')]\n",
    "    actual_params = [p for p in potential_params if p and p not in ('self', 'cls') and not p.startswith('*')]\n",
    "\n",
    "    if not actual_params:\n",
    "        return 1.0 \n",
    "\n",
    "    covered_params = 0\n",
    "    docstring_lower = docstring_str.lower()\n",
    "    for param_name in actual_params:\n",
    "        if re.search(r\"\\b\" + re.escape(param_name.lower()) + r\"\\b\", docstring_lower):\n",
    "            covered_params += 1\n",
    "        elif f\"{param_name.lower()}:\" in docstring_lower or f\"parameter {param_name.lower()}\" in docstring_lower:\n",
    "             covered_params += 1\n",
    "    return covered_params / len(actual_params) if actual_params else 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fb4b710f-6bb9-4a98-bba0-ad7f1fce7459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Return Value Coverage Calculation Function ---\n",
    "def calculate_return_coverage(code_str, docstring_str):\n",
    "    \"\"\"\n",
    "    Checks if the docstring mentions a return value if the code seems to have one.\n",
    "    Returns 1 if covered/not applicable, 0 if potentially missing, None on error.\n",
    "    \"\"\"\n",
    "    has_return_statement = False\n",
    "    for line in code_str.splitlines():\n",
    "        stripped_line = line.strip()\n",
    "        if stripped_line.startswith(\"return \") and not stripped_line.endswith(\"return None\") and len(stripped_line) > len(\"return \"):\n",
    "            has_return_statement = True\n",
    "            break\n",
    "    \n",
    "    if not has_return_statement:\n",
    "        return 1.0 \n",
    "\n",
    "    docstring_lower = docstring_str.lower()\n",
    "    return_keywords = [\"return\", \"returns\", \"yield\", \"yields\"] \n",
    "    if any(keyword in docstring_lower for keyword in return_keywords):\n",
    "        return 1.0\n",
    "    else:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "42ff3a86-a3fa-47c4-8a1c-028d7466b86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Basic Faithfulness Metric Function ---\n",
    "def calculate_basic_faithfulness(generated_docstring, retrieved_context_text):\n",
    "    \"\"\"\n",
    "    Calculates a basic faithfulness score based on token overlap.\n",
    "    This is a crude proxy for actual faithfulness.\n",
    "    Returns a float (0.0 to 1.0) or None.\n",
    "    \"\"\"\n",
    "    # Simple tokenization and stopword removal\n",
    "    stop_words = set([\"a\", \"an\", \"the\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"do\", \"does\", \"did\", \"will\", \"would\", \"should\", \"can\", \"could\", \"may\", \"might\", \"must\", \"and\", \"or\", \"but\", \"if\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"to\", \"in\", \"on\", \"this\", \"that\", \"it\", \"its\", \"you\", \"your\", \"i\", \"me\", \"my\", \"he\", \"she\", \"him\", \"her\", \"they\", \"them\", \"their\"])\n",
    "    \n",
    "    try:\n",
    "        gen_tokens = set(token.lower() for token in re.findall(r'\\b\\w+\\b', generated_docstring) if token.lower() not in stop_words)\n",
    "        ctx_tokens = set(token.lower() for token in re.findall(r'\\b\\w+\\b', retrieved_context_text) if token.lower() not in stop_words)\n",
    "    except Exception as e:\n",
    "        print(f\"Error tokenizing for faithfulness: {e}\")\n",
    "        return None\n",
    "\n",
    "    if not gen_tokens: # If generated docstring has no valid tokens after filtering\n",
    "        return 0.0 \n",
    "\n",
    "    overlapping_tokens = gen_tokens.intersection(ctx_tokens)\n",
    "    \n",
    "    return len(overlapping_tokens) / len(gen_tokens) if gen_tokens else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a2e6ef2e-a38a-4c93-8aad-a7494425a3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_exception_coverage(code_str, docstring_str):\n",
    "    if not all(isinstance(s, str) for s in [code_str, docstring_str]) or not docstring_str.strip() or docstring_str.startswith((\"# ERROR:\", \"# SKIPPED:\")): return None\n",
    "    raised_exceptions = set(re.findall(r\"raise\\s+(\\w+)\", code_str)) # Basic: finds exception names\n",
    "    if not raised_exceptions: return 1.0 # No exceptions to cover\n",
    "    \n",
    "    docstring_lower = docstring_str.lower()\n",
    "    mentions_raises_section = \"raises:\" in docstring_lower\n",
    "    covered_exceptions = 0\n",
    "    for exc_name in raised_exceptions:\n",
    "        if re.search(r\"\\b\" + re.escape(exc_name.lower()) + r\"\\b\", docstring_lower):\n",
    "            covered_exceptions += 1\n",
    "            \n",
    "    # If a \"Raises:\" section exists, it's good, even if not all specific exceptions are named (simple check)\n",
    "    if mentions_raises_section and raised_exceptions: return 1.0 \n",
    "    if not raised_exceptions: return 1.0 # Should have been caught above\n",
    "    return covered_exceptions / len(raised_exceptions) if raised_exceptions else 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bb1fef2a-470b-4776-9ac2-91a029a9e21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Adherence to Docstring Conventions (Pydocstyle) ---\n",
    "PYDOCSTYLE_ENABLED = True\n",
    "def check_docstring_adherence_pydocstyle(code_str, generated_docstring_content):\n",
    "    \"\"\"\n",
    "    Checks adherence of a generated docstring to PEP 257 using pydocstyle.\n",
    "    The generated_docstring_content should be the *content* of the docstring,\n",
    "    not including the triple quotes.\n",
    "    Returns:\n",
    "        float: A score from 0.0 to 1.0 (1.0 means no errors, 0.0 means many errors).\n",
    "               Returns None if pydocstyle is not enabled or an error occurs.\n",
    "    \"\"\"\n",
    "    # Sanitize content for embedding within triple quotes\n",
    "    safe_content = generated_docstring_content.replace('\\\\', '\\\\\\\\') # Escape backslashes\n",
    "    safe_content = safe_content.replace('\"\"\"', '\\\\\"\\\\\"\\\\\"') # Escape internal triple-double-quotes\n",
    "    safe_content = safe_content.replace(\"'''\", \"\\\\'\\\\'\\\\'\") # Escape internal triple-single-quotes\n",
    "    \n",
    "    # Prepare the content for insertion, ensuring correct indentation for multi-line docstrings\n",
    "    lines = safe_content.split('\\n')\n",
    "    if len(lines) == 1:\n",
    "        # Single line docstring content, no special indentation needed beyond the initial one\n",
    "        indented_docstring_body = lines[0]\n",
    "    else:\n",
    "        # Multi-line: first line as is, subsequent lines indented with 4 spaces\n",
    "        # This assumes the docstring will be placed with an initial 4-space indent.\n",
    "        indented_docstring_body = lines[0] + '\\n' + '\\n'.join(['    ' + line for line in lines[1:]])\n",
    "\n",
    "\n",
    "    # Construct a minimal, valid Python snippet for pydocstyle\n",
    "    # Try to place the docstring correctly within a class or function if identifiable\n",
    "    code_prefix = \"\"\n",
    "    code_suffix = \"\\n    pass\" # Default suffix\n",
    "\n",
    "    class_match = re.search(r\"^(.*\\bclass\\s+\\w+\\s*\\(?.*\\)?:)\", code_str, re.MULTILINE)\n",
    "    func_match = re.search(r\"^(.*\\b(async\\s+)?def\\s+\\w+\\s*\\(?.*\\)?:)\", code_str, re.MULTILINE)\n",
    "\n",
    "    if class_match:\n",
    "        header = class_match.group(1)\n",
    "        # Find the end of the header line to insert the docstring\n",
    "        code_prefix = code_str[:class_match.end()] + f'\\n    \"\"\"{indented_docstring_body}\"\"\"'\n",
    "        code_suffix = code_str[class_match.end():] # The rest of the original class code\n",
    "        # Ensure there's at least a 'pass' or some body if the original was just a header\n",
    "        if not code_suffix.strip() or code_suffix.strip().startswith(\"#\"):\n",
    "            code_suffix = \"\\n    pass\" + code_suffix \n",
    "        code_for_pydocstyle_check = code_prefix + code_suffix\n",
    "\n",
    "    elif func_match:\n",
    "        header = func_match.group(1)\n",
    "        code_prefix = code_str[:func_match.end()] + f'\\n    \"\"\"{indented_docstring_body}\"\"\"'\n",
    "        code_suffix = code_str[func_match.end():]\n",
    "        if not code_suffix.strip() or code_suffix.strip().startswith(\"#\"):\n",
    "            code_suffix = \"\\n    pass\" + code_suffix\n",
    "        code_for_pydocstyle_check = code_prefix + code_suffix\n",
    "    else:\n",
    "        # Fallback: treat as module-level docstring if no class/def found\n",
    "        # This is less ideal as the original code_str might not be a full module\n",
    "        code_for_pydocstyle_check = f'\"\"\"{generated_docstring_content}\"\"\"\\n{code_str}'\n",
    "\n",
    "\n",
    "    errors_count = 0\n",
    "    filtered_errors_count = 0\n",
    "    tmp_file_path = None \n",
    "    try:\n",
    "        with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False, encoding='utf-8') as tmp_file:\n",
    "            tmp_file.write(code_for_pydocstyle_check)\n",
    "            tmp_file_path = tmp_file.name\n",
    "        \n",
    "        command = ['pydocstyle', tmp_file_path]\n",
    "        process = subprocess.run(command, capture_output=True, text=True, encoding='utf-8')\n",
    "        \n",
    "        output = process.stdout.strip()\n",
    "        print\n",
    "        if output:\n",
    "            all_errors = output.splitlines()\n",
    "            errors_count = len(all_errors)\n",
    "            # Filter out D100 (Missing docstring in public module) as it's an artifact\n",
    "            # and D101, D102, D103 if we are only checking the first docstring.\n",
    "            # For now, just D100 as the dummy structure is a module.\n",
    "            filtered_errors = [err for err in all_errors if not err.strip().endswith(\"D100: Missing docstring in public module\")]\n",
    "            filtered_errors_count = len(filtered_errors)\n",
    "        \n",
    "        if process.stderr:\n",
    "            if \"Cannot parse file\" in process.stderr or \"unexpected EOF while parsing\" in process.stderr or \"invalid syntax\" in process.stderr :\n",
    "                 print(f\"Pydocstyle CRITICAL PARSE ERROR for temp file {tmp_file_path}: {process.stderr}\")\n",
    "                 print(\"--- Content written to temp file that failed parsing: ---\")\n",
    "                 print(code_for_pydocstyle_check)\n",
    "                 print(\"--------------------------------------------------------\")\n",
    "                 return 0.0 # Penalize heavily for parse error\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An exception occurred during pydocstyle check: {str(e)}\")\n",
    "        if tmp_file_path and os.path.exists(tmp_file_path): # Check if tmp_file_path was assigned\n",
    "             try:\n",
    "                with open(tmp_file_path, 'r', encoding='utf-8') as f_err:\n",
    "                    print(f\"Content of temp file '{tmp_file_path}' that caused exception:\\n{f_err.read()}\")\n",
    "             except Exception as read_err:\n",
    "                print(f\"Could not read temp file {tmp_file_path}: {read_err}\")\n",
    "        return None # Error during check\n",
    "    finally:\n",
    "        if tmp_file_path and os.path.exists(tmp_file_path):\n",
    "            os.remove(tmp_file_path)\n",
    "    \n",
    "    # Normalize score based on filtered errors.\n",
    "    # Using 10 as the denominator makes the score less harsh than 5.\n",
    "    return max(0.0, 1.0 - (filtered_errors_count / 10.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c416c1fd-30ae-41a9-a7bb-9eebcf2fe2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_coverage_list = []\n",
    "return_coverage_list = []\n",
    "faithfulness_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "028750bd-9e11-45b8-af76-efb47177d3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in data_1.iterrows():\n",
    "    param_coverage_list.append(calculate_parameter_coverage(str(row[\"Code_without_comments\"]), str(row[\"RAG_Docstring\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c5ca2f03-1d59-4f69-bf2a-a3a06c643b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1[\"Parameter_Coverage\"] = param_coverage_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "038f6490-8cf6-4e91-91ca-c382fe9f40ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in data_1.iterrows():\n",
    "    return_coverage_list.append(calculate_return_coverage(str(row[\"Code_without_comments\"]), str(row[\"RAG_Docstring\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "22e25928-033b-4873-ba77-ff0f0e7cac47",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1[\"Return_Coverage\"] = return_coverage_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "58b710da-b124-43a3-9380-92b1662e34ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1[\"Retrieved_Contexts\"] = retrieved_contexts_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "eaae38ed-583a-456a-a0d0-a1d507b459bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in data_1.iterrows():\n",
    "    faithfulness_list.append(calculate_basic_faithfulness(str(row[\"RAG_Docstring\"]), str(row[\"Retrieved_Contexts\"])))\n",
    "    #faithfulness_list.append(faithfulness_score)\n",
    "#if faithfulness_score is not None: print(f\"    -> Basic Faithfulness: {faithfulness_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "91c9b272-3360-4e48-8f1e-9ae1004f6412",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1[\"Faithfulness_Score\"] = faithfulness_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "22a67374-ce1c-45fb-8876-002981773d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "pydocstyle_adherence_list_1 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "71a95624-9282-4fc1-bc72-829d8e998f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "for index, row in data_1.iterrows():\n",
    "    pydocstyle_adherence_list_1.append(check_docstring_adherence_pydocstyle(str(row[\"Code_without_comments\"]), str(row[\"RAG_Docstring\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9f78d7a0-b379-4e06-8dde-2631128a5ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1[\"PythonStyle_Adherence\"] = pydocstyle_adherence_list_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7efec646-00f5-4cd9-b6a4-87c5624a0fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "exception_coverage_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c018d917-7205-45cc-97e4-d95838d77a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in data_1.iterrows():\n",
    "    exception_coverage_list.append(calculate_exception_coverage(str(row[\"Code_without_comments\"]), str(row[\"RAG_Docstring\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2f6ba4b7-a904-4d1f-846f-ae1c378367a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1[\"Exception_Coverage\"] = exception_coverage_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "737000ad-284b-4acd-b260-0e3e41e047ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1.to_excel('./deepseek/self_rag.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "dd89bf12-78cd-4fcd-b8d6-60a266a3a500",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1.to_pickle('./deepseek/self_rag.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91285b71-9dcf-4f59-894b-3514638de302",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phd_env",
   "language": "python",
   "name": "phd_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
