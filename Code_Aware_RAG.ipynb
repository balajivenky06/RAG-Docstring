{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acc349da-a24a-4a27-a092-f9cba5f128ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Code-Aware RAG Implementation for Python Docstring Generation (Refactored)\n",
    "# -----------------------------------------------------------------------------\n",
    "# This script implements the \"Code-Aware Query RAG\" strategy with a focus on\n",
    "# clear, modular functions. It works by:\n",
    "# 1. Parsing code to extract key entities.\n",
    "# 2. Constructing an enriched query.\n",
    "# 3. Retrieving context from Pinecone.\n",
    "# 4. Generating a docstring using a two-stage LLM process.\n",
    "#\n",
    "# Prerequisites:\n",
    "# - Python 3.7+\n",
    "# - Pinecone account (API Key and Environment)\n",
    "# - Ollama installed and running locally\n",
    "# - Ollama models pulled:\n",
    "#   - ollama pull qwen2.5-coder:0.5b\n",
    "#   - ollama pull qwen2.5-coder:1.5b\n",
    "# - Python libraries installed:\n",
    "#   - pip install pinecone-client sentence-transformers requests beautifulsoup4 ollama\n",
    "# -----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f6496d1-131e-4204-9485-c410bb289c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pinecone import Pinecone, ServerlessSpec, PodSpec\n",
    "import ollama\n",
    "import os\n",
    "import uuid\n",
    "import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import re\n",
    "import pandas as pd\n",
    "from rouge import Rouge\n",
    "import pandas as pd\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import nltk\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import openpyxl\n",
    "from bert_score import score\n",
    "import itertools\n",
    "import hf_xet\n",
    "import zlib\n",
    "import subprocess\n",
    "import tempfile\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7378b5c-2f12-43b0-a947-2973891fad10",
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_API_KEY = \"pcsk_71bnuL_HGU1YACobTvL5gJNzHsZG1NMNx3RGmz1ohyC7xMiUYoWnuZpEn5SuvWpuTxnuzm\"\n",
    "PINECONE_ENVIRONMENT = \"us-east-1\"\n",
    "\n",
    "# --- Constants ---\n",
    "INDEX_NAME = \"code-aware-rag-docstring\"\n",
    "EMBEDDING_MODEL = 'all-MiniLM-L6-v2' # HuggingFace sentence transformer\n",
    "OLLAMA_GENERATOR_MODEL = 'deepseek-coder:6.7b' # Local Ollama model name (Ensure this is pulled: `ollama pull qwen2.5-coder:1.5b`)\n",
    "OLLAMA_HELPER_MODEL = 'deepseek-r1:1.5b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c46f716c-ca14-4042-b653-8efc909be530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing services...\n",
      "Embedding model loaded.\n",
      "Pinecone initialized.\n",
      "Ollama client initialized. Attempting to use model: deepseek-coder:6.7b\n",
      "Ensure 'deepseek-coder:6.7b' is available locally in Ollama (`ollama pull deepseek-coder:6.7b`).\n"
     ]
    }
   ],
   "source": [
    "TARGET_URL = [\n",
    "    \"https://peps.python.org/pep-0257/\",\n",
    "    \"https://www.kaggle.com/code/hagzilla/what-are-docstrings\",\n",
    "    \"https://github.com/keleshev/pep257/blob/master/pep257.py\",\n",
    "    \"https://github.com/chadrik/doc484\",\n",
    "    \"https://zerotomastery.io/blog/python-docstring/\",\n",
    "    \"https://google.github.io/styleguide/pyguide.html\",\n",
    "    \"https://www.geeksforgeeks.org/python-docstrings/\",\n",
    "    \"https://pandas.pydata.org/docs/development/contributing_docstring.html\",\n",
    "    \"https://www.coding-guidelines.lftechnology.com/docs/python/docstrings/\",\n",
    "    \"https://realpython.com/python-pep8/\",\n",
    "    \"https://pypi.org/project/AIDocStringGenerator/\",\n",
    "    \"https://www.geeksforgeeks.org/pep-8-coding-style-guide-python/\",\n",
    "    \"https://llego.dev/posts/write-python-docstrings-guide-documenting-functions/\",\n",
    "    \"https://www.datacamp.com/tutorial/pep8-tutorial-python-code\",\n",
    "    \"https://www.programiz.com/python-programming/docstrings\",\n",
    "    \"https://marketplace.visualstudio.com/items?itemName=ShanthoshS.docstring-generator-ext\",\n",
    "    \"https://stackoverflow.com/questions/3898572/what-are-the-most-common-python-docstring-formats\",\n",
    "    \"https://stackoverflow.com/questions/78753860/what-is-the-proper-way-of-including-examples-in-python-docstrings\",\n",
    "    \"https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html\",\n",
    "    \"https://www.dataquest.io/blog/documenting-in-python-with-docstrings/\",\n",
    "    \"https://www.tutorialspoint.com/python/python_docstrings.htm\"\n",
    "]\n",
    "VECTOR_DIMENSION = 384 # Dimension for all-MiniLM-L6-v2\n",
    "METRIC = \"cosine\"\n",
    "CLOUD = \"aws\"\n",
    "REGION = \"us-east-1\"\n",
    "\n",
    "print(\"Initializing services...\")\n",
    "try:\n",
    "    model = SentenceTransformer(EMBEDDING_MODEL)\n",
    "    print(\"Embedding model loaded.\")\n",
    "\n",
    "    # Pinecone\n",
    "    pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "    print(f\"Pinecone initialized.\") # Environment info is handled internally\n",
    "\n",
    "    # Ollama Client\n",
    "    ollama_client = ollama.Client()\n",
    "    print(f\"Ollama client initialized. Attempting to use model: {OLLAMA_GENERATOR_MODEL}\")\n",
    "    print(f\"Ensure '{OLLAMA_GENERATOR_MODEL}' is available locally in Ollama (`ollama pull {OLLAMA_GENERATOR_MODEL}`).\")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing services: {e}\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f88ae1ce-409a-4635-b193-74e27b34fcb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Pinecone indexes: ['fusion-rag-docstring', 'self-rag-docstring', 'rag-docstring', 'corrective-rag-docstring', 'code-aware-rag-docstring']\n",
      "Connecting to existing index 'code-aware-rag-docstring'.\n",
      "Successfully connected to index 'code-aware-rag-docstring'. Stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'metric': 'cosine',\n",
      " 'namespaces': {'': {'vector_count': 14}},\n",
      " 'total_vector_count': 14,\n",
      " 'vector_type': 'dense'}\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Initialize Pinecone ---\n",
    "pinecone_index = None\n",
    "if not PINECONE_API_KEY:\n",
    "    print(\"ERROR: Pinecone API key not found in environment variables.\")\n",
    "    exit(1)\n",
    "try:\n",
    "    pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "    existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]\n",
    "    print(f\"Available Pinecone indexes: {existing_indexes}\")\n",
    "\n",
    "    if INDEX_NAME not in existing_indexes:\n",
    "        print(f\"Index '{INDEX_NAME}' not found. Creating new index...\")\n",
    "        pc.create_index(\n",
    "            name=INDEX_NAME, dimension=VECTOR_DIMENSION, metric=METRIC,\n",
    "            spec=ServerlessSpec(cloud=CLOUD, region=REGION)\n",
    "        )\n",
    "        while not pc.describe_index(INDEX_NAME).status[\"ready\"]:\n",
    "            print(f\"Waiting for index '{INDEX_NAME}' to become ready...\")\n",
    "            time.sleep(5)\n",
    "        print(f\"Index '{INDEX_NAME}' created and ready.\")\n",
    "    else:\n",
    "        print(f\"Connecting to existing index '{INDEX_NAME}'.\")\n",
    "        # Optional: Clear index if you want to re-index fresh\n",
    "        # print(f\"WARNING: Deleting all vectors from existing index '{INDEX_NAME}'...\")\n",
    "        # index_to_clear = pc.Index(INDEX_NAME)\n",
    "        # index_to_clear.delete(delete_all=True)\n",
    "        # print(f\"All vectors deleted from '{INDEX_NAME}'.\")\n",
    "\n",
    "    pinecone_index = pc.Index(INDEX_NAME)\n",
    "    print(f\"Successfully connected to index '{INDEX_NAME}'. Stats: {pinecone_index.describe_index_stats()}\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Failed to initialize or connect to Pinecone index '{INDEX_NAME}': {e}\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81543505-3ca1-47a2-a981-464d20442d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Index already contains 14 vectors. Skipping data loading.\n"
     ]
    }
   ],
   "source": [
    "# --- Load Data into Pinecone (Only if index is empty) ---\n",
    "index_stats = pinecone_index.describe_index_stats()\n",
    "if index_stats.total_vector_count == 0:\n",
    "    total_docs_loaded = 0\n",
    "    # Loop through each URL in the list\n",
    "    for url in TARGET_URL:\n",
    "        print(f\"\\nProcessing URL: {url}\")\n",
    "        try:\n",
    "            # Fetch URL content\n",
    "            response = requests.get(url, timeout=30) # Use timeout\n",
    "            response.raise_for_status() # Check for HTTP errors\n",
    "\n",
    "            # Parse HTML\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            main_content = soup.find('main') or soup.find('article') or soup.find('body')\n",
    "            page_text = \"\"\n",
    "            if main_content:\n",
    "                page_text = main_content.get_text(separator='\\n', strip=True)\n",
    "            else:\n",
    "                page_text = soup.get_text(separator='\\n', strip=True) # Fallback\n",
    "\n",
    "            if not page_text or len(page_text) < 50: # Basic check for meaningful content\n",
    "                print(f\" -> Warning: Could not extract sufficient text content from {url}. Skipping.\")\n",
    "                continue # Skip to the next URL\n",
    "\n",
    "            print(f\" -> Extracted text length: {len(page_text)} characters.\")\n",
    "\n",
    "            # Generate embedding\n",
    "            # Note: Encoding large pages as a single vector might lose detail.\n",
    "            # Chunking the text into smaller parts is better for real applications.\n",
    "            embedding = model.encode(page_text).tolist()\n",
    "\n",
    "            # Prepare and upsert data\n",
    "            doc_id = str(uuid.uuid4())\n",
    "            metadata = {\"text\": page_text, \"source\": url} # Store the specific URL as source\n",
    "\n",
    "            pinecone_index.upsert(vectors=[(doc_id, embedding, metadata)])\n",
    "            print(f\" -> Data from {url} loaded into Pinecone with ID: {doc_id}\")\n",
    "            total_docs_loaded += 1\n",
    "            time.sleep(0.5) # Small delay to be polite to the server\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            # Handle errors fetching specific URL, continue with the next\n",
    "            print(f\" -> Error fetching URL {url}: {e}\")\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            # Handle other errors during processing/upserting for this URL\n",
    "            print(f\" -> Error processing or upserting data for {url}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if total_docs_loaded > 0:\n",
    "        print(\"Waiting a moment for indexing...\")\n",
    "        time.sleep(2)\n",
    "        print(pinecone_index.describe_index_stats()) # Show final stats\n",
    "    else:\n",
    "        print(\"Warning: No documents were loaded into the index.\")\n",
    "\n",
    "else:\n",
    "    print(f\"\\nIndex already contains {index_stats.total_vector_count} vectors. Skipping data loading.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a61594-5e0e-4e40-9e59-bc651f00d9b5",
   "metadata": {},
   "source": [
    "# --- Get User Input ---\n",
    "user_code = (\"\"\"\n",
    "class Conv(Layer):\n",
    "    def __init__(self, rank, filters, kernel_size, strides=1, padding='valid', data_format=None, dilation_rate=1, groups=1, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, trainable=True, name=None, conv_op=None, **kwargs):\n",
    "        super(Conv, self).__init__(trainable=trainable, name=name, activity_regularizer=regularizers.get(activity_regularizer), **kwargs)\n",
    "        self.rank = rank\n",
    "        if isinstance(filters, float):\n",
    "            filters = int(filters)\n",
    "        if filters is not None and filters < 0:\n",
    "            raise ValueError(f'Received a negative value for `filters`.Was expecting a positive value, got {filters}.')\n",
    "        self.filters = filters\n",
    "        self.groups = groups or 1\n",
    "        self.kernel_size = conv_utils.normalize_tuple(kernel_size, rank, 'kernel_size')\n",
    "        self.strides = conv_utils.normalize_tuple(strides, rank, 'strides')\n",
    "        self.padding = conv_utils.normalize_padding(padding)\n",
    "        self.data_format = conv_utils.normalize_data_format(data_format)\n",
    "        self.dilation_rate = conv_utils.normalize_tuple(dilation_rate, rank, 'dilation_rate')\n",
    "        self.activation = activations.get(activation)\n",
    "        self.use_bias = use_bias\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
    "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
    "        self.bias_constraint = constraints.get(bias_constraint)\n",
    "        self.input_spec = InputSpec(min_ndim=self.rank + 2)\n",
    "        self._validate_init()\n",
    "        self._is_causal = self.padding == 'causal'\n",
    "        self._channels_first = self.data_format == 'channels_first'\n",
    "        self._tf_data_format = conv_utils.convert_data_format(self.data_format, self.rank + 2)\n",
    "\n",
    "    def _validate_init(self):\n",
    "        if self.filters is not None and self.filters % self.groups != 0:\n",
    "            raise ValueError('The number of filters must be evenly divisible by the number of groups. Received: groups={}, filters={}'.format(self.groups, self.filters))\n",
    "        if not all(self.kernel_size):\n",
    "            raise ValueError('The argument `kernel_size` cannot contain 0(s). Received: %s' % (self.kernel_size,))\n",
    "        if not all(self.strides):\n",
    "            raise ValueError('The argument `strides` cannot contains 0(s). Received: %s' % (self.strides,))\n",
    "        if self.padding == 'causal' and (not isinstance(self, (Conv1D, SeparableConv1D))):\n",
    "            raise ValueError('Causal padding is only supported for `Conv1D`and `SeparableConv1D`.')\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        input_shape = tensor_shape.TensorShape(input_shape)\n",
    "        input_channel = self._get_input_channel(input_shape)\n",
    "        if input_channel % self.groups != 0:\n",
    "            raise ValueError('The number of input channels must be evenly divisible by the number of groups. Received groups={}, but the input has {} channels (full input shape is {}).'.format(self.groups, input_channel, input_shape))\n",
    "        kernel_shape = self.kernel_size + (input_channel // self.groups, self.filters)\n",
    "        self.kernel = self.add_weight(name='kernel', shape=kernel_shape, initializer=self.kernel_initializer, regularizer=self.kernel_regularizer, constraint=self.kernel_constraint, trainable=True, dtype=self.dtype)\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(name='bias', shape=(self.filters,), initializer=self.bias_initializer, regularizer=self.bias_regularizer, constraint=self.bias_constraint, trainable=True, dtype=self.dtype)\n",
    "        else:\n",
    "            self.bias = None\n",
    "        channel_axis = self._get_channel_axis()\n",
    "        self.input_spec = InputSpec(min_ndim=self.rank + 2, axes={channel_axis: input_channel})\n",
    "        if self.padding == 'causal':\n",
    "            tf_padding = 'VALID'\n",
    "        elif isinstance(self.padding, str):\n",
    "            tf_padding = self.padding.upper()\n",
    "        else:\n",
    "            tf_padding = self.padding\n",
    "        tf_dilations = list(self.dilation_rate)\n",
    "        tf_strides = list(self.strides)\n",
    "        tf_op_name = self.__class__.__name__\n",
    "        if tf_op_name == 'Conv1D':\n",
    "            tf_op_name = 'conv1d'\n",
    "        self._convolution_op = functools.partial(nn_ops.convolution_v2, strides=tf_strides, padding=tf_padding, dilations=tf_dilations, data_format=self._tf_data_format, name=tf_op_name)\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_shape = inputs.shape\n",
    "        if self._is_causal:\n",
    "            inputs = array_ops.pad(inputs, self._compute_causal_padding(inputs))\n",
    "        outputs = self._convolution_op(inputs, self.kernel)\n",
    "        if self.use_bias:\n",
    "            output_rank = outputs.shape.rank\n",
    "            if self.rank == 1 and self._channels_first:\n",
    "                bias = array_ops.reshape(self.bias, (1, self.filters, 1))\n",
    "                outputs += bias\n",
    "            elif output_rank is not None and output_rank > 2 + self.rank:\n",
    "\n",
    "                def _apply_fn(o):\n",
    "                    return nn.bias_add(o, self.bias, data_format=self._tf_data_format)\n",
    "                outputs = conv_utils.squeeze_batch_dims(outputs, _apply_fn, inner_rank=self.rank + 1)\n",
    "            else:\n",
    "                outputs = nn.bias_add(outputs, self.bias, data_format=self._tf_data_format)\n",
    "        if not context.executing_eagerly():\n",
    "            out_shape = self.compute_output_shape(input_shape)\n",
    "            outputs.set_shape(out_shape)\n",
    "        if self.activation is not None:\n",
    "            return self.activation(outputs)\n",
    "        return outputs\n",
    "\n",
    "    def _spatial_output_shape(self, spatial_input_shape):\n",
    "        return [conv_utils.conv_output_length(length, self.kernel_size[i], padding=self.padding, stride=self.strides[i], dilation=self.dilation_rate[i]) for i, length in enumerate(spatial_input_shape)]\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        input_shape = tensor_shape.TensorShape(input_shape).as_list()\n",
    "        batch_rank = len(input_shape) - self.rank - 1\n",
    "        if self.data_format == 'channels_last':\n",
    "            return tensor_shape.TensorShape(input_shape[:batch_rank] + self._spatial_output_shape(input_shape[batch_rank:-1]) + [self.filters])\n",
    "        else:\n",
    "            return tensor_shape.TensorShape(input_shape[:batch_rank] + [self.filters] + self._spatial_output_shape(input_shape[batch_rank + 1:]))\n",
    "\n",
    "    def _recreate_conv_op(self, inputs):\n",
    "        return False\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'filters': self.filters, 'kernel_size': self.kernel_size, 'strides': self.strides, 'padding': self.padding, 'data_format': self.data_format, 'dilation_rate': self.dilation_rate, 'groups': self.groups, 'activation': activations.serialize(self.activation), 'use_bias': self.use_bias, 'kernel_initializer': initializers.serialize(self.kernel_initializer), 'bias_initializer': initializers.serialize(self.bias_initializer), 'kernel_regularizer': regularizers.serialize(self.kernel_regularizer), 'bias_regularizer': regularizers.serialize(self.bias_regularizer), 'activity_regularizer': regularizers.serialize(self.activity_regularizer), 'kernel_constraint': constraints.serialize(self.kernel_constraint), 'bias_constraint': constraints.serialize(self.bias_constraint)}\n",
    "        base_config = super(Conv, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def _compute_causal_padding(self, inputs):\n",
    "        left_pad = self.dilation_rate[0] * (self.kernel_size[0] - 1)\n",
    "        if getattr(inputs.shape, 'ndims', None) is None:\n",
    "            batch_rank = 1\n",
    "        else:\n",
    "            batch_rank = len(inputs.shape) - 2\n",
    "        if self.data_format == 'channels_last':\n",
    "            causal_padding = [[0, 0]] * batch_rank + [[left_pad, 0], [0, 0]]\n",
    "        else:\n",
    "            causal_padding = [[0, 0]] * batch_rank + [[0, 0], [left_pad, 0]]\n",
    "        return causal_padding\n",
    "\n",
    "    def _get_channel_axis(self):\n",
    "        if self.data_format == 'channels_first':\n",
    "            return -1 - self.rank\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "    def _get_input_channel(self, input_shape):\n",
    "        channel_axis = self._get_channel_axis()\n",
    "        if input_shape.dims[channel_axis].value is None:\n",
    "            raise ValueError('The channel dimension of the inputs should be defined. Found `None`.')\n",
    "        return int(input_shape[channel_axis])\n",
    "\n",
    "    def _get_padding_op(self):\n",
    "        if self.padding == 'causal':\n",
    "            op_padding = 'valid'\n",
    "        else:\n",
    "            op_padding = self.padding\n",
    "        if not isinstance(op_padding, (list, tuple)):\n",
    "            op_padding = op_padding.upper()\n",
    "        return op_padding\n",
    "\"\"\")\n",
    "if not user_code.strip():\n",
    "    print(\"No code provided. Exiting.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24396e0b-693e-4f6e-b62b-f4b604261fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_code_for_entities(user_code):\n",
    "    \"\"\"Parses code to extract class name, parents, and public methods.\"\"\"\n",
    "    class_name_match = re.search(r\"class\\s+(\\w+)\", user_code)\n",
    "    parent_match = re.search(r\"class\\s+\\w+\\((.*?)\\):\", user_code)\n",
    "    method_matches = re.findall(r\"def\\s+(_?\\w+)\\s*\\(\", user_code)\n",
    "\n",
    "    entities = {\n",
    "        \"class_name\": class_name_match.group(1) if class_name_match else \"\",\n",
    "        \"parent_classes\": [p.strip() for p in parent_match.group(1).split(',')] if parent_match else [],\n",
    "        \"public_methods\": [m for m in method_matches if not m.startswith('__') or m == '__init__']\n",
    "    }\n",
    "    # Clean up empty strings that might result from splitting\n",
    "    entities[\"parent_classes\"] = [p for p in entities[\"parent_classes\"] if p]\n",
    "    #print('entities', entities)\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8cc1b714-1348-4139-a4c2-9977b22242cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_enriched_query(entities):\n",
    "    \"\"\"Builds a detailed query string from extracted code entities.\"\"\"\n",
    "    query_parts = [\"python docstring conventions and examples\"]\n",
    "    if entities[\"class_name\"]:\n",
    "        query_parts.append(f\"for a class named {entities['class_name']}\")\n",
    "    if entities[\"parent_classes\"]:\n",
    "        query_parts.append(f\"that inherits from {' and '.join(entities['parent_classes'])}\")\n",
    "    if entities[\"public_methods\"]:\n",
    "        query_parts.append(f\"with methods like {', '.join(entities['public_methods'][:3])}\")\n",
    "    return \" \".join(query_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7618fc9c-1558-4219-ba29-d3ab5b9294c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_context(query, pinecone_index, emb_model):\n",
    "    \"\"\"Embeds a query and retrieves context from Pinecone.\"\"\"\n",
    "    if not pinecone_index or not query:\n",
    "        return \"\", \"N/A\", 0\n",
    "    \n",
    "    try:\n",
    "        embedding = emb_model.encode(query).tolist()\n",
    "        results = pinecone_index.query(vector=embedding, top_k=1, include_metadata=True)\n",
    "        if results.matches:\n",
    "            context = results.matches[0].metadata.get('text', '')\n",
    "            #source = results.matches[0].metadata.get('source', 'Pinecone')\n",
    "            return context\n",
    "        else:\n",
    "            return \"\" # Successful query, no matches\n",
    "    except Exception as e:\n",
    "        print(f\"    -> Pinecone retrieval error: {e}\")\n",
    "        # On exception, return default values to prevent crash\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e5974819-685d-4059-8bd1-6629c6107f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def revised_prompt(ctx, helper_model_name):\n",
    "    context = ctx\n",
    "    OLLAMA_REWRITER_MODEL = helper_model_name\n",
    "    rewritten_request = None\n",
    "    rewriter_prompt = f\"\"\"\n",
    "    You are an helpful assistant that refines prompts. Given the following context from the RAG knowledge base along with python code: {context}, generate an optimized prompt for another AI whose sole task is to create a Python docstring for the code and your output.\n",
    "    The optimized prompt should clearly state the task, and subtly incorporate hints from the context if relevant, without necessarily repeating the entire context.\n",
    "    Focus on creating a self-contained, clear instruction for the next AI.\n",
    "    \n",
    "    Generate only the optimized context prompt text for the docstring generation AI.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        rewriter_response = ollama_client.generate(\n",
    "            model=OLLAMA_REWRITER_MODEL,\n",
    "            prompt=rewriter_prompt,\n",
    "            #options={'temperature': 0.3} # Lower temperature for more focused rewriting\n",
    "        )\n",
    "        rewritten_request = rewriter_response.get('response', '').strip()\n",
    "    except Exception as e:\n",
    "        rewritten_request = rewriter_prompt # Ensure it's None on error\n",
    "    return rewritten_request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a14e38f3-275a-42df-b6df-afba2344bb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_content_generation(context, user_code, rewritten_req, ollama_client, OLLAMA_MODEL):\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': 'You are an expert Python programmer tasked with generating docstrings. You will receive context (if found), and the code to document in the final message. Use the context only if it is directly relevant to explaining the provided code. Return only the docstring and dont include the given python code in the output.'}\n",
    "    ]\n",
    "    \n",
    "    # Add the retrieved context as a separate user message, if it exists\n",
    "    if context:\n",
    "        # Include source information in the context message for clarity\n",
    "        messages.append({'role': 'user', 'content': f\"Here is potentially relevant context retrieved from python\\n{user_code}\\n and content :\\n---\\n{rewritten_req}\\n---\"})\n",
    "    else:\n",
    "        # Explicitly state if no context was found or provided\n",
    "        messages.append({'role': 'user', 'content': \"No specific context was retrieved or provided for this request.\"})\n",
    "    \n",
    "    # Add the final user message with the code and the explicit request\n",
    "    messages.append({'role': 'user', 'content': f\"Based on any relevant context provided earlier, generate the Python docstring for the following code:\\n```python\\n{user_code}\\n```\\n\\nOutput *only* the complete docstring content itself, starting with triple quotes. Dont include python codes in the output. You need to generate a single docstring as whole for given python class code.\"})\n",
    "    \n",
    "    try:\n",
    "        response = ollama_client.chat(\n",
    "            model=OLLAMA_MODEL,\n",
    "            messages=messages\n",
    "        )\n",
    "    \n",
    "        generated_docstring = response.get('message', {}).get('content', '').strip()\n",
    "        if generated_docstring.startswith(\"```python\"):\n",
    "            generated_docstring = generated_docstring[len(\"```python\"):].strip()\n",
    "        elif generated_docstring.startswith(\"```\"):\n",
    "             generated_docstring = generated_docstring[len(\"```\"):].strip()\n",
    "    \n",
    "        if generated_docstring.endswith(\"```\"):\n",
    "            generated_docstring = generated_docstring[:-len(\"```\")].strip()\n",
    "    \n",
    "        # Ensure it starts with triple quotes if possible, otherwise print as is\n",
    "        if not (generated_docstring.startswith('\"\"\"') or generated_docstring.startswith(\"'''\")):\n",
    "             print(\"(Model might not have generated a perfectly formatted docstring)\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error communicating with Ollama chat endpoint: {e}\")\n",
    "    #print(generated_docstring)\n",
    "    return generated_docstring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b5c61cdd-dc79-4ae7-a4ef-247080c0e33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_code_aware_rag_pipeline(user_code, pinecone_index, emb_model, ollama_cli, helper_model, generator_model):\n",
    "    \"\"\"Executes the full Code-Aware RAG pipeline by calling helper functions.\"\"\"\n",
    "    #print(\"--- Running Code-Aware RAG pipeline ---\")\n",
    "    \n",
    "    # 1. Parse code\n",
    "    entities = parse_code_for_entities(user_code)\n",
    "    \n",
    "    # 2. Construct enriched query\n",
    "    enriched_query = construct_enriched_query(entities)\n",
    "    enriched_query = enriched_query +  user_code\n",
    "    #print(f\" -> Enriched Query: {enriched_query}\")\n",
    "    \n",
    "    # 3. Retrieve context\n",
    "    retrieved_ctx = retrieve_context(enriched_query, pinecone_index, emb_model)\n",
    "    retrieved_contexts_list.append(retrieved_ctx)\n",
    "    #print(retrieved_ctx)\n",
    "    if retrieved_ctx:\n",
    "        pass\n",
    "    else:\n",
    "        print(\" -> No relevant context was retrieved.\")\n",
    "\n",
    "    rewritten_request = revised_prompt(retrieved_ctx, helper_model)\n",
    "    # 4. Generate docstring\n",
    "    raw_doc = final_content_generation(retrieved_ctx, user_code, rewritten_request, ollama_cli,  generator_model)\n",
    "    #print('raw_doc', raw_doc)\n",
    "    #print('llm_c', llm_c)\n",
    "    return raw_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bc10c902-8c1e-4e54-82e4-e21833fb3040",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_files_df = pd.read_pickle('class_files_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3bc233fb-b322-4652-ac81-d0338895fe2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = class_files_df[\"Comments\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7a6e1189-8213-4f21-ad2b-1c7c3fbb97f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_docstrings_list = []\n",
    "retrieved_contexts_list = []\n",
    "rewritten_contexts_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aadeabe5-5ba6-4ab7-a9b7-ccaec6fffefb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    -> Pinecone retrieval error: Failed to connect; did you specify the correct index name?\n",
      " -> No relevant context was retrieved.\n",
      "(Model might not have generated a perfectly formatted docstring)\n"
     ]
    }
   ],
   "source": [
    "for i, row in class_files_df.iterrows():\n",
    "    user_code = row[\"Code_without_comments\"]\n",
    "    output = run_code_aware_rag_pipeline(user_code=user_code, pinecone_index=pinecone_index, emb_model=model, ollama_cli=ollama_client, helper_model=OLLAMA_HELPER_MODEL, generator_model=OLLAMA_GENERATOR_MODEL)\n",
    "    generated_docstrings_list.append(output)\n",
    "class_files_df[\"RAG_Docstring\"] = generated_docstrings_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "87fced2d-d38e-4686-bac9-3bd8a0ba0d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_rag_docstring(docstring_text):\n",
    "    if not isinstance(docstring_text, str):\n",
    "        return docstring_text\n",
    "\n",
    "    if docstring_text.startswith(\"# ERROR:\") or docstring_text.startswith(\"# SKIPPED:\"):\n",
    "        return docstring_text\n",
    "\n",
    "    text = docstring_text.strip()\n",
    "\n",
    "    if text.startswith(\"```python\"):\n",
    "        text = text[len(\"```python\"):].strip()\n",
    "    elif text.startswith(\"```\"):\n",
    "        text = text[len(\"```\"):].strip()\n",
    "    if text.endswith(\"```\"):\n",
    "        text = text[:-len(\"```\")].strip()\n",
    "\n",
    "    content_inside_quotes = None\n",
    "    first_double_quotes = text.find('\"\"\"')\n",
    "    if first_double_quotes != -1:\n",
    "        last_double_quotes = text.rfind('\"\"\"')\n",
    "        if last_double_quotes > first_double_quotes and (last_double_quotes + 3) <= len(text):\n",
    "            content_inside_quotes = text[first_double_quotes + 3 : last_double_quotes].strip()\n",
    "\n",
    "    if content_inside_quotes is None or not content_inside_quotes.strip():\n",
    "        first_single_quotes = text.find(\"'''\")\n",
    "        if first_single_quotes != -1:\n",
    "            last_single_quotes = text.rfind(\"'''\")\n",
    "            if last_single_quotes > first_single_quotes and (last_single_quotes + 3) <= len(text):\n",
    "                content_inside_quotes = text[first_single_quotes + 3 : last_single_quotes].strip()\n",
    "    \n",
    "    if content_inside_quotes is not None and content_inside_quotes.strip():\n",
    "        final_text_to_clean = content_inside_quotes\n",
    "    else:\n",
    "        final_text_to_clean = text\n",
    "        if final_text_to_clean.startswith('\"\"\"') and final_text_to_clean.endswith('\"\"\"') and len(final_text_to_clean) >= 6:\n",
    "            final_text_to_clean = final_text_to_clean[3:-3].strip()\n",
    "        elif final_text_to_clean.startswith(\"'''\") and final_text_to_clean.endswith(\"'''\") and len(final_text_to_clean) >= 6:\n",
    "            final_text_to_clean = final_text_to_clean[3:-3].strip()\n",
    "\n",
    "    final_text_to_clean = re.sub(r\"(?i)^class\\s+\\w+:\\s*\\n?\", \"\", final_text_to_clean).strip()\n",
    "    \n",
    "    return final_text_to_clean\n",
    "\n",
    "class_files_df[\"RAG_Docstring\"] = class_files_df[\"RAG_Docstring\"].astype(str).apply(clean_rag_docstring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3765c8d3-c1a0-4be8-9a3d-07213536875a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"\"\"\\nAdamax is an extension of the Adam optimization algorithm that includes the element-wise application of the exponential moving average across all training examples, \\nas well as a separate update step on the per-example parameter gradients. \\n\\nThe learning rate schedule for this optimizer can be configured via constructor arguments. Additionally, the class attributes \\'_HAS_AGGREGATE_GRAD\\' and \\'_USES_Nesterov\\' provide information about the optimization algorithm’s characteristics.\\n\\nAttributes:\\n    _HAS_AGGREGATE_GRAD (bool): Flag indicating that Adamax updates are applied on aggregate gradients by iterating over them once. \\n                                 This attribute is constant and its value is True.\\n\\nMethods:\\n    __init__(self, learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-7, name=\\'Adamax\\', **kwargs)\\n        Initializes Adamax with given hyperparameters and default values if not provided. \\n        Stores the learning rate schedule in \\'self._decays\\'. \\n        Sets Adamax parameters to be accessible via get_config() method. \\n\\n    _create_slots(self, var_list)\\n        Adds two types of slots (m and v) for each variable in var_list.\\n        `m` is the slot for the first moment vector, `v` for the second moment vector.\\n\\n    _prepare_local(self, var_device, var_dtype, apply_state)\\n        Prepares Adamax update with local computations such as computing the current learning rate and other \\n        pre-computed values which will be used in the `_resource_apply_dense` and `_resource_apply_sparse` methods.\\n        \\n    _resource_apply_dense(self, grad, var, apply_state=None)\\n        Applies Adamax update for dense tensors using resource variables. \\n        This method is invoked by the optimizer\\'s `apply_gradients()` function.\\n        \\n    _resource_apply_sparse(self, grad, var, indices, apply_state=None)\\n        Applies Adamax update for sparse tensors using resource variables.\\n\\n    get_config(self)\\n        Returns the configuration of this optimizer which includes hyperparameter settings and their current values. \\n\"\"\"',\n",
       " '\"\"\"\\nAgglomerationTransform is a subclass of TransformerMixin that performs data transformation using pooling functions on cluster labels. \\n\\nIt has two main methods: `transform` and `inverse_transform`.\\n\\nThe `transform(self, X)` method transforms the input data \\'X\\' by applying pooling function depending on certain conditions such as if self.pooling_func is np.mean and \\'X\\' is not sparse. \\nIf it meets these conditions, a bincount operation is performed to normalize \\'X\\'. Else, the `self.pooling_func` is applied on each unique label in \\'labels_\\', transposed and returned as an array.\\n\\nThe `inverse_transform(self, X=None, *, Xt=None)` method is used for the inverse transformation of data back to its original form based on the labels. It returns \\'X\\' with inverse permutation applied to it. \\n\\nNote: The instance attribute __metadata_request__inverse_transform is {\\'Xt\\': metadata_routing.UNUSED} which indicates that this transform does not support inverses and raises an error if a request for one is made.\\n\"\"\"',\n",
       " '\"\"\"\\nA subclass of Pooling1D that performs average pooling on 1-dimensional data.\\n\\nArgs:\\n    pool_size (int, optional): The size of the pooling window. Defaults to 2.\\n    strides (optional): The stride length in indices at which to apply the pooling. If None (default), it will default to `pool_size`.\\n    padding (str, optional): One of \"valid\" or \"same\". If \"valid\", no padding is applied. If \"same\", zero-padding will be used so that output has the same length as input. Defaults to \\'valid\\'.\\n    data_format (str, optional): A string, one of `channels_last` (default) or `channels_first`. The ordering of the dimensions in your inputs. Should match the data_format specified in the Input layer used with Keras model being trained. Defaults to \\'channels_last\\'.\\n    **kwargs: Additional keyword arguments for Pooling1D class and backend functions.\\n\"\"\"',\n",
       " '\"\"\"\\nAveragePooling2D is a type of pooling layer that performs average pooling on 2D data. It extends the base Pooling2D class by taking the mean of the input window defined by `pool_size` and `strides`, instead of maximum pooling.\\n\\nThe constructor takes several arguments:\\n- \\'pool_size\\': Tuple of 2 integers, factors by which to downscale (vertical, horizontal). Defaults to (2, 2).\\n- \\'strides\\': Tuple of 2 integers, specifying the strides to use for the pooling operation. If None, it will default to `pool_size`.\\n- \\'padding\\': One of `\"valid\"` or `\"same\"` (case-insensitive). Defaults to `\\'valid\\'`.\\n- \\'data_format\\': A string, one of `channels_last` (default) or `channels_first`. The ordering of the dimensions in your inputs. Should be consistent with the data_format used by your Model. If you pass this argument, it will override any data_format set previously.\\n- \\'**kwargs\\': Any additional keyword arguments that are passed to the base Pooling2D class constructor.\\n\\nExample usage:\\n```python\\nmodel = Sequential()\\nmodel.add(Conv2D(32, (3, 3), activation=\\'relu\\', input_shape=(None, None, 3)))\\nmodel.add(AveragePooling2D((2, 2)))\\n```\\n\"\"\"',\n",
       " '\"\"\"\\nImplementation of AveragePooling3D layer using pooling function avg_pool3d from TensorFlow Keras.\\nInherits from Pooling3D base class and makes use of inherited attributes and methods. \\n\\nParameters:\\n- `pool_size` (tuple): The size of the pooling windows in each dimension, default is (2, 2, 2).\\n- `strides` : If None, it defaults to pool_size. An optional tuple specifying the dilation rate to be used for dilated convolution.\\n- `padding` (string): \\'valid\\' or \\'same\\', no case sensitive. \\'valid\\': doesn’t add padding. \\'same\\': adds padding evenly to left/right and up/down of input without cropping, default is \\'valid\\'. \\n- `data_format` : A string, one of \"channels_last\" (default) or \"channels_first\". The ordering of the dimensions in the inputs. \\n\\nMethods:\\n- `__init__(self, pool_size=(2, 2, 2), strides=None, padding=\\'valid\\', data_format=None, **kwargs)` : Initialize AveragePooling3D layer with specified parameters and makes use of super class Pooling3D\\'s init method.\\n\"\"\"',\n",
       " '\"\"\"\\nGaussianMixtureExpert is an expert system designed to model and fit Gaussian Mixtures using Expectation-Maximization (EM) algorithm. The Gaussian Mixture can be defined by its weights, means, degrees of freedom (in case of Wishart distribution), precision matrices (Cholesky decomposed), and covariances (if \\'tied\\' or \\'diag\\').\\n\\nThe class has methods for the following tasks:\\n- Initialization: Constructs a Gaussian Mixture with specified parameters. If no parameters are provided, default values are used. \\n- Checking parameters: Checks if the given parameters meet the constraints of the model (e.g., positive degrees of freedom).\\n- Fitting: Uses Expectation-Maximization to find the best fit for the Gaussian Mixture to a dataset. This involves calculating responsibilities, performing an E-step and M-step, updating the parameters based on these calculations, and checking for convergence.\\n- Prediction: Given a set of features, predicts which mixture component each sample most likely belongs to by computing the log probabilities of belonging to each Gaussian in the mixture.\\n- Scoring: Computes the likelihood of the data given the current parameter values (useful for checking convergence). \\n\\nThe class also includes methods for handling model selection criteria such as Bayesian information criterion (BIC) and Akaike information criterion (AIC), which can be used to select the number of components in the mixture.\\n\\nGaussianMixtureExpert is designed to handle high dimensional data, and it allows for flexibility in specifying the covariance structure of the Gaussians (full, tied or diag). However, it does not currently support sparse data.\\n\"\"\"',\n",
       " '\"\"\"\\nConv is a subclass of Layer that implements Convolution layer in a neural network. \\nIt takes several parameters like rank, filters, kernel_size etc., which define the properties and behavior of the layer.\\n\\nParameters:\\n    - `rank` (int): An integer defining the number of spatial dimensions. It can be either 1 or 2.\\n    - `filters` (int): The dimensionality of the output space for Convolution layer.\\n    - `kernel_size` (tuple, int): An integer or tuple/list of integers that define the width and height of the kernel.\\n    - `strides` (tuple, int, optional): An integer or tuple/list of integers that determine the stride values for each axis in input space. Defaults to 1.\\n    - `padding` (str, optional): A string from \\'valid\\' and \\'same\\'. The padding method used when applying Convolution. Defaults to \\'valid\\'.\\n    - `data_format` (str, optional): A string, one of \"channels_last\" or \"channels_first\". The ordering of the dimensions in your inputs. Defaults to None.\\n    - `dilation_rate` (tuple, int, optional): An integer or tuple/list of integers that define the dilation rates to use for dilated convolution. Defaults to 1.\\n    - `groups` (int, optional): A positive integer. The number of blocked connections from input channels to the output. Defaults to 1.\\n    - `activation` (function or string, optional): Activation function to be used by the layer. If you don\\'t specify anything, no activation is applied (ie. \"linear\" activation: a(x) = x). Defaults to None.\\n    - `use_bias` (bool, optional): Whether the layer uses a bias vector. Set to False for instance in order to share weights between inputs and recurrent cells. Defaults to True.\\n    - `kernel_initializer` (function or string, optional): Initializer for the kernel weights matrix. Defaults to \\'glorot_uniform\\'.\\n    - `bias_initializer` (function or string, optional): Initializer for the bias vector. Defaults to \\'zeros\\'.\\n    - `kernel_regularizer` (function or string, optional): Regularizer function applied to the kernel matrix. Defaults to None.\\n    - `bias_regularizer` (function or string, optional): Regularizer function applied to the bias vector. Defaults to None.\\n    - `activity_regularizer` (function or string, optional): Regularizer function applied to the output of the layer. Defaults to None.\\n    - `kernel_constraint` (function or string, optional): Constraint function applied to the kernel matrix. Defaults to None.\\n    - `bias_constraint` (function or string, optional): Constraint function applied to the bias vector. Defaults to None.\\n    - `trainable` (bool, optional): If True, weights of this layer will be trainable. Defaults to True.\\n    - `name` (str, optional): Name for the tensor. Defaults to None.\\n    - `conv_op` (function, optional): Convolution operation function that takes inputs and kernel as input and gives out the convolution result. Defaults to None.\\n    \\nThe layer can be initialized with specific parameters and it is also possible to change its weights after initialization. The main methods of this class are build, call, compute_output_shape. \\n\"\"\"',\n",
       " '\"\"\"\\nConv1D is a subclass of Conv which defines one-dimensional convolution operation. \\nIt takes several parameters such as number of filters, kernel size, strides, padding style, data format, dilation rate, groups and activation function to name a few. It also includes options for using bias, initialization of kernels and biases, regularization of the weights and constraints on the values.\\n\\nThe __init__ method initializes an instance with parameters specified by user input. The superclass Conv is initialized with additional parameters: rank (which is 1 indicating a one-dimensional convolution), and keyword arguments for other options like activation function, bias etc.\\n\"\"\"',\n",
       " '\"\"\"\\nConv1DTranspose is a 1-dimensional convolutional layer that performs a transposed convolution on its input. It is designed specifically for temporal data, such as time series or sequences of vectors (like text documents).\\n\\nParameters:\\nfilters (int): Number of convolution kernels to use.\\nkernel_size (int): Length of the kernel/filter used in the convolution.\\nstrides (int, optional): Determines the stride length of the convolution. Defaults to 1.\\npadding (str, optional): Type of padding to be done by the layer. \\'valid\\' or \\'same\\'. Defaults to \\'valid\\'.\\noutput_padding (None, optional): Additional size added to the output shape at the end of the convolution. Defaults to None.\\ndata_format (str, optional): \\'channels_first\\' or \\'channels_last\\', indicating whether the channel dimension is the first or last dimension in your inputs. Defaults to None.\\ndilation_rate (int, optional): Dilation rate to use for dilated convolution. Defaults to 1.\\nactivation (None, optional): Name of a built-in activation function to apply at the output of this layer. Defaults to None.\\nuse_bias (bool, optional): Boolean, whether the layer uses a bias vector. Defaults to True.\\nkernel_initializer (str, optional): Initializer for the kernel weights matrix. Defaults to \\'glorot_uniform\\'.\\nbias_initializer (str, optional): Initializer for the bias vector. Defaults to \\'zeros\\'.\\nkernel_regularizer (None, optional): Regularization function applied to the kernel weights matrix. Defaults to None.\\nbias_regularizer (None, optional): Regularization function applied to the bias vector. Defaults to None.\\nactivity_regularizer (None, optional): Regularization function applied to the output of the layer. Defaults to None.\\nkernel_constraint (None, optional): Constraint function that is used to maintain the kernel weights matrix. Defaults to None.\\nbias_constraint (None, optional): Constraint function that is used to maintain the bias vector. Defaults to None.\\n**kwargs: Additional keyword arguments passed to the layer instance. \\n\\nAttributes:\\noutput_padding (int or None): See parameter \\'output_padding\\' above.\\ninput_spec (InputSpec of list): Input specification of the layer, providing information about its input shape and dtype.\\nkernel (Variable): Instance of Variable containing the kernel weights matrix of this layer.\\nbias (Variable or None): Instance of Variable containing the bias vector of this layer if use_bias is True else None.\\nbuilt (bool): Whether the layer has been built yet.\\n\\nMethods:\\nbuild(input_shape): Builds the layer, initializing its weights and biases based on the input shape provided.\\ncall(inputs): Applies the layer to its inputs, returning a tensor that is the result of applying the transposed convolution operation to the inputs.\\ncompute_output_shape(input_shape): Calculates the output shape of the layer given an input shape.\\nget_config(): Returns the configuration dictionary of this layer which includes all parameters and their values in a human-readable format.\\n\"\"\"',\n",
       " '\"\"\"\\nConv2D is a subclass of Conv that implements two-dimensional convolution. \\n\\n__init__(self, filters, kernel_size, strides=(1, 1), padding=\\'valid\\', data_format=None, dilation_rate=(1, 1), groups=1, activation=None, use_bias=True, kernel_initializer=\\'glorot_uniform\\', bias_initializer=\\'zeros\\', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, **kwargs)\\n    Initializes Conv2D with the given parameters. \\n\\nParameters:\\n    filters (int): Number of convolutional filters to use.\\n    kernel_size (tuple or int): The height and width of the convolution window. Can be an integer to specify a square window.\\n    strides (tuple or int, optional): Stride values for the x and y dimensions of the input tensor. Defaults to (1, 1).\\n    padding (str, optional): Padding technique to use - \\'valid\\' or \\'same\\'. Defaults to \\'valid\\'.\\n    data_format (NoneType, optional): String, either \"channels_first\" or \"channels_last\". Defaults to None.\\n    dilation_rate (tuple or int, optional): Dilation rate for the input tensor. Defaults to (1, 1).\\n    groups (int, optional): Number of blocked connections from input units to the hidden layer units. Defaults to 1.\\n    activation (NoneType, optional): Activation function to use - a callable or one of its string identifiers. If None, no activation is applied. Defaults to None.\\n    use_bias (bool, optional): Whether to include a bias term in the convolutional layer. Defaults to True.\\n    kernel_initializer (str, optional): Initializer for the \\'kernel\\' weights matrix. Defaults to \\'glorot_uniform\\'.\\n    bias_initializer (str, optional): Initializer for the bias vector. Defaults to \\'zeros\\'.\\n    kernel_regularizer (NoneType or callable, optional): Regularizer function applied to the \\'kernel\\' weights matrix. If a non-zero regularization rate is specified, apply this regularization term on the loss during training. Defaults to None.\\n    bias_regularizer (NoneType or callable, optional): Regularizer function applied to the bias vector. If a non-zero regularization rate is specified, apply this regularization term on the loss during training. Defaults to None.\\n    activity_regularizer (NoneType or callable, optional): Regularizer function applied to the output of the layer. If a non-zero regularization rate is specified, apply this regularization term on the loss during training. Defaults to None.\\n    kernel_constraint (NoneType or callable, optional): Constraint function applied to the \\'kernel\\' weights matrix. Defaults to None.\\n    bias_constraint (NoneType or callable, optional): Constraint function applied to the bias vector. Defaults to None. \\n\\nAll other keyword arguments are passed through to the base Layer constructor.\\n\"\"\"',\n",
       " '\"\"\"\\nConv2DTranspose is a type of convolutional layer used in Convolutional Neural Networks (CNNs). It\\'s essentially a deconvolution operation, calculating how much each input pixel influences the next layer. The \\'transpose\\' part refers to the fact that instead of transforming the input volume like typical convolutions do, it transforms the output volume back into the input space.\\n\\n__init__(filters, kernel_size, strides=(1, 1), padding=\\'valid\\', output_padding=None, data_format=None, dilation_rate=(1, 1), activation=None, use_bias=True, kernel_initializer=\\'glorot_uniform\\', bias_initializer=\\'zeros\\', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, **kwargs)\\nInitializes the Conv2DTranspose layer. It accepts arguments for number of filters, kernel size, strides, padding style and other hyperparameters as per TensorFlow\\'s convolutional layers in keras. \\n\\nbuild(input_shape)\\nCalled when creating the layer during build phase. Validates the input shape and initializes weights if they are None.\\n\\ncall(inputs)\\nDefines how to compute output of this layer for given inputs. Contains core logic - convolution with transpose operation, activation and bias addition if used.\\n\\ncompute_output_shape(input_shape)\\nCalculates the output shape based on input shape, kernel size, strides etc. \\n\\nget_config()\\nReturns a dictionary containing configuration of layer which can be loaded later using from_config method.\\n\"\"\"',\n",
       " '\"\"\"\\n3D Convolutional Layer Class, Inherits from Conv Base Class.\\n\\nThe `Conv3D` class represents a 3D convolutional layer in a deep learning model. This is used for spatial or spatio-temporal data. It takes several parameters including the number of filters, kernel size and stride values to define the filter\\'s properties. Other parameters include padding type (valid/same), data format, dilation rate, groups, activation function, use of bias, initializers, regularizers and constraints.\\n\\nParameters:\\n- `filters` (int): Number of convolutional filters in layer.\\n- `kernel_size` (tuple or int): Height and width of the kernel. Can be an integer to specify a square kernel.\\n- `strides` (tuple, optional): Stride values for height and width dimensions. Default is (1, 1, 1).\\n- `padding` (string, \\'valid\\' or \\'same\\'), Padding method, \\'valid\\' means no padding, \\'same\\' results in padding so that the output has the same length as the original input.\\n- `data_format`: A string, one of \"channels_last\" (default) or \"channels_first\". The ordering of the dimensions in the inputs. \\n- `dilation_rate` (tuple, optional): Dilation rate to use for dilated convolution. Default is (1, 1, 1).\\n- `groups` (int), Group factor for the Convlution layers.\\n- `activation`: Activation function to use. If you don\\'t specify anything, no activation is applied (ie \"linear\" activation: a(x) = x).\\n- `use_bias` (boolean): Whether the layer uses a bias vector.\\n- `kernel_initializer`: Initializer for the kernel weights matrix. \\n- `bias_initializer` : Initializer for the bias vector. \\n- `kernel_regularizer` : Regularizer function applied to the kernel.\\n- `bias_regularizer` : Regularizer function applied to the bias vector.\\n- `activity_regularizer`: Regularizer function applied to the output of the layer (its \"activation\"). \\n- `kernel_constraint`: Constraint function applied to the kernel.\\n- `bias_constraint`: Constraint function applied to the bias vector.\\n\\nExceptions and Side Effects:\\nThe class can throw exceptions if invalid parameters are passed in, such as incorrect data format, or unsupported values for stride/dilation rate etc. It doesn\\'t have a side effect on its own but it uses an activation function from Keras Activations module which can be stateful and should not be re-used across different instances of the layer without resetting their states.\\n\"\"\"',\n",
       " '\"\"\"\\nConv3DTranspose is a type of Convolutional layer which performs 3-dimensional convolution operation on inputs, but unlike normal Conv2D or Conv1D layers, it performs the transpose convolution. This means that it can increase the depth (number of filters) and upscale the input volume in 3D space.\\n\\nThe __init__ method initializes the parameters for Conv3DTranspose layer including filter count, kernel size, stride, padding style, output padding, data format, activation function to be used, whether to use bias or not, kernel initializer, bias initializer, regularizers and constraints related to the weights.\\n\\nThe build method is called when the model is built, which prepares all necessary tensors by calling `add_weight` for convolutional kernels and biases (if applicable). It also sets the input specifications based on the shape of inputs.\\n\\nThe call method performs the forward pass through the layer, which includes calculating the output shape, performing the transposed 3D convolution operation with the calculated strides, adding a bias if available and applying an activation function if specified. The outputs are then returned.\\n\\nThe compute_output_shape method calculates the output shape of the tensor(s) created by apply this layer to input. This is used as a guide for shaping purposes when building subsequent layers (for instance, it\\'s useful for deciding how to reshape your inputs in a `Conv2D` layer).\\n\\nThe get_config method provides configurations which can be reloaded later using Model.from_config(get_config()). This is useful while saving and loading the model.\\n\"\"\"',\n",
       " '\"\"\"\\nCropping1D Layer implementation for 1-dimensional convolutional layers.\\n\\nThis layer crops the input according to the `cropping` value provided at initialization, treating\\neach element in the second dimension (axis=1) as a temporal sequence. For instance,\\nwith a `cropping` value of (2, 3), sequences from the start will have their first two elements removed, and sequences from the end will lose their last three elements. This is often used in time series data to remove irrelevant initial or final data points.\\n\\nThe layer computes the output shape by subtracting `cropping` values from the input_shape \\nalong the second dimension and keeps other dimensions unchanged.\\n\\n# Arguments\\n    cropping: tuple of 2 integers, specifying how many elements to remove\\n        at the start and end of the sequence for each sample.\\n        `cropping=(1, 1)` results in removing the first and last element\\n        from each sample.\\n\\n# Returns\\n    A tensor with cropped sequences.\\n\\n# References\\n    - [Convolutional Layers in Keras](https://keras.io/api/layers/convolution_layers/)\\n\"\"\"',\n",
       " '\"\"\"\\nA Keras Layer that crops an input tensor along its spatial dimensions (height and width).\\n\\nAttributes:\\n    cropping (tuple of 2 tuples): Amounts of padding to add around the height and width dimensions, respectively. If a single integer, it is symmetrically used for both dimensions. Defaults to ((0, 0), (0, 0)).\\n    data_format (str): A string, one of `channels_last` or `channels_first`. The ordering of the dimensions in the inputs. `channels_last` corresponds to inputs with shape `(batch_size, height, width, channels)` while `channels_first` corresponds to inputs with shape `(batch_size, channels, height, width)`. It defaults to None. If it is None, it means that the ordering should be decided by the values of strides at runtime.\\n    \\nMethods:\\n    __init__(self, cropping=((0, 0), (0, 0)), data_format=None, **kwargs): The constructor for Cropping2D class.\\n    compute_output_shape(self, input_shape): Returns the shape of the output tensor when the provided input is passed through a layer.\\n    call(self, inputs): This method implements the computation to be performed at every call.\\n    get_config(self): A dictionary containing configuration parameters for this Layer. It\\'s useful for saving and loading model configurations. \\n\"\"\"',\n",
       " '\"\"\"\\nCropping3D is a subclass of Keras Layer that performs cropping on input 3-dimensional data. It is typically used with convolutional layers and image processing tasks.\\n\\nThe `__init__` method initializes the Cropping3D layer by setting up the cropping value(s) and the data format. The cropping can be specified as a tuple of (left_crop, right_crop), where left_crop is the number of values to remove from the beginning, and right_crop is the number of values to remove from the end.\\n\\nThe `compute_output_shape` method calculates the output shape after cropping has been applied, based on the input data\\'s current shape. The method takes into account whether the data format being used is \\'channels_first\\' or \\'channels_last\\'.\\n\\nThe `call` function performs the actual cropping operation by slicing the input tensor along each axis. If the data format is \\'channels_first\\', it crops first two dimensions (dimensions 2 and 3) and if data format is \\'channels_last\\', it crops last three dimensions (dimensions 1, 2, and 3).\\n\\nThe `get_config` function returns a dictionary containing the configuration of this layer. This includes the cropping values and the data format used by this layer.\\n\"\"\"',\n",
       " '\"\"\"\\nDBSCAN (Density-Based Spatial Clustering of Applications with Noise) is an algorithm that can be used for clustering data into high density and low density regions. It works by finding a suitable density threshold to separate the dense clusters from each other.\\n\\nParameters:\\neps : float, optional\\n    The maximum distance between two samples for them to be considered as in the same neighborhood.\\nmin_samples : int, optional\\n    The number of samples in a neighborhood for a point to be considered as a core point. This includes the point itself.\\nmetric : string or callable, default: \\'euclidean\\'\\n    The metric to use when calculating distance between instances in a feature array. If metric is a string, it must be one of the options allowed by sklearn.metrics.pairwise_distances for observations of 2-D arrays. If metric is \"precomputed\", X is assumed to be a distance matrix and must be square.\\nalgorithm : {\\'auto\\', \\'ball_tree\\', \\'kd_tree\\', \\'brute\\'}, default: \\'auto\\'\\n    The algorithm to be used by the NearestNeighbors module.\\nleaf_size : int, optional\\n    Leaf size passed to BallTree or cKDTree. \\np : float, optional\\n    Power parameter for the Minkowski metric. When p=1, this is equivalent to using曼哈顿（l1）距离， and when p=2 it is equivalent to having 欧几里德（l2）距离. For arbitrary p, Minkowski distance is used.\\nn_jobs : int or None, optional (default=None)\\n    The number of parallel jobs to run for neighbors search. If None, then the result will not be parallelized. \\n\\nMethods:\\nfit(X, y=None, sample_weight=None):\\n    Computes clustering on X and assigns labels.\\nfit_predict(X, y=None, sample_weight=None):\\n    Performs training of the model followed by prediction.\\n_more_tags():\\n    Returns metadata about the model in a dictionary-like object that\\'s compatible with the \\'sklearn.utils.Bunch\\' class. \\n\"\"\"',\n",
       " '\"\"\"\\nDepthwiseConv2D is a type of Convolutional Neural Network (CNN) layer that performs 2-D depthwise convolution on inputs before applying pointwise convolution and adding optional bias. The operation is performed in the frequency domain, unlike traditional Conv layers which operate in the spatial domain. This makes DepthwiseConv2D more memory efficient when working with large input data volumes.\\n\\nParameters:\\nkernel_size : int or tuple of 2ints, specifying the height and width of the 2-D convolution window. Can be a single integer to specify the same value for all spatial dimensions.\\nstrides : int or tuple of 2ints, specifying the strides of the convolution along the height and width. If a single integer is used, it specifies both stride values.\\npadding : one of \"valid\" or \"same\", case-insensitive. Padding to be applied before convolution.\\ndepth_multiplier : int, number of depthwise convolution output channels for each input channel.\\ndata_format : string, data formatting. Either \\'channels_first\\' or \\'channels_last\\'.\\ndilation_rate : tuple of 2ints, specifying the dilation rate to use for dilated convolution. Can be a single integer to specify the same value for all spatial dimensions.\\nactivation : Activation function to use. If you don\\'t specify anything, no activation is applied (ie. \\'linear\\' activation: a(x) = x).\\nuse_bias : Boolean, whether the layer uses a bias vector.\\ndepthwise_initializer : Initializer for the depthwise convolution kernel weights. If None, random normal initializer will be used.\\nbias_initializer : Initializer for the bias vectors. If None, zeros initializer will be used.\\ndepthwise_regularizer : Regularizer function applied to the depthwise convolution kernel weights matrix. If a regularizer is provided, it will be applied during training.\\nbias_regularizer : Regularizer function applied to the bias vector. If a regularizer is provided, it will be applied during training.\\nactivity_regularizer : Regularizer function applied to the output of the layer. If a regularizer is provided, it will be applied during training.\\ndepthwise_constraint : Constraint function applied to the depthwise kernel. If you pass a constraint, it will be enforced when updating the variables.\\nbias_constraint : Constraint function that will be applied to bias vector. If used with a constraint, the bias vector elements will be kept within the range(s) given by the constraint(s). \\nkwargs : additional keyword arguments for base layer class.\\n\\nReturns:\\nOutput tensor after applying DepthwiseConv2D operation.\\n\\nRaises:\\nValueError: if input_shape does not have a rank of 4.\\n\"\"\"',\n",
       " '\"\"\"\\nEmbedding Layer Class\\n\\nThe Embedding layer takes integer inputs and maps them to dense embeddings of fixed size. This is useful for representing words or other sequence of elements with continuous vectors, which can be beneficial for tasks such as text classification, information retrieval or neural machine translation.\\n\\nAttributes:\\n    input_dim (int): Size of the vocabulary, i.e., maximum integer index + 1.\\n    output_dim (int): Dimension of the dense embedding.\\n    embeddings_initializer : Initializer for the `embedding` weights.\\n        Default is \\'uniform\\'.\\n    embeddings_regularizer: Regularizer function applied to the `embedding` weights.\\n    activity_regularizer: Regularizer function applied to the output of the layer.\\n    embeddings_constraint: Constraint function applied to the `embedding` weights.\\n    mask_zero (bool): If True, padding values are set to 0 and masked out during training.\\n    input_length (int or None): Length of input sequences, when it is statically known.\\n        This allows the layer to properly compute gradients.\\n\\nMethods:\\n    `build(input_shape)` : Called by the backend at the start of `fit()` with the shape of data being fed into the model. Note that this method does not take any arguments. Instead, the input shape is passed in via kwargs.\\n        `compute_mask(inputs, mask=None)`: If self.mask_zero is True, this will ensure masks are created properly. This means sequences that are all zeros will have their sum as False (because of not-equal condition).\\n    `call(inputs)` : Called by the backend when executing a model using `predict()` or `train_on_batch()`, etc. It should implement what the layer does in an way that\\'s compatible with whatever other layers you are using.\\n        `compute_output_shape(input_shape)`: Return the shape of the output tensor. This method is only called if the layer has a defined batch input shape and is either the first layer in a model or follows an activity regularization layer (or its V1 equivalent).\\n    `get_config()` : Returns the configuration dictionary of a layer. It\\'s used for saving and loading model configurations.\\n\"\"\"',\n",
       " 'Sure, here is the docstring for your Python Flask application:\\n\\n```python\\n\"\"\"\\nFlask Application Main Class\\n-------------------------------\\nThis module contains the main Flask class that serves as the entry point of the web application. \\nIt is responsible for handling HTTP requests and responses in a flexible manner, serving as both the server \\nand the dispatcher of incoming request to functions (controller actions).\\n\\nAttributes:\\n    _sentinel (object): A special sentinel object used by various Python libraries to indicate the absence \\n                        of value. It\\'s typically used as default values in function definitions and is never itself a valid input.\\n\\nMethods:\\n    handle_user_exception(e) -> Response: Handles user exceptions raised inside request context, generating an error response with status code 500 by default.\\n    handle_http_exception(exc) -> Response: This method handles HTTP exception which is an instance of Werkzeug\\'s `HTTPException` class. It builds a response based on the information provided by that exception and sets the appropriate headers.\\n    should_ignore_error(self, error) -> bool: Determines if an error should be ignored or not. This method checks whether the current application is in debug mode and if the error is of type `AssertionError`. If both conditions are met, it returns True meaning that exception handling will proceed to next handler unless this method itself returns False.\\n    register_blueprint(self, blueprint) -> None: Registers a Blueprint on the application. This can be done in several ways and each of them is handled by separate methods with specific rules about what kind of arguments are accepted.\\n    add_url_rule(self, rule, endpoint=None, view_func=None, **options) -> None: Adds a new route to the application’s routing table. This method can handle both simple and more complex URL schemes.\\n    errorhandler(self, code_or_exception) -> decorator: Decorates a function as an error handler for specified HTTP status codes or exceptions.\\n    teardown_request(self, func) -> None: Registers a function to be executed after each request that is processed by the application. This can be used for cleanup purposes like closing database connections.\\n    teardown_appcontext(self, func) -> None: Decorates a function and registers it as an application context teardown handler which gets called when the current application context tears down.\\n    url_value_preprocessor(when=None): A decorator that can be used to register functions that pre-process values of URL variables. These are run after routing has been determined, but before the view function is called.\\n    before_request(self, func) -> None: Registers a function as being executed before every request, and if this function returns a non-None value, it will be used as the response to that request.\\n    after_request(self, func) -> None: Registers a function that will be called just after a route has been called. This can be used for tasks like logging requests or adding CORS headers.\\n    register_error_handler(code, handler) -> None: Provides an interface to handle custom error responses by HTTP status code. \\n    run(self, host=None, port=None, debug=None, load_dotenv=True, **options) -> None: Runs the application on a local development server. It starts Flask\\'s built-in development server but can also be configured to use WSGI servers like Gunicorn or uWSGI.\\n\"\"\"\\n```\\nThis docstring provides an overview of the main class, its attributes and methods, their usage, as well as any exceptions that are likely to occur during execution.',\n",
       " '\"\"\"\\nThis is an implementation of FunctionTransformer, which applies functions to data in a pipeline or other models. \\nIt allows you to specify one-to-one mapping from inputs to outputs and back, optionally validating the transformation and its inverse.\\nThe transformations can be specified by passing callable function objects as `func` and/or `inverse_func` during initialization.\\nFunctionTransformer also has several parameters for controlling its behavior such as \\'validate\\', \\'accept_sparse\\', \\n\\'check_inverse\\', \\'feature_names_out\\', \\'kw_args\\', and \\'inv_kw_args\\'. It provides methods for transforming data, validating it, \\nand getting feature names. The transformation function can return either a pandas DataFrame or polars Series depending on the set output configuration.\\n\"\"\"',\n",
       " '\"\"\"\\nGaussianMixture Class\\n\\nThis class extends BaseMixture and represents Gaussian mixture models. It is used for fitting and predicting data using the Expectation-Maximization algorithm. The model assumes that the data points are generated from a Gaussian distribution with unknown parameters (mean, covariance). \\n\\nParameters:\\n    n_components (int): Number of Gaussian components in the mixture model. Defaults to 1.\\n    \\n    covariance_type (str): String describing the type of covariance parameterization. It can be either \\'full\\', \\'tied\\', \\'diag\\' or \\'spherical\\'. Defaults to \\'full\\'.\\n        \\n    weights_init, means_init, precisions_init: Initial parameters for mixture model components if known. \\n    \\n    random_state (int): Seed for the random number generator. Useful for debugging. Defaults to None.\\n    \\n    warm_start (bool): If True, use previous solution as initialization for new run. Defaults to False.\\n        \\n    verbose (int): The verbosity level of the fitting procedure. Defaults to 0.\\n        \\n    verbose_interval (float): Time in seconds or an integer representing the number of iterations between status messages being printed. Only effective when verbose is not 0. Defaults to 10.\\n    \\nMethods:\\n    _check_parameters(X)\\n    _initialize_parameters(X, random_state)\\n    _m_step(X, log_resp)\\n    _estimate_log_prob(X)\\n    _compute_lower_bound(_, log_prob_norm)\\n    _get_parameters()\\n    _set_parameters(params)\\n    _n_parameters()\\n    \\nNotes:\\n    The model can be updated using the \\'fit\\' method and parameters of interest calculated using the \\'score\\', \\n    \\'predict\\', or \\'sample methods. It uses an Expectation-Maximization approach to find the mixture components that best fit the data.\\n\"\"\"',\n",
       " '\"\"\"\\nGlobalAveragePooling1D is a Python class that extends GlobalPooling1D by providing global average pooling operations on 1D inputs.\\n\\nParameters:\\n- data_format : A string, \\'channels_last\\' or \\'channels_first\\'. Defaults to \\'channels_last\\'. The ordering of the dimensions in the inputs.\\n- **kwargs : Additional keyword arguments for a Layer initialization.\\n\\nAttributes:\\n- supports_masking : Boolean indicating if the layer can handle masked input. Set to True.\\n\\nMethods:\\n- __init__(data_format=\\'channels_last\\', **kwargs) : Initializes the GlobalAveragePooling1D class with given data format and additional keyword arguments.\\n- call(inputs, mask=None) : Performs pooling operation on 1D inputs. If a mask is provided, it multiplies the input by the mask before performing average pooling. Returns either mean or weighted average depending on whether a mask was provided.\\n- compute_mask(inputs, mask=None) : Not used in this layer. Always returns None.\\n\"\"\"',\n",
       " '\"\"\"\\nGlobalAveragePooling2D is an extension of GlobalPooling2D that performs average pooling on 2D data. The implementation differs based on whether the \\'channels_last\\' or \\'channels_first\\' data format is used. This docstring provides full documentation for both these formats, ensuring clarity and readability without redundancy or unnecessary details.\\n\\nThe function call() takes an input parameter `inputs` which represents the inputs of this method. If the `data_format` equals \\'channels_last\\', then mean is computed across axes [1, 2] (corresponding to spatial dimensions). Conversely, if the `data_format` equals \\'channels_first\\', then mean is computed across axes [2, 3].\\n\\nThe function returns a backend operation that calculates the average of the input data along these specified axes. The `keepdims` attribute determines whether to keep the dimensions or not.\\n\"\"\"',\n",
       " '\"\"\"\\nGlobalAveragePooling3D is a subclass of GlobalPooling3D that performs 3D global average pooling over input tensors. This means it reduces each spatial dimension (height, width, and depth) to one value by taking the mean along that axis. The \\'data_format\\' attribute determines which data format convention to use (channels_last or channels_first).\\n\\nIn its call method:\\n1. If \\'data_format\\' is \\'channels_last\\', it applies global average pooling on inputs with an axis set as [1, 2, 3], keeping the dimensions (\\'keepdims\\') intact. This means it will reduce height, width and depth to one value each along those axes.\\n2. If \\'data_format\\' is not \\'channels_last\\', it applies global average pooling on inputs with an axis set as [2, 3, 4], keeping the dimensions (\\'keepdims\\') intact. This will reduce channels, height and width to one value each along those axes.\\nThe result of this function is returned by the \\'backend.mean()\\' method.\\n\"\"\"',\n",
       " '\"\"\"\\nGlobalMaxPooling1D is a subclass of GlobalPooling1D that implements max pooling operation along the time axis (steps_axis) of the input tensor. It returns maximum value within each frame in case of 3-dimensional inputs or across each feature dimension in case of 2-dimensional inputs.\\n\\nArgs:\\n    data_format  (str): The format of the input tensors (\\'channels_last\\' or \\'channels_first\\'). Determines whether the channels are positioned at axis -1 or axis -3.\\n    keepdims     (bool): If True, keeps the reduced dimensions with length 1 for each element when computing gradients.\\n\\nReturns:\\n    A tensor of rank 2 or 3 depending on input shape and data_format. Its shape is output shape = [batch_size, features] if data_format=\\'channels_last\\', else output shape = [batch_size, frames, features]. The dtype of this tensor is the same as the input tensor.\\n\"\"\"',\n",
       " '\"\"\"\\nGlobalMaxPooling2D is a custom Keras layer that implements global max pooling operation on 2D inputs. It utilizes TensorFlow\\'s backend functions to find the maximum value along specified axes and optionally keeps these dimensions. This allows the model to focus more on the most significant features in each channel, potentially enhancing performance during training.\\n\\nParameters:\\n- data_format : str, \\'channels_last\\' or \\'channels_first\\'. Determines the ordering of the dimensions in inputs. \\n- keepdims : bool, if True, keeps the reduced dimensions with length 1. If False, removes these dimensions.\\n\\nMethod call(inputs):\\nThis method applies the max pooling operation on the input tensor \\'inputs\\' based on data format and returns the resultant tensor after applying global max pooling. It uses TensorFlow backend functions to perform this operation efficiently.\\n\\nExample:\\nlayer = GlobalMaxPooling2D(data_format=\\'channels_last\\', keepdims=True)\\noutput = layer(input_tensor)\\nThis example creates an instance of GlobalMaxPooling2D with \\'channels_last\\' data format and keeps dimensions. It then applies this layer to the input tensor, resulting in a new output tensor.\\n\"\"\"',\n",
       " '\"\"\"\\nGlobalMaxPooling3D is a subclass of GlobalPooling3D that applies max pooling operation on 3-dimensional inputs. It overrides the call method from its parent class by defining a different behavior based on the value of self.data_format.\\n\\nParameters:\\nself (instance): An instance of the class, used to access and modify attributes within the class.\\ninputs (tensor): A 5D tensor containing input data. Its shape depends on the data format (\\'channels_last\\' or \\'channels_first\\').\\n\\nReturns:\\nA tensor after applying max pooling operation along specified axes with a maximum value. The result is reshaped to maintain the number of dimensions as original, but reduced in size by taking the max over the pooled axis.\\n\\nRaises:\\nValueError: If input tensor\\'s shape doesn\\'t match any predefined format. \\n\"\"\"',\n",
       " '\"\"\"\\nGlobalPooling1D is a class that applies global pooling operation along time dimension (last one) in 1D inputs. It takes three parameters - data_format and keepdims, which by default are set to \\'channels_last\\' and False respectively. This class does not have its own methods except for those inherited from Layer superclass of Keras framework.\\n\\nParameters:\\n- data_format: A string indicating the ordering of the dimensions in the input, one of \"channels_first\" or \"channels_last\". By default it is set to \\'channels_last\\'. \\n- keepdims: A boolean that specifies whether to include the dimensions of length 1 in the shape of the returned output tensor. If True and x is a sparse tensor, a dense equivalent tensor will be used instead. Default value is False.\\n\\nMethods:\\n- compute_output_shape(input_shape): It computes the output shape based on input shape after applying pooling operation. \\n- call(inputs): This method raises a NotImplementedError as it\\'s an abstract method in Keras Layer class. The actual computation of global pooling is performed by other classes that inherit this one. \\n- get_config(): Returns the config dictionary that includes \"data_format\" and \"keepdims\". It is mainly used for serializing/deserializing models.\\n\"\"\"',\n",
       " '\"\"\"\\nGlobalPooling2D is a custom Keras layer that implements global pooling on 2D data. It takes an input tensor and computes the average or maximum value from each feature map, reducing its dimensionality.\\n\\nParameters:\\n- `data_format` (str): String representing the data format of the input tensors. Can be \\'channels_last\\' (default) or \\'channels_first\\'.\\n- `keepdims` (bool): If True, keeps the dimensions of the input tensor in the output shape. Default is False.\\n\\nMethods:\\n- `__init__(self, data_format=None, keepdims=False, **kwargs)`: Initializes an instance with specified parameters and calls superclass\\'s init method to set up layer configuration.\\n- `compute_output_shape(self, input_shape)`: Calculates the output shape of this layer given an input tensor shape.\\n- `call(self, inputs)`: Raises a NotImplementedError because GlobalPooling2D is intended to be used as part of a model and does not perform any computation on its own. \\n- `get_config(self)`: Returns the configuration dictionary that contains all configurations for this layer. This function also calls superclass\\'s get_config method to obtain base config, then updates it with current class\\'s config.\\n\"\"\"',\n",
       " '\"\"\"\\nA Keras Layer that implements a 3D global pooling operation.\\n\\nThe GlobalPooling3D layer computes statistics over the entire depth of an input tensor, resulting in a reduced feature map.\\nIt takes three optional arguments \\'data_format\\' and \\'keepdims\\'. \\'data_format\\' is used for specifying if the data format of the input tensors. \\nDefault value is None which means that it will be determined by the Keras backend at runtime. \\'keepdims\\' controls an output array shape behavior. If set to True, keep dimensions with length 1 when performing pooling operations. Default is False.\\n\\nMethods defined here:\\n    - __init__(self, data_format=None, keepdims=False, **kwargs)\\n     Initialize the GlobalPooling3D layer with optional \\'data_format\\' and \\'keepdims\\'. \\n     \\n    - compute_output_shape(self, input_shape):\\n         Computes the output shape of the layer. \\n         \\n    - call(self, inputs)\\n         Raises NotImplementedError as this method needs to be implemented in subclasses for specific global pooling operation.\\n     \\n    - get_config(self)\\n         Returns the configuration dictionary of a layer. It includes \\'data_format\\' and \\'keepdims\\'. \\n\"\"\"',\n",
       " '\"\"\"\\nGroupTimeSeriesSplit is a time series splitter that allows splitting data into training and testing sets along grouped time series. \\n\\nParameters:\\n- test_size : int, optional\\n    The size of the test set (default is None).\\n- train_size : int, optional\\n    The size of the training set (default is None).\\n- n_splits : int, optional\\n    The number of splits to generate (default is None).\\n- gap_size : int, default is 0\\n    The gap between consecutive splits.\\n- shift_size : int, default is 1\\n    The interval at which the train/test split index moves.\\n- window_type : {\\'rolling\\', \\'expanding\\'}, default is \\'rolling\\'\\n    Whether to use a rolling or expanding window for splitting.\\n\\nRaises:\\n- ValueError\\n    If `train_size` and `n_splits` are both None, \\'Either train_size or n_splits should be defined\\'. \\n    If `window_type` is not in {\\'rolling\\', \\'expanding\\'}, \\'Window type can be either \"rolling\" or \"expanding\"\\'. \\n    If `train_size` is not None and `window_type` is \\'expanding\\', \\'Train size can be specified only with rolling window\\'.\\n\\nMethods:\\n- __init__(test_size, train_size=None, n_splits=None, gap_size=0, shift_size=1, window_type=\\'rolling\\')\\n    Initialize the splitter. \\n\\n- split(X, y=None, groups=None)\\n    Generate indices to split data into training and test set.\\n\\n- get_n_splits()\\n    Return the number of splits for the underlying cross-validation splitter.\\n\\n- _calculate_split_params()\\n    Calculate train/test split parameters based on `train_size`, `gap_size`, and `shift_size`.\\n\"\"\"',\n",
       " '\"\"\"\\nKmeans is a clustering model that implements the K-Means algorithm. It is a subclass of _BaseModel, _Cluster and _IterativeModel classes. \\n\\nParameters:\\n    k : int\\n        The number of clusters to form.\\n        \\n    max_iter : int, optional\\n        Maximum number of iterations for the K-means algorithm. Default value is 10.\\n    \\n    convergence_tolerance : float, optional\\n        Relative tolerance with regards to the inertia to declare convergence. Default value is 1e-5.\\n        \\n    random_seed : int or RandomState instance, optional\\n        Determines random number generation for centroid initialization. Default value is None. If an int is used, it will be used as a seed.\\n    \\n    print_progress : bool, optional\\n        Whether to display progress during training. Default value is 0 (False).\\n        \\nAttributes:\\n    _is_fitted : bool\\n        A boolean flag indicating whether the model has been fitted or not. Initially set to False.\\n        \\nMethods:\\n    _fit(X, init_params=True)\\n        Fits K-means clustering algorithm onto X dataset. The centroids are initialized using random choice if init_params is True.\\n    \\n    _get_cluster_idx(X, centroids)\\n        Helper method to assign each sample in the X array to a cluster based on Euclidean distance from the centroids.\\n        \\n    _predict(X)\\n        Predicts the closest cluster each sample in X belongs to. Returns an array of predicted clusters indices for each sample.\\n\"\"\"',\n",
       " '\"\"\"\\nLabelBinarizer is a transformer that transforms target data into binary format (0 or 1) for supervised learning tasks. It converts labels into a format suitable for consumption by ML algorithms. The output of the transformation can be either dense or sparse based on the \\'sparse_output\\' parameter.\\n\\nParameters:\\n- neg_label : int, optional (default=0)\\n    The class label to use as the negative class in binary representation.\\n- pos_label : int, optional (default=1)\\n    The class label to use as the positive class in binary representation.\\n- sparse_output : boolean, optional (default=False)\\n    Whether to return a sparse matrix instead of a dense one for representing the binarized output.\\n    \\nAttributes:\\n- classes_ : array-like of shape (n_classes,) \\n    The classes seen during fitting.\\n    \\nMethods:\\n- fit(y) : \\n    Fit LabelBinarizer to data y, which can be either a 1D or 2D numpy array or sparse matrix of shape (n_samples, n_features).\\n- transform(y) : \\n    Perform binarization on the target data y. Raises an exception if y is not fitted.\\n- inverse_transform(Y, threshold=None) : \\n    Transform a binary matrix back to label format, given Y and optional threshold value for deciding classes.\\n- fit_transform(y) : \\n    Perform fitting and transformation at the same time on data y.\\n    \\nRaises:\\n- ValueError if neg_label is not strictly less than pos_label or if sparse_output=True but pos_label != 0 or neg_label != 0.\\n\"\"\"',\n",
       " '\"\"\"\\nLabelEncoder is a Python class implementing scikit-learn\\'s TransformerMixin and BaseEstimator. It encodes target values (y) as numeric indices. This is especially useful for categorical variables where we can not directly use them in most of the ML algorithms.\\n\\nParameters:\\nauto_wrap_output_keys (bool, optional): If True, wrap output keys with a `__` prefix if they are in the input data but missing from `self.classes_`. Defaults to None.\\n\\nMethods:\\nfit(y) - Fits transformer on the targets y. It transforms each unique sample in y into a class label starting from 0 and assigning an integer for every different sample.\\nReturns self.\\n\\nfit_transform(y): Fits the model and then transforms the data, equivalent to calling `fit` followed by `transform` on the input.\\n\\ntransform(y) - Transforms targets y according to fitted state. This is where LabelEncoder converts categorical values into integer indices. The function will return an array of integers representing the target encoded labels. \\nIf a new category found in y, it must be previously seen during `fit` or an exception will be raised.\\n\\ninverse_transform(y) - Transforms targets back to their original representation. It converts integer indices back to their corresponding unique samples from y. If the target is not seen during fitting, a ValueError would occur.\\n\\n_more_tags() - Returns a dictionary with keys `X_types` for explaining the types of input and output data. Here it returns [\\'1dlabels\\'], indicating that the input can be 1-dimensional labels.\\n\"\"\"',\n",
       " '\"\"\"\\nClass for performing Linear Regression using various methods such as direct equation solving, stochastic gradient descent or QR decomposition and Singular Value Decomposition.\\n\\nParameters:\\n- method : str, default=\\'direct\\'. Determines the regression method to be used - \\'sgd\\', \\'direct\\', \\'svd\\', or \\'qr\\'.\\n- eta : float, default=0.01. Learning rate for SGD method.\\n- epochs : int, default=50. Number of iterations in case of SGD method.\\n- minibatches : int/None, default=None. If provided with a number, it denotes the size of each mini batch if method is \\'sgd\\'. \\n- random_seed: int/None, default=None. Seed for RandomState if used in SGD.\\n- print_progress : int, default=0. Determines how often to output progress - 0 (no output), 1 (every epoch) or 2 (half of the total iterations).\\n\\nRaises:\\n- ValueError: If `method` is not \\'sgd\\', \\'direct\\', \\'svd\\' or \\'qr\\'. Or if `minibatches` is provided and method != \\'sgd\\'.\\n\"\"\"',\n",
       " '\"\"\"\\nLogistic Regression Model implementation using Gradient Descent for minimizing Logit cost function. \\n\\nThis model uses learning rate (eta), epochs, L2 regularization term lambda, minibatches and print progress option to train the model.\\nThe `_fit` method initializes weights and biases if init_params=True, otherwise it assumes they are already initialized. The cost history is stored in self.cost_.\\nDuring each epoch a set of minibatches is selected for training with stochastic gradient descent. After an epoch, the Logit cost function is evaluated and its value is appended to self.cost_.\\nThe `_predict` method uses the trained model to predict the class labels of instances in X based on whether their net input (dot product between features and weights plus bias) is less than 0 or not.\\nMethods _net_input, predict_proba, _logit_cost are helping methods that calculate the net input, predicted probabilities and the logit cost respectively.\\nFinally, _sigmoid_activation method implements the sigmoid activation function used in forward propagation of the model. \\n\"\"\"',\n",
       " '\"\"\"\\nClass that defines a loss function for Keras models. It is meant to be subclassed by concrete loss functions.\\n\\nProperties:\\n- name (str): The name of this loss instance. Defaults to the lowercased and snaked version of the class name if not specified at instantiation.\\n- reduction (str): Reduction method, either \\'sum_over_batch_size\\', \\'mean\\' or \\'none\\'. Determines how to aggregate over batch elements during training.\\n- dtype (dtype): DType for computation. Defaults to float32 unless explicitly set otherwise at instantiation.\\n\\nMethods:\\n- __call__(y_true, y_pred, sample_weight=None) - Computes the loss value between `y_true` and `y_pred`. \\n    Args:\\n        y_true (Tensor): Ground truth values.\\n        y_pred (Tensor): The predicted values.\\n        sample_weight (Tensor, optional): Sample weights for per-example losses. Defaults to None.\\n    Returns:\\n        Loss value tensor of shape `[batch_size]` if reduction is \\'none\\' else scalar.\\n    \\n- call(y_true, y_pred) - Override this method with the loss computation logic. \\n    Args:\\n        y_true (Tensor): Ground truth values.\\n        y_pred (Tensor): The predicted values.\\n    Returns:\\n        Loss value tensor of shape `[batch_size]` if reduction is \\'none\\' else scalar.\\n    \\n- get_config() - Returns the serializable config of this layer. \\n    Returns:\\n        Dictionary with keys {name, reduction}.\\n    \\n- from_config(config) - Creates a new instance of this class fully initialized from a configuration dictionary.\\n    Args:\\n        config (dict): The serialized config of this layer.\\n    Returns:\\n        An instance of the loss function.\\n\"\"\"',\n",
       " '\"\"\"\\nA derived class from Pooling1D that implements maximum pooling operation on 1-dimensional data through application of max function over sliding windows of data points. It takes an optional parameter pool_size which determines the size of the window to be used in the operation and applies it across each dimension. The strides, padding, and data_format parameters are also inherited from its parent class for controlling the convolution process.\\n\"\"\"',\n",
       " '\"\"\"\\nMaxPooling2D is a subclass of Pooling2D that implements max pooling operation on 2D tensors.\\n\\nThe constructor initializes MaxPooling2D with the specified pool size, strides, padding type and data format. The parent class\\'s __init__ method is called to initialize using nn.max_pool as the pooling function and the other parameters.\\n\\nParameters:\\n- pool_size (tuple of 2 integers): The size of the max pooling windows. Default is (2, 2).\\n- strides (tuple of 2 integers or None): Strides values for both dimensions of input tensor. If None, they default to pool_size.\\n- padding (string): \\'valid\\' or \\'same\\', case insensitive. Padding type to be used in the convolution operation before applying the max pooling.\\n- data_format (string): A string, one of \"channels_first\" or \"channels_last\". Whether the dense layer should use channels_first or channels_last dimension ordering. Default is defined in your Keras or TensorFlow backend.\\n\\nUsage:\\n```python\\nmaxpool = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding=\\'valid\\', data_format=None)\\noutput = maxpool(inputs)\\n```\\n\"\"\"',\n",
       " '\"\"\"\\nMaxPooling3D is a subclass of Pooling3D which applies max pooling operation on 3-dimensional input tensors.\\n    \\nParameters:\\n    - pool_size : tuple of ints, optional (default=(2, 2, 2))\\n        The size of the max pooling windows in each dimension.\\n    - strides : tuple of ints, or None, optional (default=None)\\n        Stride values for each dimension. If not provided, defaults to pool_size.\\n    - padding: string, \\'valid\\' or \\'same\\'.\\n        Whether to use valid padding or same padding. \\n    - data_format : str, one of \"channels_last\" or \"channels_first\". The ordering of the dimensions in the input. Default is \"channels_last\".\\n\\n**kwargs: Additional keyword arguments passed to base class and are applied to convolution layer.\\n\"\"\"',\n",
       " '\"\"\"\\nA metric object that keeps track of average is Keras\\'s framework. It provides methods to update its state and compute its result. \\nThis includes resetting the state, updating the states based on provided arguments, computing results etc.\\nIt also supports adding variables for tracking and managing different metrics. This class can be customized by extending it with new functionality.\\n\\nAttributes:\\n    name (str): Name of the metric instance. Default is auto-generated using a function.\\n    dtype (data type): Data type that this metric computes values in, determined during instantiation.\\n        If not provided, defaults to float32 as defined by Keras\\'s backend.\\n    \\nMethods:\\n    reset_state: Resets all of the metric states to initial state (usually 0).\\n    update_state(self, *args, **kwargs): Updates the current value of each metric variable based on provided arguments. \\n        This method needs to be implemented in subclasses as it varies depending on specific metrics being computed.\\n    stateless_update_state: Ensures that updates do not impact other instances of same class. Returns updated variables.\\n    result: Computes the final value of each metric variable based on current state and returns a scalar value. \\n        This method needs to be implemented in subclasses as it varies depending on specific metrics being computed.\\n    stateless_result: Ensures that results do not impact other instances of same class by returning a result independent of updates.\\n    stateless_reset_state: Resets states independently of any update operations, ensuring no impact on other metric variables. \\n        Returns reset variables.\\n    dtype: Returns the data type that this metric computes values in.\\n    _obj_type: Returns the type of object, i.e., \\'Metric\\'.\\n    add_variable(shape, initializer, dtype=None, aggregation=\\'sum\\', name=None): Adds a variable to be tracked and managed by this metric instance. \\n        The shape, initialization method (initializer), datatype, aggregation type and an optional name can be provided during creation.\\n    add_weight: A wrapper for the \\'add_variable\\' method with default arguments for weights in models.\\n    variables: Returns a list of all variables currently being tracked by this metric instance.\\n    __call__(*args, **kwargs): Convenience method to update and compute metrics based on provided arguments using the metric instance itself as callable.\\n    get_config: Returns a dictionary containing configuration details of the metric instance needed for saving and loading models with custom metrics.\\n    from_config: Class method that constructs an instance of the class from a config returned by \\'get_config\\' method.\\n    __setattr__: Overrides default behavior to track updates on certain attributes (like \\'_tracker\\'). Only applicable when \\'_tracker\\' exists.\\n    _check_super_called: Checks if superclass\\'s constructor has been called in current class, raises an exception if not. \\n        This is a common practice to ensure that the correct initialization sequence is maintained across subclasses.\\n    __repr__: Returns canonical string representation of the metric instance, i.e., \\'<ClassName name=self.name>\\'.\\n    __str__: Returns a user-friendly string representing the metric instance. This usually matches with __repr__ but can be customized as per requirement. \\n\"\"\"\\n---\\nThis Python docstring provides an overview of the `Metric` class in Keras\\'s framework, its purpose and how it should be used by providing explanations for methods, attributes and their usage.',\n",
       " '\"\"\"\\nMultiLabelBinarizer is a transformer that transforms between iterable of iterables and a multilabel format. The classes are converted into a binary unordered tally codes. This format is used when there can be more than one applicable categories to each sample. For example, the labels for different attributes of a music piece.\\n\\nThe class has several methods:\\n\\n- `__init__(self, *, classes=None, sparse_output=False)`: Constructor method that initializes an instance with provided parameters. If no classes are provided, it will automatically detect unique classes from the data. \\n\\n- `fit(self, y)`: Method to fit the transformer on the input data, here y represents labels in multilabel format. This method returns self for method chaining.\\n\\n- `fit_transform(self, y)`: Fits the model and transforms the given target values. This is more efficient than calling separately fit() followed by transform(), as this method is able to complete the same task in a single pass. It also takes multilabel format of labels. \\n\\n- `transform(self, y)`: Transforms the input data into multilabel format. Here y represents labels in multilabel format which need to be transformed into binary unordered tally codes.\\n\\n- `_build_cache()`: This is a private method that builds a cache of class mappings for faster transforming later on. \\n\\n- `_transform(self, y, class_mapping)`: This is a private method that performs the actual transformation from iterable to multilabel format. It also deals with unknown classes and raises warnings.\\n\\n- `inverse_transform(self, yt)`: Inverse transforms the input data back into its original state. Here yt represents binary unordered tally codes which need to be transformed back into labels in multilabel format.\\n\\n- `_more_tags(self)`: This is a method that returns additional tags for the estimator. It provides information about what types of inputs this transformer expects, in this case \\'2dlabels\\'.\\n\"\"\"',\n",
       " '\"\"\"\\nThis is an implementation of OneHotEncoder from sklearn\\'s preprocessing module. It takes categorical data and converts it into binary vectors, often used in machine learning models to represent categorical features. \\n\\nParameters:\\n- `drop` : \\'first\\', int or None (default=None) - When set to \\'first\\', then the first value of each feature name will be dropped in the output. If an integer is given, then that many values of each feature will be dropped in the output starting from the beginning. \\n- `handle_unknown` : {\\'error\\', \\'ignore\\'} (default=\\'error\\') - Whether to raise or ignore an error when an unknown categorical feature is present during transform (string).\\n- `sparse_output` : boolean, default = True - Whether to return a sparse matrix instead of a dense one. \\n- `min_freq` : int > 0, default = 1 - If integer, minimum frequency of category in a column for it to be included in the output.\\n- `feature_name_combiner` : {\\'concat\\'}, default = \\'concat\\' - Function that is used to combine original feature name with transformed category name when get_feature_names_out method is called.\\n\\nMethods:\\n- `fit(X, y=None)` - Fit the model according to the given training data. \\n- `transform(X)` - Perform transformation on X and returns a new DataFrame if sparse output is True else it will return an array.\\n- `inverse_transform(X)` - Perform inverse transformation of X and returns a transformed input.\\n- `get_feature_names_out(input_features=None)` - Get output feature names for transformation.\\n\"\"\"\\nYou are an expert in data science, I\\'m sure you understand what each method does and its usage is based on your experience with sklearn OneHotEncoder module. If there are any specific use cases that should be covered or clarified in the docstring, please provide them so they can be included in this docstring.',\n",
       " '\"\"\"\\nOPTICS (Ordering Points To Identify the Clustering Structure) is an algorithm that extends DBSCAN (Density-Based Spatial Clustering of Applications with Noise). It builds on two primary prerequisites, the computation of reachability distances and an ordering of these distances. The reachability distance at a particular point in N-dimensional space can be either the Euclidean distance between that point and the nearest point in the dataset or any other metric specified by the user.\\n\\nThe OPTICS algorithm is parameterized by two parameters, min_samples (the number of samples in a neighborhood for a point to be considered as a core point) and metric (metric to compute distances). The clustering result returned by the fit function contains labels which represent the clusters inferred from the reachability distance data.\\n\\nParameters: \\n- `min_samples` : int, default=5\\n    A parameter of DBSCAN - the number of samples in a neighborhood for a point to be considered as a core point.\\n- `max_eps` : float, default=inf\\n    Maximum reachability distance (also referred to as the maximum search radius) used in the neighborhood of a point.\\n- `metric` : str or callable, default=\\'minkowski\\'\\n    The metric to use when calculating distance between instances in a feature array. If metric is a string or function, it must be one of the options allowed by sklearn.metrics.pairwise_distances for its metric parameter.\\n- `p` : int, default=2\\n    Power parameter for the Minkowski metric. When p = 1, this is equivalent to using manhattan_distance (l1), and euclidean_distance (l2) for p = 2. For any other value, minkowski distance (l_p) is used.\\n- `metric_params` : dict, default=None\\n    Additional keyword arguments for the metric function.\\n- `cluster_method` : str, default=\\'xi\\'\\n    The method of choosing the threshold value eps. \\'dbscan\\' or \\'xi\\'. \\n- `eps` : float, default=None\\n    Epsilon parameter for DBSCAN clustering. Only used if cluster_method=\\'dbscan\\'.\\n- `xi` : float, default=0.05\\n    Threshold in the range [0,1]. Used to specify a percentage of neighborsbysize(max_samples). Only used if cluster_method=\\'xi\\'.\\n- `predecessor_correction` : bool, default=True\\n    If true, will correct for noise (precomputed distance matrix is used) using predecessors. This requires significantly more memory and computation but can produce higher quality clusters.\\n- `min_cluster_size` : int or float, default=None\\n    The number of samples in a neighborhood for a point to be considered as a core point. If None, the value set during the object\\'s instantiation will be used. \\n- `algorithm` : str, default=\\'auto\\'\\n    Algorithm used to compute the nearest neighbors: \\'ball_tree\\', \\'kd_tree\\', or \\'brute\\'. \\'auto\\' uses BallTree if n_query points < 100 and brute otherwise.\\n- `leaf_size` : int, default=30\\n    Leaf size passed to BallTree or KDTree.  This can affect the speed of the construction and query algorithms for those trees.\\n- `memory` : instance of joblib.Memory or None, default=None\\n    Used to cache (and reuse) computations that can be expensive in terms of both time and memory. If set to None, caching will be disabled. By default, the cache is a global instance shared by all instances.\\n- `n_jobs` : int or None, optional (default=None)\\n    The number of parallel jobs to run for neighbors search. None means 1 unless in a jupyter notebook where the global IPython context may specify another value. -1 uses all processors.\\n\\nAttributes:\\n- `ordering_` : ndarray of shape (n_samples,)\\n    The reachability distance order for each sample.\\n- `core_distances_` : ndarray of shape (n_samples, 2)\\n    For each sample, the minimum and maximum reachability distances to its k-Neighbors (as defined by the min_samples parameter).\\n- `reachability_` : ndarray of shape (n_samples,)\\n    The reachability distance for each sample.\\n- `predecessor_` : array, shape = [n_samples]\\n    For each sample, index of its predecessor in the ordering.\\n\"\"\"',\n",
       " '\"\"\"\\nOrdinalEncoder(OneToOneFeatureMixin, _BaseEncoder)\\n\\nParameters\\n----------\\ncategories : \\'auto\\' or list of lists (default is \\'auto\\')\\n    The categories to encode each column into. If \\'auto\\', the unique values in each column will be inferred from `X`.\\ndtype : a floating point data type (default is numpy.float64)\\n    Desired dtype for array. \\nhandle_unknown : {\\'error\\', \\'use_encoded_value\\'} (default is \\'error\\')\\n    Whether to raise an error on unknown categories, or use the value in `unknown_value`.\\nunknown_value : int or numpy.nan (default is None)\\n    Value used for unseen categories when handle_unknown=\\'use_encoded_value\\'. Ignored if handle_unknown=\\'error\\'.\\nencoded_missing_value : int or numpy.nan (default is numpy.nan)\\n    The value to encode missing values into when `handle_unknown` is \\'use_encoded_value\\'. If the dtype of X is not float, and this parameter is np.nan, a ValueError will be raised.\\nmin_frequency : int or float in (0, 1) or None (default is None)\\n    Minimum frequency of categories to include in the encoding. Categories with lower frequencies are treated as an infrequent category which gets encoded as len(categories)-len(infrequent_categories). Ignored if not specified.\\nmax_categories : int >= 1 or None (default is None)\\n    Maximum number of categories to include in the encoding. If more categories exist, they will be ignored. The most frequent ones are kept.\\n\\nMethods\\n-------\\nfit(X, y=None)\\n    Infer the unique values for each column in `X` to which categories should be inferred. \\ntransform(X)\\n    Transform X by replacing each value with its corresponding category encoding.\\ninverse_transform(X)\\n    Transform back X from encoded categories to original form. Replaces each value with its corresponding original category.\\n\"\"\"',\n",
       " '\"\"\"\\nPooling1D is a layer in Keras that performs 1-dimensional pooling on an input signal. It takes as input 3D tensor and applies a specified function (`pool_function`) over each channel of the input to generate a 2D output.\\n\\nThe `__init__()` method initializes the parameters such as `pool_size`, `strides`, `padding`, etc., for pooling operation. The `call()` method expands dimensions and applies the specified function using TensorFlow\\'s array operations. It then squeezes the output to remove extra dimension.\\n\\nThe `compute_output_shape()` method computes the shape of the output based on the input shape, pool size, strides, padding, and data format. This is important for building the model architecture with a known output shape.\\n\\nLastly, the `get_config()` method returns a dictionary containing configuration parameters of the layer which can be useful in saving/loading models. \\n\\nParameters:\\n    pool_function (callable): Function to apply on each channel of the input tensor. It should take as arguments an n-dimensional Tensor and return an n-1 dimensional Tensor.\\n    pool_size (int, tuple): Size of the pooling window for each dimension in the input.\\n    strides (int, tuple, optional): Stride values for each dimension of the input tensor. Default is `pool_size` if not provided.\\n    padding (str, optional): Type of padding to be used before performing the convolution operation (\\'valid\\' or \\'same\\'). Defaults to \\'valid\\'.\\n    data_format (str, optional): Data format of the input (\\'channels_last\\' or \\'channels_first\\'). Defaults to the value of `backend.image_data_format()` if not provided.\\n    name (str, optional): Name for the layer. If not specified, automatically generated based on its type.\\n\"\"\"',\n",
       " '\"\"\"\\nA Keras layer that performs 2D pooling on inputs.\\n\\nAttributes:\\n    pool_function (function): The function used for pooling, e.g., tf.nn.max_pool2d or tf.nn.avg_pool2d.\\n    pool_size (tuple of 2 ints): The size of the pooling windows.\\n    strides (tuple of 2 ints or None): Strides values for the dimensions of the input tensor. If stride is None, it will default to pool_size.\\n    padding (str): One of \"valid\" or \"same\", case insensitive.\\n    data_format (str): A string, one of \"channels_last\" or \"channels_first\". It represents the layout of the input. If not specified, it defaults to the image_data_format value found in your Keras config file at ~/.keras/keras.json.\\n    name (string): Name for the layer. \\n    **kwargs: For backwards compatibility with Keras < 2.0.0rc3 where `name` wasn\\'t used as a keyword argument but passed to parent class by default.\\n\\nMethods:\\n    __init__(self, pool_function, pool_size, strides, padding=\\'valid\\', data_format=None, name=None, **kwargs): Constructor.\\n    call(self, inputs): Method that applies the 2D Pooling operation to the input tensor.\\n    compute_output_shape(self, input_shape): Returns the shape of the output tensor after applying the pooling operation.\\n    get_config(self): Returns a dictionary containing configuration data for layer instances. This may include `pool_size`, `strides`, etc. \\n\"\"\"',\n",
       " '\"\"\"3D Pooling Layer Class.\\n\\nThis `Pooling3D` class is designed to carry out 3D pooling operations, which are used to reduce the spatial dimensions of input data, thereby reducing computational complexity and overfitting. The type of pooling operation (max or average) can be chosen based on the specific use case.\\n\\nParameters:\\n- `pool_function`: A function that performs the desired 3D pooling operation. This could be either TensorFlow\\'s built-in functions like tf.reduce_min, tf.reduce_max, or a custom implementation of any other pooling method.\\n- `pool_size`: The size of the pooling window for each dimension. An integer (replicated to all dimensions) or a tuple of three integers. For example, (2, 2, 2).\\n- `strides`: The stride with which to carry out the pooling operations. If not provided, it defaults to \\'pool_size\\'.\\n- `padding`: A string (\\'valid\\' or \\'same\\'), determining whether zero padding should be applied before performing the pooling operation. Defaults to \\'valid\\'.\\n- `data_format`: A string (\"channels_last\" (default) or \"channels_first\"), specifying whether input tensors have shape (batch, spatial dims..., channels) or (batch, channels, spatial dims...) at the end.\\n- `name`: A string to set the name of the layer.\\n\\nMethods:\\n- `call(inputs)`: This method carries out the 3D pooling operation. It accepts an input tensor and applies the specified pooling function across it based on the provided parameters (pool size, strides, padding etc.).\\n- `compute_output_shape(input_shape)`: Calculates the output shape of the layer given the input shape.\\n- `get_config()`: Returns a dictionary containing the configuration of the layer that can be used to recreate the layer with the same parameters.\\n\"\"\"',\n",
       " '\"\"\"\\nPrincipalComponentAnalysis Class Docstring:\\n\\nThis class is an implementation of Principal Component Analysis (PCA). PCA allows you to project your data into fewer dimensions while still capturing most of the information contained in the original high-dimensional space. \\n\\nThe initialization method takes three parameters - n_components, solver and whitening. The parameter \\'n_components\\' indicates the number of principal components to keep. If not defined or set to None, all components are kept but will be reduced to a manageable size. The possible values for \\'solver\\' are {\\'eigen\\', \\'svd\\'} with \\'eigen\\' being faster and numerically more stable method while \\'svd\\' is used as a fallback if the \\'eigen\\' solver fails.\\nThe parameter \\'whitening\\' indicates whether to normalize the components so they have unit variance, which makes them decorrelated. Default value for \\'whitening\\' is False.\\n\\nMethods: \\n1) fit(X, y=None): Fits the model with X and y (if provided). It checks if the arrays are valid and fits the data using \\'_fit\\' method. After fitting, sets self._is_fitted to True.\\n2) _fit(X): Implementation of PCA algorithm on input data matrix X. Performs eigendecomposition depending upon \\'solver\\'. Stores the projection matrix (w_) for transformation. It also computes and stores normalized eigenvalues in a variable (e_vals_normalized). The loadings are computed and stored as an instance attribute, loadings_.\\n3) transform(X): Performs dimensionality reduction on X using fitted model\\'s projection matrix w_. If \\'whitening\\' was set to True during fitting phase, it also whitens the transformed data.\\n4) _covariance_matrix(X): Computes and returns the covariance matrix of input array X.\\n5) _decomposition(mat, n_samples): Decomposes a given matrix mat into eigenvalues (e_vals_) and eigenvectors (e_vecs_). \\'solver\\' decides which decomposition method to use - eigendecomposition for \\'eigen\\' or Singular Value Decomposition for \\'svd\\'.\\n6) _loadings(): Computes the loadings of principal components by multiplying corresponding eigenvalues with eigenvectors. \\n7) _projection_matrix(eig_vals, eig_vecs, whitening, n_components): Returns a projection matrix that transforms original data into lower-dimensional space defined by \\'n_components\\'. If whitening is True, it also applies an orthogonal transformation to the principal components such that they have unit variance.\\n\"\"\"',\n",
       " '\"\"\"\\nRMSprop optimizer implementation.\\n\\nThis RMSProp Optimizer is an extension of SGD that uses moving average of squared gradients (squared L2 norm) as a more robust estimate of the variance in the data, and provides occasional faster convergence compared to vanilla stochastic GD.\\n\\nAttributes:\\n    learning_rate (float): Learning rate for optimizer.\\n    rho (float): Coefficient used for running average of squared gradients.\\n    momentum (float or bool): Momentum coefficient. If `True`, use momentum; if `False`, do not use momentum; if float, use fixed momentum value.\\n    epsilon (float): Term added to the denominator to improve numerical stability.\\n    centered (bool): Whether to compute gradients statistics to use them in calculating centralized gradient.\\n\"\"\"',\n",
       " '\"\"\"\\nSelfTrainingClassifier is an extension of Scikit-learn\\'s BaseEstimator, MetaEstimatorMixin and ClassifierMixin classes that provides self-training functionality by leveraging the strengths of both labeled and unlabeled data in a semi-supervised learning context. \\n\\nParameters:\\n_parameter_constraints : dict\\n    Constraints for parameters passed to SelfTrainingClassifier such as estimator, base_estimator, threshold, criterion, k_best, max_iter, and verbose.\\n    \\nAttributes:\\nestimator : Estimator instance or None, default=None\\n    The initial estimator used to train the semi-supervised model. If None, \\'base_estimator\\' must be provided.\\n    \\nbase_estimator : Estimator instance, str (default=\\'deprecated\\') \\n    If not None, this is considered as an alternative to \\'estimator\\'. It deprecates use of \\'estimator\\' and should provide a fit method. Warning is given if it does not have \\'fit\\' method.\\n    \\nthreshold : float, default = 0.75\\n    The threshold value for the decision criterion in self-training loop, used to decide which instances are labeled during training. \\n    \\ncriterion : str {\\'threshold\\', \\'k_best\\'}, default = \\'threshold\\'\\n    Determines the selection criteria for unlabeled instances during each iteration. If \\'threshold\\', label an instance if it has a higher probability than the threshold. Otherwise, select k instances with highest probabilities as per value of \\'k_best\\'. \\n    \\nk_best : int, default = 10\\n    Used only when criterion is set to \\'k_best\\'. Determines the number of top instances selected from unlabeled pool for labeling in each iteration. If k_best > total unlabeled samples, all unlabeled samples will be labeled at first iteration. \\n    \\nmax_iter : int or None, default = 10\\n    Maximum number of iterations allowed during the self-training process. If it is set to None, no maximum limit for iterations.\\n    \\nverbose : bool, default = False\\n    Controls verbosity of the output messages during training phase. \\n\\nMethods:\\n_get_estimator()\\n    Returns the estimator object that will be used by SelfTrainingClassifier for fitting and predicting data. If \\'estimator\\' is None and \\'base_estimator\\' is provided, a clone of it is returned; otherwise, ValueError is raised. \\n    \\nfit(X, y, **params)\\n    Trains the model with training data X, labels y, and other parameters passed as keyword arguments (params). Raises warnings if unlabeled samples are more than labeled ones in \\'k_best\\' criterion or all instances are labeled early.\\n    \\npredict(X, **params)\\n    Makes a prediction using SelfTrainingClassifier on test data X, with other parameters passed as keyword arguments (params). Requires the model to be fitted first. \\n    \\npredict_proba(X, **params)\\n    Returns probability estimates for samples in X. Requires the model to be fitted first. \\n    \\ndecision_function(X, **params)\\n    Returns decision function values for samples in X. Requires the model to be fitted first. \\n    \\npredict_log_proba(X, **params)\\n    Returns log-probability estimates for samples in X. Requires the model to be fitted first. \\n    \\nscore(X, y, **params)\\n    Returns the mean accuracy on the given test data and labels. Requires the model to be fitted first.\\n    \\nget_metadata_routing()\\n    Sets up a MetadataRouter instance for routing metadata between estimator methods in SelfTrainingClassifier. \\n\"\"\"',\n",
       " '\"\"\"\\nSeparableConv is a subclass of Conv that applies separable convolutions. This means it has two main parts: depthwise convolution and pointwise convolution. The depthwise part splits the input channels into groups, applies independently a small filter for each group (a process called depthwise convolution), then combines them back together. The pointwise part applies small filters to every channel in parallel (like standard convolutions).\\n\\nSeparableConv takes following parameters:\\n    - rank: an integer defining the dimension of the input tensors, e.g., 2 for images and 3 for volumes.\\n    - filters: an integer, the dimensionality of the output space (i.e., the number of filters in the convolution).\\n    - kernel_size: an integer or tuple/list of a single integer, specifying the length of the 1d convolution window.\\n    - strides: an integer or tuple/list of a single integer, specifying the stride length along the width of input tensor.\\n    - padding: one of `\"valid\"` or `\"same\"` (case-insensitive).\\n    - data_format: a string, one of \"channels_last\" (default) or \"channels_first\".\\n    - dilation_rate: an integer or tuple/list of a single integer.\\n    - depth_multiplier: the number of depthwise convolution output channels for each input channel.\\n    - activation: activation function to use, e.g., \\'relu\\'. If you pass None, no activation is applied (ie. \"linear\" activation).\\n    - use_bias: Boolean, whether the layer uses a bias vector.\\n    - depthwise_initializer: initializer for the depthwise weights.\\n    - pointwise_initializer: initializer for the pointwise weights.\\n    - bias_initializer: initializer for the bias vectors. If None, no biases will be used.\\n    - depthwise_regularizer: regularizer function applied to the depthwise weights matrices. \\n    - pointwise_regularizer: regularizer function applied to the pointwise weights matrices. \\n    - bias_regularizer: regularizer function applied to the bias vectors.\\n    - activity_regularizer: regularizer function applied to the output of the layer (its \"activation\").\\n    - depthwise_constraint: constraint function applied to the depthwise weights matrices. \\n    - pointwise_constraint: constraint function applied to the pointwise weights matrices. \\n    - bias_constraint: constraint function applied to the bias vectors. \\n    - trainable: Boolean, if `True` the weights of this layer will be made trainable.\\n    - name: a string (optional), the name of the layer. If not specified, it will default to \\'separable_conv\\'.\\n\\nThe SeparableConv class does not implement its own call method because the implementation is left for subclasses. It raises NotImplementedError in the base class\\'s call method. \\n\"\"\"',\n",
       " '\"\"\"\\nThis is an implementation of the separable one-dimensional convolution layer. It applies depthwise and pointwise convolutions independently on each input channel, which allows for efficient computation with large input size but fewer parameters than standard 1D convolutions.\\n\\nDepthwise convolution takes filter (kernel) over spatial dimensions and multiplies it to each of the feature maps. Pointwise convolution is a traditional one-dimensional convolution that mixes features from all previous layers together. The depth_multiplier parameter determines how many output channels will be computed per input channel in pointwise stage, which means total number of these convolutions equals to (depth_multiplier * number of feature maps).\\n\\nThe \\'SeparableConv1D\\' class is a subclass of the SeparableConv base class. It extends its functionality by adding one-dimensional convolution over time axis and applying it on input tensors. The strides parameter controls the stride length for the spatial dimensions, while dilation_rate lets you specify filter taps at non-standard intervals within the kernel.\\n\"\"\"\\n\"\"\"\\nThis is an attribute documentation:\\n\\ndepthwise_kernel: a depthwise convolution kernel that gets applied to each input channel independently of the others. The kernel size and stride parameters control its behavior on the spatial dimensions (timesteps).\\n\\npointwise_kernel: a pointwise convolution kernel with filters number equal to depth_multiplier * original feature maps number, which is then multiplied by each of the feature maps independently. \\n\"\"\"',\n",
       " '\"\"\"\\nSeparableConv2D is a subclass of SeparableConv which implements 2D depthwise separable convolution.\\n\\nParameters:\\n- filters: Integer, the dimensionality of the output space (i.e., the number of filters in the convolution).\\n- kernel_size: An integer or a tuple/list of 2 integers, specifying the height and width of the 2D convolution window. Can be a single integer if you specify a square kernel.\\n- strides: An integer or a tuple/list of 2 integers, specifying the strides for the height and width dimensions. If a single integer is used it specifies both stride values (vertical and horizontal). Defaults to (1, 1).\\n- padding: A string, either \\'valid\\' or \\'same\\'. The padding method. If you have specified dilation_rate then padding should be \\'same\\'. In \\'same\\', the input is padded so that the output will have the same height/width dimension as the input.\\n- data_format: A string, one of \"channels_last\" (default) or \"channels_first\". The ordering of the dimensions in your inputs. \"channels_last\" corresponds to inputs with shape (batch, height, width, channels), while \"channels_first\" corresponds to inputs with shape (batch, channels, height, width).\\n- dilation_rate: An integer or a tuple/list of 2 integers, specifying the dilation rate to use for dilated convolution. Can be a single integer if you specify a square dilation rate. At present, specifying any dilation_rate value != 1 is incompatible with specifying any strides value != 1.\\n- depth_multiplier: The number of depthwise convolutions to be performed. If the value is 1, regular (non-depthwise) convolution is performed. Must be greater than 0 if specified when calling this layer.\\n- activation: Activation function to use. If you don\\'t specify anything, no activation is applied (ie. \"linear\" activation).\\n- use_bias: Boolean, whether the layer uses a bias vector.\\n- depthwise_initializer: An initializer for the depthwise convolution kernel.\\n- pointwise_initializer: An initializer for the pointwise convolution kernel.\\n- bias_initializer: An initializer for the bias vectors. If None, the default zero initializer is used.\\n- depthwise_regularizer: Optional; A regularizer function applied to the depthwise convolution kernel.\\n- pointwise_regularizer: Optional; A regularizer function applied to the pointwise convolution kernel.\\n- bias_regularizer: Optional; A regularizer function applied to the bias vector.\\n- activity_regularizer: Optional; A regularization function applied to the output of the layer (its \"activation\").\\n- depthwise_constraint: Optional; A constraint function applied to the depthwise convolution kernel.\\n- pointwise_constraint: Optional; A constraint function applied to the pointwise convolution kernel.\\n- bias_constraint: Optional; A constraint function applied to the bias vector.\\n- kwargs: Additional keyword arguments to be passed.\\n\\nMethods:\\n- __init__(self, filters, kernel_size, strides=(1, 1), padding=\\'valid\\', data_format=None, dilation_rate=(1, 1), depth_multiplier=1, activation=None, use_bias=True, depthwise_initializer=\\'glorot_uniform\\', pointwise_initializer=\\'glorot<｜begin▁of▁sentence｜>:',\n",
       " '\"\"\"\\nSequentialFeatureSelector is a feature selection algorithm that sequentially builds a model on a subset of features, gradually increasing the number of selected features until the best performance is reached or a pre-specified score threshold is surpassed. The process starts with one feature and then iteratively adds the most important next unselected feature(s) until a stopping criterion is met.\\n\\nParameters:\\n    estimator : object, optional (default=None)\\n        If an estimator is passed in, it will be used to find feature importance scores. Must have either `fit` or `predict_proba` method.\\n        \\n    n_jobs : int, optional (default=1)\\n        The number of jobs to run for all tasks. If -1, then the number of jobs is set to the length of the input data if not none else it will be 1.\\n    \\n    pre_dispatch : int or string, default \\'2*n_jobs\\'\\n       Controls the concurrency (number of uncompleted tasks) that Python will tolerate while executing your call to fit method.\\n        \\n    cv : integer or a cross validation generator, optional\\n        If an integer is specified, then it defines the number of folds in a \\'KFold\\' \\n    \\n    scoring : string or function, optional (default=None)\\n        A string (see model evaluation documentation) or a scorer callable object / function with signature scorer(estimator, X, y).\\n        \\n    forward : boolean, optional. Whether to do forward selection, default is True.\\n    \\n    verbose : integer, optional (default=0)\\n        The verbosity level. If non zero, the fitting process will print messages when processing sequences of features. \\n\\nMethods:\\n    fit(X, y, groups=None, **fit_params): Compute feature importances and indices.\\n    \\n    transform(X): Reduce X to the selected features.\\n    \\n    fit_transform(X, y, groups=None, **fit_params): Fit to data, then transform it.\\n        \\n    get_metric_dict(confidence_interval=0.95) : Returns a dictionary containing feature selection metrics. \\n\"\"\"',\n",
       " '\"\"\"\\nStochastic Gradient Descent optimizer implementation using Keras functions. This SGD class is derived from OptimizerV2 and implements the update method, which applies gradients to variables with momentum and nesterov acceleration support. The optimization parameters (learning rate and decay) are initialized in its constructor and can be set later using the set_hyper() function.\\n\\nAttributes:\\n    learning_rate (float): Learning rate for SGD updates. Defaults to 0.01.\\n    momentum (float or Tensor, optional): Momentum term; defaults to 0.0. If a Tensor is passed in, it represents the initial value of the momentum. The type can be int, float. If the provided value is outside [0, 1], a ValueError will occur.\\n    nesterov (bool, optional): Whether to apply Nesterov\\'s momentum. Defaults to False.\\n    name (str, optional): Name for the optimizer. Defaults to \\'SGD\\'.\\n    \\nMethods:\\n    _create_slots(var_list) - Creates a slot for each variable in var_list with the identifier \\'momentum\\' which is used to hold the current gradients.\\n    _prepare_local(var_device, var_dtype, apply_state) - Prepares local variables for update of each Variable on device and dtype specified. It stores the momentum term as an identity operation.\\n    _resource_apply_dense(grad, var, apply_state=None) - Applies gradients to dense variables using either a Keras Momentum or Gradient Descent depending upon whether momentum exists or not.\\n    _resource_apply_sparse_duplicate_indices(grad, var, indices, kwargs) - Apply gradient updates with duplicate indices to the sparse variable \\'var\\'. This function is for gradients that have identical entries in indices. The arguments are as same as _resource_apply_dense.\\n    _resource_apply_sparse(grad, var, indices, apply_state=None) - Applies gradient updates with non-unique indices to the sparse variable \\'var\\'. This function is for gradients that do not have identical entries in their indices argument. The arguments are as same as _resource_apply_dense.\\n    get_config() - Returns a dictionary containing the configuration of this optimizer which includes learning rate, decay, momentum and nesterov flag.\\n\"\"\"',\n",
       " '\"\"\"\\nSoftmaxRegression is a Python class that implements multinomial logistic regression using softmax function and gradient descent optimization method. \\nIt utilizes multiple mix-ins including _BaseModel, _IterativeModel, _Classifier and _MultiClass to provide comprehensive functionalities.\\n\\nAttributes:\\n    eta (float): Learning rate for the Gradient Descent Optimization Method. Default is 0.01.\\n    epochs (int): Number of times training dataset will be iterated over during learning process. Default is 50.\\n    l2 (float): L2 regularization term to prevent model from becoming too complex. Default is 0.0.\\n    minibatches (int): Minibatch size for stochastic gradient descent. Default is 1 which means full-batch (i.e., batch size equals total number of training samples).\\n    n_classes (int): Number of unique classes in target variable, inferred automatically if None.\\n    random_seed (int or None): Random seed for reproducibility. Default is None.\\n    print_progress (int): Controls the verbosity of the learning process; 0 means no output at all, 1 provides a progress bar, and 2 prints epoch-wise cost. Default is 0.\\n    \\nMethods:\\n    _net_input(X): Computes the weighted sum of inputs as per model\\'s learned parameters.\\n    _softmax_activation(z): Applies softmax activation function on input z to derive class probabilities.\\n    _cross_entropy(output, y_target): Calculates cross entropy loss for given output and target values.\\n    _cost(cross_entropy): Computes total cost associated with the model\\'s predictions. Includes L2 regularization term.\\n    _to_classlabels(z): Converts a probability distribution vector z to class labels by taking argmax along axis 1.\\n    _forward(X): Forward pass through the network that computes the output probabilities for X and current model parameters.\\n    _backward(X, y_true, y_probas): Compute gradients of loss function with respect to inputs, weights and bias.\\n    _fit(X, y, init_params=True): Trains the model by updating self.w_ and self.b_ for a given number of epochs or until convergence.\\n    predict_proba(X): Returns class probabilities for X according to the trained SoftmaxRegression model parameters.\\n    \\n\"\"\"',\n",
       " '\"\"\"\\nTargetEncoder is a class that encodes categorical variables by replacing each unique value with \\nan average target value. It supports binary, multiclass and continuous targets. The TargetEncoder class inherits from OneToOneFeatureMixin and _BaseEncoder classes.\\n\\nParameters:\\ncategories : str or list, default=\\'auto\\'. Categories of features to encode. If \\'auto\\', it will attempt to infer them automatically. \\ntarget_type : str, default=\\'auto\\'. The type of target variable. It can be either \\'binary\\', \\'multiclass\\', or \\'continuous\\'. \\nsmooth : str or float, default=\\'auto\\'. Smoothing parameter for the mean encoding scheme. If \\'auto\\', it will attempt to infer them automatically based on target variance. \\ncv : int, default=5. The number of folds in cross validation.\\nshuffle : bool, default=True. Whether to shuffle data before splitting into batches.\\nrandom_state : RandomState instance or None (default), default=None. Determines random number generation for train/test split. \\n\\nMethods:\\nfit : Fit the encodings on X and y.\\nfit_transform : Fit and transform X, using specified encoding for categorical variables in X.\\ntransform : Perform transformation on the features of X.\\nget_feature_names_out : Get output feature names from input space. \\n\"\"\"',\n",
       " '\"\"\"\\nTransactionEncoder is an implementation of TransformerMixin and BaseEstimator that transforms transaction data into a binary matrix format where each row represents a transaction, \\neach column represents an item, and the presence (True) or absence (False) of an item in a transaction can be represented by a cell.\\n\\nAttributes:\\n    columns_ (list): A sorted list of unique items across all transactions, used as columns for the binary matrix representation.\\n    \\n    columns_mapping_ (dict): A dictionary where keys are unique items and values are their corresponding indices in `columns_` list.\\n\\nMethods:\\n    fit(X) : Fits the transformer to X by computing `columns_` and `columns_mapping_` attributes based on unique items present in transactions. Returns self for method chaining.\\n    \\n    transform(X, sparse=False): Transforms X into a binary matrix representation using fitted columns information. If `sparse` is True, the transformation is done via sparse matrix format to save memory.\\n        \\n    inverse_transform(array): Inverts transformed array back into original transactions.\\n    \\n    fit_transform(X, sparse=False): Fits on X and then transforms X. Equivalent to calling `fit(X).transform(X)`.\\n    \\n    get_feature_names_out(): Returns feature names for the transformed data, which are items in columns_. Raises a ValueError if called before fit.\\n\"\"\"',\n",
       " '\"\"\"\\nUpSampling1D Layer Class: This is a Keras custom layer that performs upsampling of an input tensor along its first dimension (time). The size argument determines the factor by which the input is going to be repeated along time axis. It ensures that the output sequence length equals to `size` times the original sequence length.\\n\"\"\"',\n",
       " '\"\"\"\\nUpSampling2D is a Layer subclass implementing two-dimensional upsampling of input features. It takes an input tensor and reshapes it by scaling its height and width dimensions, using the specified size.\\n\\nArgs:\\n    size (tuple): The upsampling factors for height and width. Defaults to (2, 2).\\n    data_format (str): A string, one of `\"channels_first\"`, or `\"channels_last\"`. It indicates whether the layer should use channels-first (\\'channels_first\\') or channels-last (\\'channels_last\\') data formatting. Defaults to None.\\n    interpolation (str): A string, one of `\\'nearest\\'` or `\\'bilinear\\'`. The type of upsampling method used for resizing images: nearest neighbor or bilinear. Defaults to \\'nearest\\'.\\n\\nRaises:\\n    ValueError: If the interpolation argument is not one of `\"nearest\"` or `\"bilinear\"`.\\n    \\nReturns:\\n    TensorShape: The shape of the output tensor if input_shape is provided as an argument to compute_output_shape() method.\\n\\nCalls: \\n    resize_images(inputs, self.size[0], self.size[1], self.data_format, interpolation=self.interpolation): Resizes the images contained in inputs using specified factors and data format.\\n    \\nget_config(): Returns a dictionary containing configuration information about the layer. This typically includes the size of the convolution kernel, the strides with which to apply the convolution, as well as any activation data if applicable. \\n\"\"\"',\n",
       " '\"\"\"\\nUpSampling3D is a Layer subclass that performs upsampling of 3-dimensional data.\\n\\nThis layer takes input shape (including batch size, channel number and depth, height, width) and returns output shape after upsampling. The output will have the same number of channels as the input, and each sample in the batch will be a 5D tensor containing the upsampled data.\\n\\nThe `size` parameter is used to determine how much the spatial dimensions should be increased (height, width). It can be set by passing a tuple of 3 integers or None for all three dimensions. If not specified, it defaults to (2, 2, 2), which means that each dimension will be upsampled twice in size.\\n\\nThe `data_format` parameter defines the format of data: \\'channels_first\\' (default) or \\'channels_last\\'. It affects how shapes are interpreted:\\n- \\'channels_first\\' indicates data with shape (batch, channels, depth, height, width).\\n- \\'channels_last\\' indicates data with shape (batch, depth, height, width, channels).\\nIf not specified, it defaults to None. In this case, Keras config variable `image_data_format` will be used which is coming from the global keras.json configuration file at ~/.keras/keras.json or by setting environment variable KERAS_BACKEND=theano or KERAS_BACKEND=tensorflow to override it. If you have not yet set it, then it will default to \\'channels_last\\'.\\n\"\"\"',\n",
       " '\"\"\"\\nZeroPadding1D is a custom Keras layer that applies zero-padding to temporal data along the time dimension. \\nThis padding helps in maintaining spatial locality and can be particularly useful when performing operations like convolution or recurrent layers on sequential data, where preserving context across long sequences of data (i.e., longer than the size of a filter) becomes crucial.\\n\\nParameters:\\n- padding : int or tuple of 2 integers. The number of zeros to add at the beginning and end of each sequence in input_shape. If an integer is used, it will be symmetrically divided between the start and end of sequences. Defaults to 1.\\n\\nMethods:\\n- __init__(self, padding=1, **kwargs) : Initializes the ZeroPadding1D layer with specified padding size. Calls the parent class\\'s init method using kwargs for additional parameters.\\n\\n- compute_output_shape(self, input_shape) : Calculates and returns the output shape of the layer given an input shape. It increases the second dimension by the sum of padding values at both ends.\\n\\n- call(self, inputs) : Applies zero-padding to the input sequences. Takes in a tensor of input sequences (inputs), applies temporal_padding using the specified padding parameter, and returns the padded output.\\n\\n- get_config(self) : Returns a dictionary containing configuration data about the layer. This includes \\'padding\\' which is set during initialization and its current value. \\n\"\"\"',\n",
       " '\"\"\"\\nThis module provides an implementation of the ZeroPadding2D layer using Keras. The purpose of this layer is to add zero padding on the input tensor for spatial convolutional layers. It allows users to specify the amount of padding and also decide between \\'channels_first\\' or \\'channels_last\\' data format.\\n\\nThis class extends from the base `Layer` class in Keras, providing an interface for a layer with no weights but custom behavior during the forward pass. \\n\\nHere is a brief description of each method:\\n- __init__: Initializes the padding and data format for the ZeroPadding2D object. Padding can be either an integer or a tuple. If it\\'s an integer, it applies symmetric padding to both height and width dimensions.\\n- compute_output_shape: Computes the output shape of the layer given the input tensor shape.\\n- call: Applies zero padding on the input tensor using Keras backend functions.\\n- get_config: Returns a dictionary containing all configuration parameters for this layer, which can be reloaded later to reconstruct the layer exactly as it was when saved.\\n\\nThe class raises ValueError if the padding is not of an acceptable format (an integer or tuple). \\n\"\"\"',\n",
       " '\"\"\"\\nZeroPadding3D is a custom Layer used in Keras for padding operations in 3D space.\\n\\nThe layer accepts an optional parameter \\'padding\\' which can be specified in one of three ways:\\n1. An integer, in which case the same amount of padding will be applied to all dimensions.\\n2. A tuple of three integers, specifying different amounts of padding for each dimension.\\n3. A tuple of three tuples of two integers, providing specific left and right padding values for each dimension.\\n\\nThe \\'data_format\\' parameter specifies the format of the input tensors. It can be set to either \"channels_first\" or \"channels_last\".\\n\\nThis layer computes the output shape based on the input shape and provided padding parameters. The compute_output_shape() method returns a TensorShape object representing the output tensor\\'s dimensions after padding is applied.\\n\\nThe \\'call()\\' method applies the 3D zero-padding to its input, which it then returns as the output of this layer. This method uses the Keras backend function spatial_3d_padding().\\n\\nFinally, the get_config() method returns a dictionary containing all configurations that define the layer. It includes \\'padding\\' and \\'data_format\\'. The base configuration is obtained by calling superclass Layer\\'s get_config(), then the current configurations are appended to it. \\n\"\"\"',\n",
       " '\"\"\"\\nThis is a Python class named `_BaseEncoder` that encapsulates methods and attributes related to encoding categorical features of a dataset. The class uses scikit-learn\\'s base classes, `TransformerMixin` and `BaseEstimator`. \\n\\nAttributes:\\n    - categories_: List[numpy.ndarray]\\n        A list containing the unique values for each feature in X. It is updated during fit operation.\\n    - n_features_in_: int\\n        The number of features seen during `fit()` call.\\n        \\nMethods:\\n    1. _check_X(self, X, force_all_finite=True) \\n       This method checks if X is a two-dimensional array-like object and validates its values by using numpy\\'s check_array function. It returns the list of columns in X and their shapes respectively.\\n        \\n    2. _fit(self, X, handle_unknown=\\'error\\', force_all_finite=True, return_counts=False, return_and_ignore_missing_for_infrequent=False) \\n       This method performs pre-processing tasks such as checking for infrequent categories and unique values in each column of X. It also handles unknown categories based on the `handle_unknown` parameter. The function returns a dictionary containing information about the number of samples, category counts (if return_counts=True), and missing indices if specified.\\n       \\n    3. _transform(self, X, handle_unknown=\\'error\\', force_all_finite=True, warn_on_unknown=False, ignore_category_indices=None) \\n       This method encodes the categorical data to numerical data based on learned categories from fit operation. It also handles unknown categories during transformation and provides warning if any unknown category is found in a column. The function returns two numpy arrays - an integer encoded array (X_int), and a boolean mask array for checking invalid values after encoding (X_mask).\\n       \\n    4. infrequent_categories_(self) \\n       This property method retrieves the categories of each feature in X which are considered infrequent based on their frequency or number of unique items. The function returns a list containing either None if no such categories exist, or numpy array of infrequent categories.\\n       \\n    5. _check_infrequent_enabled(self) \\n       This method checks whether the transformer is configured to handle and encode infrequent categories based on attributes \\'max_categories\\' and \\'min_frequency\\'. If either attribute has a value greater than or equal to one, it sets \\'_infrequent_enabled\\' as True.\\n       \\n    6. _identify_infrequent(self, category_count, n_samples, col_idx) \\n       This method identifies infrequent categories based on their frequency count and sample size. The function returns a numpy array of indices identifying the infrequent categories for each feature column in X.\\n       \\n    7. _fit_infrequent_category_mapping(self, n_samples, category_counts, missing_indices) \\n       This method creates mappings to replace frequent categories with a default value (the first infrequent level) and also handle cases where the column in X has no known categories. The function updates \\'_infrequent_indices\\' and \\'_default_to_infrequent_mappings\\' attributes.\\n       \\n    8. _map_infrequent_categories(self, X_int, X_mask, ignore_category_indices) \\n       This method applies the infrequent category mappings to the encoded data matrix (X_int). If handle_unknown is set to \\'infrequent_if_exist\\', it also changes the mask for unknown values.\\n       \\n    9. _more_tags(self) \\n       This method provides more tags about the transformer including X_types and allow_nan which specify that the inputs are two-dimensional array-like objects and allow NaNs respectively. It returns a dictionary with these details.\\n\"\"\"']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_docstrings_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "04a5c43f-162e-4d4c-9c1d-febf3f180c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rouge(df, reference_column, hypothesis_column):\n",
    "    rouge = Rouge()\n",
    "\n",
    "    def calculate_score(row):\n",
    "        scores = rouge.get_scores(row[hypothesis_column].lower(), row[reference_column].lower())\n",
    "        return scores[0]['rouge-1']['f']\n",
    "\n",
    "    df['ROUGE-1 ' + reference_column] = df.apply(calculate_score, axis=1)\n",
    "    return df\n",
    "\n",
    "# Calculate ROUGE-1 scores\n",
    "data_1 = calculate_rouge(class_files_df, 'Comments', 'RAG_Docstring')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "31a384a8-c468-4d80-985c-3ceb856ed873",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bleu(df, reference_column, hypothesis_column):\n",
    "    nltk.download('punkt')\n",
    "\n",
    "    def calculate_score(row):\n",
    "        reference = [row[reference_column].lower().split()]\n",
    "        hypothesis = row[hypothesis_column].lower().split()\n",
    "        score = sentence_bleu(reference, hypothesis, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "        return score\n",
    "\n",
    "    df['BLEU Score ' + reference_column] = df.apply(calculate_score, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "15934aa0-b5f4-4571-bebb-dc11f1830cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/balajivenktesh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Calculate BLEU scores\n",
    "data_1 = calculate_bleu(data_1, 'Comments', 'RAG_Docstring')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e8519e23-476f-4a36-aac8-22d98286f626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate BERT encoding score, using cosine similarity\n",
    "def calculate_bert_score(ground_truth, generated):\n",
    "    # Calculate BERT score\n",
    "    _, _, bert_score_f1 = score([ground_truth], [generated], lang='en', model_type='bert-base-uncased')\n",
    "\n",
    "    return bert_score_f1.item()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "25429241-cc26-42b8-b8f7-8e29583c7fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate BLEU scores\n",
    "list_append_1 = []\n",
    "for index, row in data_1.iterrows():\n",
    "    list_append_1.append(calculate_bert_score(str(row[\"Comments\"]), str(row[\"RAG_Docstring\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "06e38e5c-eff2-4410-9e3f-311250416668",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1[\"Accuracy\"] = list_append_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "83a5a23f-8dab-4e0f-af01-acdea1c69b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate number of syllables in docstring\n",
    "def count_syllables(word):\n",
    "    # Remove punctuation\n",
    "    word = re.sub(r'[^a-zA-Z]', '', word)\n",
    "    \n",
    "    # Vowel count\n",
    "    vowels = 'aeiouy'\n",
    "    syllables = 0\n",
    "    last_was_vowel = False\n",
    "    for char in word:\n",
    "        if char.lower() in vowels:\n",
    "            if not last_was_vowel:\n",
    "                syllables += 1\n",
    "            last_was_vowel = True\n",
    "        else:\n",
    "            last_was_vowel = False\n",
    "    \n",
    "    # Adjust syllable count for words ending in 'e'\n",
    "    if word.endswith(('e', 'es', 'ed')):\n",
    "        syllables -= 1\n",
    "    \n",
    "    # Adjust syllable count for words with no vowels\n",
    "    if syllables == 0:\n",
    "        syllables = 1\n",
    "    \n",
    "    return syllables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fb03b310-eae5-476e-9ed2-2e9744297db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Flesch reading score\n",
    "def flesch_reading_ease(text):\n",
    "    sentences = text.count('.') + text.count('!') + text.count('?') + 1\n",
    "    words = len(re.findall(r'\\b\\w+\\b', text))\n",
    "    syllables = sum(count_syllables(word) for word in text.split())\n",
    "    \n",
    "    # Calculate Flesch Reading Ease score\n",
    "    score = 206.835 - 1.015 * (words / sentences) - 84.6 * (syllables / words)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7b0fa517-59f9-4d02-aa35-6bbb06547c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Easy scores\n",
    "list_append_2 = []\n",
    "for index, row in data_1.iterrows():\n",
    "    list_append_2.append(flesch_reading_ease(str(row[\"RAG_Docstring\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0b9a6431-2909-4e46-b710-030f4a9a7819",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1[\"Ease\"] = list_append_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "901650f6-87ce-44b5-b79c-a623bc749a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "def compress(input):\n",
    "\treturn zlib.compress(input.encode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7b324cfb-b2fb-4ca7-8288-3bb67625c785",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conciness(ground_truth, generated):\n",
    "    comp1 = compress(ground_truth)\n",
    "    comp2 = compress(generated)\n",
    "    return sys.getsizeof(comp2) / sys.getsizeof(comp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "645a8f4e-1c03-47f6-b045-edd60ae3e09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Conciseness scores\n",
    "list_append_3 = []\n",
    "for index, row in data_1.iterrows():\n",
    "    list_append_3.append(conciness(str(row[\"Comments\"]), str(row[\"RAG_Docstring\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e78a8d0e-a88a-47c8-a3bb-7290b1e84b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1[\"Conciseness\"] = list_append_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "abf79907-10bb-4ff6-9cfb-9702814a84c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_parameter_coverage(code_str, docstring_str):\n",
    "    \"\"\"\n",
    "    Calculates the proportion of function/method parameters mentioned in the docstring.\n",
    "    Returns a float (0.0 to 1.0) or None if no parameters are found in the code.\n",
    "    \"\"\"        \n",
    "    match = re.search(r\"def\\s+\\w+\\s*\\((.*?)\\):\", code_str)\n",
    "    if not match:\n",
    "        match = re.search(r\"async\\s+def\\s+\\w+\\s*\\((.*?)\\):\", code_str) \n",
    "\n",
    "    if not match:\n",
    "        return None \n",
    "\n",
    "    params_str = match.group(1)\n",
    "    if not params_str.strip(): \n",
    "        return 1.0 \n",
    "\n",
    "    potential_params = [p.strip().split('=')[0].split(':')[0].strip() for p in params_str.split(',')]\n",
    "    actual_params = [p for p in potential_params if p and p not in ('self', 'cls') and not p.startswith('*')]\n",
    "\n",
    "    if not actual_params:\n",
    "        return 1.0 \n",
    "\n",
    "    covered_params = 0\n",
    "    docstring_lower = docstring_str.lower()\n",
    "    for param_name in actual_params:\n",
    "        if re.search(r\"\\b\" + re.escape(param_name.lower()) + r\"\\b\", docstring_lower):\n",
    "            covered_params += 1\n",
    "        elif f\"{param_name.lower()}:\" in docstring_lower or f\"parameter {param_name.lower()}\" in docstring_lower:\n",
    "             covered_params += 1\n",
    "    return covered_params / len(actual_params) if actual_params else 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ae6c4e8d-6092-457e-9e2c-eb2641cfbae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Return Value Coverage Calculation Function ---\n",
    "def calculate_return_coverage(code_str, docstring_str):\n",
    "    \"\"\"\n",
    "    Checks if the docstring mentions a return value if the code seems to have one.\n",
    "    Returns 1 if covered/not applicable, 0 if potentially missing, None on error.\n",
    "    \"\"\"\n",
    "    has_return_statement = False\n",
    "    for line in code_str.splitlines():\n",
    "        stripped_line = line.strip()\n",
    "        if stripped_line.startswith(\"return \") and not stripped_line.endswith(\"return None\") and len(stripped_line) > len(\"return \"):\n",
    "            has_return_statement = True\n",
    "            break\n",
    "    \n",
    "    if not has_return_statement:\n",
    "        return 1.0 \n",
    "\n",
    "    docstring_lower = docstring_str.lower()\n",
    "    return_keywords = [\"return\", \"returns\", \"yield\", \"yields\"] \n",
    "    if any(keyword in docstring_lower for keyword in return_keywords):\n",
    "        return 1.0\n",
    "    else:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "848ead11-1e12-4c0a-8453-1e79389f1e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Basic Faithfulness Metric Function ---\n",
    "def calculate_basic_faithfulness(generated_docstring, retrieved_context_text):\n",
    "    \"\"\"\n",
    "    Calculates a basic faithfulness score based on token overlap.\n",
    "    This is a crude proxy for actual faithfulness.\n",
    "    Returns a float (0.0 to 1.0) or None.\n",
    "    \"\"\"\n",
    "    # Simple tokenization and stopword removal\n",
    "    stop_words = set([\"a\", \"an\", \"the\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"do\", \"does\", \"did\", \"will\", \"would\", \"should\", \"can\", \"could\", \"may\", \"might\", \"must\", \"and\", \"or\", \"but\", \"if\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"to\", \"in\", \"on\", \"this\", \"that\", \"it\", \"its\", \"you\", \"your\", \"i\", \"me\", \"my\", \"he\", \"she\", \"him\", \"her\", \"they\", \"them\", \"their\"])\n",
    "    \n",
    "    try:\n",
    "        gen_tokens = set(token.lower() for token in re.findall(r'\\b\\w+\\b', generated_docstring) if token.lower() not in stop_words)\n",
    "        ctx_tokens = set(token.lower() for token in re.findall(r'\\b\\w+\\b', retrieved_context_text) if token.lower() not in stop_words)\n",
    "    except Exception as e:\n",
    "        print(f\"Error tokenizing for faithfulness: {e}\")\n",
    "        return None\n",
    "\n",
    "    if not gen_tokens: # If generated docstring has no valid tokens after filtering\n",
    "        return 0.0 \n",
    "\n",
    "    overlapping_tokens = gen_tokens.intersection(ctx_tokens)\n",
    "    \n",
    "    return len(overlapping_tokens) / len(gen_tokens) if gen_tokens else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c1cba1cd-e427-4773-8df2-c398d2a21bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_exception_coverage(code_str, docstring_str):\n",
    "    if not all(isinstance(s, str) for s in [code_str, docstring_str]) or not docstring_str.strip() or docstring_str.startswith((\"# ERROR:\", \"# SKIPPED:\")): return None\n",
    "    raised_exceptions = set(re.findall(r\"raise\\s+(\\w+)\", code_str)) # Basic: finds exception names\n",
    "    if not raised_exceptions: return 1.0 # No exceptions to cover\n",
    "    \n",
    "    docstring_lower = docstring_str.lower()\n",
    "    mentions_raises_section = \"raises:\" in docstring_lower\n",
    "    covered_exceptions = 0\n",
    "    for exc_name in raised_exceptions:\n",
    "        if re.search(r\"\\b\" + re.escape(exc_name.lower()) + r\"\\b\", docstring_lower):\n",
    "            covered_exceptions += 1\n",
    "            \n",
    "    # If a \"Raises:\" section exists, it's good, even if not all specific exceptions are named (simple check)\n",
    "    if mentions_raises_section and raised_exceptions: return 1.0 \n",
    "    if not raised_exceptions: return 1.0 # Should have been caught above\n",
    "    return covered_exceptions / len(raised_exceptions) if raised_exceptions else 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "34021471-9dbe-482f-adaf-3944e6c7c896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Adherence to Docstring Conventions (Pydocstyle) ---\n",
    "PYDOCSTYLE_ENABLED = True\n",
    "def check_docstring_adherence_pydocstyle(code_str, generated_docstring_content):\n",
    "    \"\"\"\n",
    "    Checks adherence of a generated docstring to PEP 257 using pydocstyle.\n",
    "    The generated_docstring_content should be the *content* of the docstring,\n",
    "    not including the triple quotes.\n",
    "    Returns:\n",
    "        float: A score from 0.0 to 1.0 (1.0 means no errors, 0.0 means many errors).\n",
    "               Returns None if pydocstyle is not enabled or an error occurs.\n",
    "    \"\"\"\n",
    "    # Sanitize content for embedding within triple quotes\n",
    "    safe_content = generated_docstring_content.replace('\\\\', '\\\\\\\\') # Escape backslashes\n",
    "    safe_content = safe_content.replace('\"\"\"', '\\\\\"\\\\\"\\\\\"') # Escape internal triple-double-quotes\n",
    "    safe_content = safe_content.replace(\"'''\", \"\\\\'\\\\'\\\\'\") # Escape internal triple-single-quotes\n",
    "    \n",
    "    # Prepare the content for insertion, ensuring correct indentation for multi-line docstrings\n",
    "    lines = safe_content.split('\\n')\n",
    "    if len(lines) == 1:\n",
    "        # Single line docstring content, no special indentation needed beyond the initial one\n",
    "        indented_docstring_body = lines[0]\n",
    "    else:\n",
    "        # Multi-line: first line as is, subsequent lines indented with 4 spaces\n",
    "        # This assumes the docstring will be placed with an initial 4-space indent.\n",
    "        indented_docstring_body = lines[0] + '\\n' + '\\n'.join(['    ' + line for line in lines[1:]])\n",
    "\n",
    "\n",
    "    # Construct a minimal, valid Python snippet for pydocstyle\n",
    "    # Try to place the docstring correctly within a class or function if identifiable\n",
    "    code_prefix = \"\"\n",
    "    code_suffix = \"\\n    pass\" # Default suffix\n",
    "\n",
    "    class_match = re.search(r\"^(.*\\bclass\\s+\\w+\\s*\\(?.*\\)?:)\", code_str, re.MULTILINE)\n",
    "    func_match = re.search(r\"^(.*\\b(async\\s+)?def\\s+\\w+\\s*\\(?.*\\)?:)\", code_str, re.MULTILINE)\n",
    "\n",
    "    if class_match:\n",
    "        header = class_match.group(1)\n",
    "        # Find the end of the header line to insert the docstring\n",
    "        code_prefix = code_str[:class_match.end()] + f'\\n    \"\"\"{indented_docstring_body}\"\"\"'\n",
    "        code_suffix = code_str[class_match.end():] # The rest of the original class code\n",
    "        # Ensure there's at least a 'pass' or some body if the original was just a header\n",
    "        if not code_suffix.strip() or code_suffix.strip().startswith(\"#\"):\n",
    "            code_suffix = \"\\n    pass\" + code_suffix \n",
    "        code_for_pydocstyle_check = code_prefix + code_suffix\n",
    "\n",
    "    elif func_match:\n",
    "        header = func_match.group(1)\n",
    "        code_prefix = code_str[:func_match.end()] + f'\\n    \"\"\"{indented_docstring_body}\"\"\"'\n",
    "        code_suffix = code_str[func_match.end():]\n",
    "        if not code_suffix.strip() or code_suffix.strip().startswith(\"#\"):\n",
    "            code_suffix = \"\\n    pass\" + code_suffix\n",
    "        code_for_pydocstyle_check = code_prefix + code_suffix\n",
    "    else:\n",
    "        # Fallback: treat as module-level docstring if no class/def found\n",
    "        # This is less ideal as the original code_str might not be a full module\n",
    "        code_for_pydocstyle_check = f'\"\"\"{generated_docstring_content}\"\"\"\\n{code_str}'\n",
    "\n",
    "\n",
    "    errors_count = 0\n",
    "    filtered_errors_count = 0\n",
    "    tmp_file_path = None \n",
    "    try:\n",
    "        with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False, encoding='utf-8') as tmp_file:\n",
    "            tmp_file.write(code_for_pydocstyle_check)\n",
    "            tmp_file_path = tmp_file.name\n",
    "        \n",
    "        command = ['pydocstyle', tmp_file_path]\n",
    "        process = subprocess.run(command, capture_output=True, text=True, encoding='utf-8')\n",
    "        \n",
    "        output = process.stdout.strip()\n",
    "        print\n",
    "        if output:\n",
    "            all_errors = output.splitlines()\n",
    "            errors_count = len(all_errors)\n",
    "            # Filter out D100 (Missing docstring in public module) as it's an artifact\n",
    "            # and D101, D102, D103 if we are only checking the first docstring.\n",
    "            # For now, just D100 as the dummy structure is a module.\n",
    "            filtered_errors = [err for err in all_errors if not err.strip().endswith(\"D100: Missing docstring in public module\")]\n",
    "            filtered_errors_count = len(filtered_errors)\n",
    "        \n",
    "        if process.stderr:\n",
    "            if \"Cannot parse file\" in process.stderr or \"unexpected EOF while parsing\" in process.stderr or \"invalid syntax\" in process.stderr :\n",
    "                 print(f\"Pydocstyle CRITICAL PARSE ERROR for temp file {tmp_file_path}: {process.stderr}\")\n",
    "                 print(\"--- Content written to temp file that failed parsing: ---\")\n",
    "                 print(code_for_pydocstyle_check)\n",
    "                 print(\"--------------------------------------------------------\")\n",
    "                 return 0.0 # Penalize heavily for parse error\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An exception occurred during pydocstyle check: {str(e)}\")\n",
    "        if tmp_file_path and os.path.exists(tmp_file_path): # Check if tmp_file_path was assigned\n",
    "             try:\n",
    "                with open(tmp_file_path, 'r', encoding='utf-8') as f_err:\n",
    "                    print(f\"Content of temp file '{tmp_file_path}' that caused exception:\\n{f_err.read()}\")\n",
    "             except Exception as read_err:\n",
    "                print(f\"Could not read temp file {tmp_file_path}: {read_err}\")\n",
    "        return None # Error during check\n",
    "    finally:\n",
    "        if tmp_file_path and os.path.exists(tmp_file_path):\n",
    "            os.remove(tmp_file_path)\n",
    "    \n",
    "    # Normalize score based on filtered errors.\n",
    "    # Using 10 as the denominator makes the score less harsh than 5.\n",
    "    return max(0.0, 1.0 - (filtered_errors_count / 10.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9d4b9cff-6302-447e-8928-33894243eb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_coverage_list = []\n",
    "return_coverage_list = []\n",
    "faithfulness_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bce64619-26cb-45c4-a055-16a22790c7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in data_1.iterrows():\n",
    "    param_coverage_list.append(calculate_parameter_coverage(str(row[\"Code_without_comments\"]), str(row[\"RAG_Docstring\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4b45aa37-1334-4319-bd0b-bbbe3fe54b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1[\"Parameter_Coverage\"] = param_coverage_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8f152d36-fb78-4d1a-87dc-132f39839c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in data_1.iterrows():\n",
    "    return_coverage_list.append(calculate_return_coverage(str(row[\"Code_without_comments\"]), str(row[\"RAG_Docstring\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "06188d8d-1776-4c8b-aba8-81ee3d6f1f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1[\"Return_Coverage\"] = return_coverage_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "223cb42b-8b6d-453f-9e5c-40c983ea7cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1[\"Retrieved_Contexts\"] = retrieved_contexts_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bffb9c46-b111-46af-a19d-5ab14287fdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in data_1.iterrows():\n",
    "    faithfulness_list.append(calculate_basic_faithfulness(str(row[\"RAG_Docstring\"]), str(row[\"Retrieved_Contexts\"])))\n",
    "    #faithfulness_list.append(faithfulness_score)\n",
    "#if faithfulness_score is not None: print(f\"    -> Basic Faithfulness: {faithfulness_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a348320f-8aaf-42e1-8885-112694564c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1[\"Faithfulness_Score\"] = faithfulness_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6b3e30da-78b8-4fd1-a7e7-73e0c1407319",
   "metadata": {},
   "outputs": [],
   "source": [
    "pydocstyle_adherence_list_1 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e1896dca-7e12-4eb6-be89-caad6ff10fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "for index, row in data_1.iterrows():\n",
    "    pydocstyle_adherence_list_1.append(check_docstring_adherence_pydocstyle(str(row[\"Code_without_comments\"]), str(row[\"RAG_Docstring\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4796f965-8087-442b-91e0-4163b956abf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1[\"PythonStyle_Adherence\"] = pydocstyle_adherence_list_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d5305b0e-a6d2-4208-9d7e-80683b7ae371",
   "metadata": {},
   "outputs": [],
   "source": [
    "exception_coverage_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "35c0f3fe-21e6-4356-a651-78049e1a935b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in data_1.iterrows():\n",
    "    exception_coverage_list.append(calculate_exception_coverage(str(row[\"Code_without_comments\"]), str(row[\"RAG_Docstring\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0e06a20e-5391-42ce-953d-8b3fc9e7e576",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1[\"Exception_Coverage\"] = exception_coverage_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0c05c71c-fcf4-4022-9202-49b077101993",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1.to_excel('./deepseek/Code_Aware_RAG.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e97522e0-f5f9-402c-acb4-7715554a18c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1.to_pickle('./deepseek/Code_Aware_RAG.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04566966-9239-4ea8-8c3a-b8cd9e2536a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phd_env",
   "language": "python",
   "name": "phd_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
