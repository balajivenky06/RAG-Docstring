{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e016439e-a179-4d9c-a281-f750ae6f382e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pinecone import Pinecone, ServerlessSpec, PodSpec\n",
    "import ollama\n",
    "import os\n",
    "import uuid\n",
    "import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import re\n",
    "import pandas as pd\n",
    "from rouge import Rouge\n",
    "import pandas as pd\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import nltk\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import openpyxl\n",
    "from bert_score import score\n",
    "import itertools\n",
    "import hf_xet\n",
    "import zlib\n",
    "import subprocess\n",
    "import tempfile\n",
    "import traceback\n",
    "\n",
    "\n",
    "from googlesearch import search as google_search_func # Renamed to avoid conflict\n",
    "GOOGLE_SEARCH_ENABLED = True\n",
    "\n",
    "PINECONE_API_KEY = \"pcsk_71bnuL_HGU1YACobTvL5gJNzHsZG1NMNx3RGmz1ohyC7xMiUYoWnuZpEn5SuvWpuTxnuzm\"\n",
    "PINECONE_ENVIRONMENT = \"us-east-1\"\n",
    "\n",
    "INDEX_NAME = \"corrective-rag-docstring\"\n",
    "EMBEDDING_MODEL = 'all-MiniLM-L6-v2' # HuggingFace sentence transformer\n",
    "OLLAMA_GENERATOR_MODEL = 'llama3.1:8b' # Main model for final docstring generation\n",
    "OLLAMA_HELPER_MODEL = 'llama3.2:latest'    # Smaller model for evaluation & rewriting\n",
    "\n",
    "VECTOR_DIMENSION = 384 # Dimension for all-MiniLM-L6-v2\n",
    "METRIC = \"cosine\"\n",
    "CLOUD = \"aws\"\n",
    "REGION = \"us-east-1\"\n",
    "\n",
    "TARGET_URL = [\n",
    "    \"https://peps.python.org/pep-0257/\",\n",
    "    \"https://www.kaggle.com/code/hagzilla/what-are-docstrings\",\n",
    "    \"https://github.com/keleshev/pep257/blob/master/pep257.py\",\n",
    "    \"https://github.com/chadrik/doc484\",\n",
    "    \"https://zerotomastery.io/blog/python-docstring/\",\n",
    "    \"https://google.github.io/styleguide/pyguide.html\",\n",
    "    \"https://www.geeksforgeeks.org/python-docstrings/\",\n",
    "    \"https://pandas.pydata.org/docs/development/contributing_docstring.html\",\n",
    "    \"https://www.coding-guidelines.lftechnology.com/docs/python/docstrings/\",\n",
    "    \"https://realpython.com/python-pep8/\",\n",
    "    \"https://pypi.org/project/AIDocStringGenerator/\",\n",
    "    \"https://www.geeksforgeeks.org/pep-8-coding-style-guide-python/\",\n",
    "    \"https://llego.dev/posts/write-python-docstrings-guide-documenting-functions/\",\n",
    "    \"https://www.datacamp.com/tutorial/pep8-tutorial-python-code\",\n",
    "    \"https://www.programiz.com/python-programming/docstrings\",\n",
    "    \"https://marketplace.visualstudio.com/items?itemName=ShanthoshS.docstring-generator-ext\",\n",
    "    \"https://stackoverflow.com/questions/3898572/what-are-the-most-common-python-docstring-formats\",\n",
    "    \"https://stackoverflow.com/questions/78753860/what-is-the-proper-way-of-including-examples-in-python-docstrings\",\n",
    "    \"https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html\",\n",
    "    \"https://www.dataquest.io/blog/documenting-in-python-with-docstrings/\",\n",
    "    \"https://www.tutorialspoint.com/python/python_docstrings.htm\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3119aad8-d225-4078-ac72-47982554d7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Generator Model: llama3.1:8b\n",
      "    Helper Model (Eval/Rewrite): llama3.2:latest\n"
     ]
    }
   ],
   "source": [
    "# --- Initialize Services ---\n",
    "try:\n",
    "    embedding_model = SentenceTransformer(EMBEDDING_MODEL)\n",
    "    pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "    ollama_client = ollama.Client()\n",
    "    ollama_client.list()\n",
    "    print(f\"    Generator Model: {OLLAMA_GENERATOR_MODEL}\")\n",
    "    print(f\"    Helper Model (Eval/Rewrite): {OLLAMA_HELPER_MODEL}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nERROR: Failed to initialize services.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98b8f497-b3c9-45b7-a320-317c6fd6c54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Pinecone indexes: ['fusion-rag-docstring', 'self-rag-docstring', 'rag-docstring', 'corrective-rag-docstring', 'code-aware-rag-docstring']\n",
      "Connecting to existing index 'corrective-rag-docstring'.\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Initialize Pinecone ---\n",
    "pinecone_index = None\n",
    "if not PINECONE_API_KEY:\n",
    "    exit(1)\n",
    "try:\n",
    "    pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "    existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]\n",
    "    print(f\"Available Pinecone indexes: {existing_indexes}\")\n",
    "\n",
    "    if INDEX_NAME not in existing_indexes:\n",
    "        print(f\"Index '{INDEX_NAME}' not found. Creating new index...\")\n",
    "        pc.create_index(\n",
    "            name=INDEX_NAME, dimension=VECTOR_DIMENSION, metric=METRIC,\n",
    "            spec=ServerlessSpec(cloud=CLOUD, region=REGION)\n",
    "        )\n",
    "        while not pc.describe_index(INDEX_NAME).status[\"ready\"]:\n",
    "            print(f\"Waiting for index '{INDEX_NAME}' to become ready...\")\n",
    "            time.sleep(5)\n",
    "        print(f\"Index '{INDEX_NAME}' created and ready.\")\n",
    "    else:\n",
    "        print(f\"Connecting to existing index '{INDEX_NAME}'.\")\n",
    "\n",
    "    pinecone_index = pc.Index(INDEX_NAME)\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Failed to initialize or connect to Pinecone index '{INDEX_NAME}': {e}\")\n",
    "    exit(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64642081-becd-4f9e-ac02-b591468c432c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load Data into Pinecone (Only if index is empty AND connection succeeded) ---\n",
    "if pinecone_index:\n",
    "    try:\n",
    "        index_stats = pinecone_index.describe_index_stats() # Get stats again to be sure\n",
    "\n",
    "        if index_stats.total_vector_count == 0:\n",
    "            total_docs_loaded = 0\n",
    "            for url in TARGET_URL:\n",
    "                print(f\"\\n -> Processing URL: {url}\")\n",
    "                try:\n",
    "                    response = requests.get(url, timeout=30, headers={'User-Agent': 'Mozilla/5.0'}) # Add User-Agent\n",
    "                    response.raise_for_status() # Check for HTTP errors\n",
    "\n",
    "                    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                    main_content = soup.find('main') or soup.find('article') or soup.find('div', role='main') or soup.find('body')\n",
    "                    page_text = \"\"\n",
    "                    if main_content:\n",
    "                        page_text = main_content.get_text(separator='\\n', strip=True)\n",
    "                    else:\n",
    "                        page_text = soup.get_text(separator='\\n', strip=True) # Fallback\n",
    "\n",
    "                    if not page_text or len(page_text) < 50: # Basic check for meaningful content\n",
    "                        continue # Skip to the next URL\n",
    "\n",
    "                    embedding = embedding_model.encode(page_text).tolist()\n",
    "\n",
    "                    doc_id = str(uuid.uuid4())\n",
    "                    metadata = {\"text\": page_text, \"source\": url} # Store the specific URL as source\n",
    "\n",
    "                    pinecone_index.upsert(vectors=[(doc_id, embedding, metadata)])\n",
    "                    total_docs_loaded += 1\n",
    "                    time.sleep(0.5) # Small delay\n",
    "\n",
    "                except requests.exceptions.RequestException as e:\n",
    "                    continue # Continue with the next URL\n",
    "                except Exception as e:\n",
    "                    #traceback.print_exc()\n",
    "                    continue # Continue with the next URL\n",
    "\n",
    "            if total_docs_loaded > 0:\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                print(\"   Warning: No documents were loaded into the index.\")\n",
    "\n",
    "        else:\n",
    "            # Index already had data, skip loading\n",
    "            pass # No message needed here as stats were printed during connection check\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nERROR during data loading phase: {e}\")\n",
    "        #traceback.print_exc()\n",
    "else:\n",
    "    print(\"\\nSkipping data loading because Pinecone index connection was not established.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd584b4-b306-4bb5-b14a-09df15a09e30",
   "metadata": {},
   "source": [
    "# --- Get User Input ---\n",
    "user_code = (\"\"\"\n",
    "class LabelBinarizer(TransformerMixin, BaseEstimator, auto_wrap_output_keys=None):\n",
    "    _parameter_constraints: dict = {'neg_label': [Integral], 'pos_label': [Integral], 'sparse_output': ['boolean']}\n",
    "\n",
    "    def __init__(self, *, neg_label=0, pos_label=1, sparse_output=False):\n",
    "        self.neg_label = neg_label\n",
    "        self.pos_label = pos_label\n",
    "        self.sparse_output = sparse_output\n",
    "\n",
    "    @_fit_context(prefer_skip_nested_validation=True)\n",
    "    def fit(self, y):\n",
    "        if self.neg_label >= self.pos_label:\n",
    "            raise ValueError(f'neg_label={self.neg_label} must be strictly less than pos_label={self.pos_label}.')\n",
    "        if self.sparse_output and (self.pos_label == 0 or self.neg_label != 0):\n",
    "            raise ValueError(f'Sparse binarization is only supported with non zero pos_label and zero neg_label, got pos_label={self.pos_label} and neg_label={self.neg_label}')\n",
    "        self.y_type_ = type_of_target(y, input_name='y')\n",
    "        if 'multioutput' in self.y_type_:\n",
    "            raise ValueError('Multioutput target data is not supported with label binarization')\n",
    "        if _num_samples(y) == 0:\n",
    "            raise ValueError('y has 0 samples: %r' % y)\n",
    "        self.sparse_input_ = sp.issparse(y)\n",
    "        self.classes_ = unique_labels(y)\n",
    "        return self\n",
    "\n",
    "    def fit_transform(self, y):\n",
    "        return self.fit(y).transform(y)\n",
    "\n",
    "    def transform(self, y):\n",
    "        check_is_fitted(self)\n",
    "        y_is_multilabel = type_of_target(y).startswith('multilabel')\n",
    "        if y_is_multilabel and (not self.y_type_.startswith('multilabel')):\n",
    "            raise ValueError('The object was not fitted with multilabel input.')\n",
    "        return label_binarize(y, classes=self.classes_, pos_label=self.pos_label, neg_label=self.neg_label, sparse_output=self.sparse_output)\n",
    "\n",
    "    def inverse_transform(self, Y, threshold=None):\n",
    "        check_is_fitted(self)\n",
    "        if threshold is None:\n",
    "            threshold = (self.pos_label + self.neg_label) / 2.0\n",
    "        if self.y_type_ == 'multiclass':\n",
    "            y_inv = _inverse_binarize_multiclass(Y, self.classes_)\n",
    "        else:\n",
    "            y_inv = _inverse_binarize_thresholding(Y, self.y_type_, self.classes_, threshold)\n",
    "        if self.sparse_input_:\n",
    "            y_inv = sp.csr_matrix(y_inv)\n",
    "        elif sp.issparse(y_inv):\n",
    "            y_inv = y_inv.toarray()\n",
    "        return y_inv\n",
    "\n",
    "    def _more_tags(self):\n",
    "        return {'X_types': ['1dlabels']}\n",
    "\"\"\")\n",
    "if not user_code.strip():\n",
    "    print(\"No code provided. Exiting.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e78697c5-e83a-4a8a-958f-46432ec4e0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** ADDED: Get separate query for context retrieval ***\n",
    "def context_qry(user_code):\n",
    "    context_query = (f\"\"\"\n",
    "    Provide clear, concise, informative, and accurate docstrings for the given python code following PEP 257 conventions and standards, \n",
    "    to generate the content for a Python docstring based on the provided code snippet and relevant PEP contexts.\n",
    "    \n",
    "    **Instructions:**\n",
    "    1.  Start with a concise summary line explaining the function/method's purpose.\n",
    "    2.  If applicable, add a blank line and then more detailed explanation.\n",
    "    3.  Use the 'Args:' section to describe each parameter, its type, and what it represents.\n",
    "    4.  Use the 'Returns:' section to describe the return value and its type.\n",
    "    5.  Use the 'Raises:' section to list any exceptions explicitly raised by the code.\n",
    "    6.  Adhere strictly to PEP 257 formatting.\n",
    "    7.  Base the docstring primarily on the 'Code Snippet to Document'. Use the 'Relevant Context' for \n",
    "    clarification or examples if needed.\n",
    "    \n",
    "    Also, check relevant content for the user given input code: {user_code}\n",
    "    \"\"\")\n",
    "    return context_query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0d5b133-40b4-49d6-8309-18e517863f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper Function for Rule-Based Grading (Used in Refinement Step Now) ---\n",
    "def grade_chunk_relevance(chunk_text, user_code, context_query):\n",
    "    \"\"\"\n",
    "    Grades a text chunk based on predefined rules (keyword matching).\n",
    "    Returns True if relevant (passes grading), False otherwise.\n",
    "    \"\"\"\n",
    "    if not chunk_text or len(chunk_text.strip()) < 10: # Skip very short/empty chunks\n",
    "        return False\n",
    "\n",
    "    # Normalize text to lower case for matching\n",
    "    chunk_lower = chunk_text.lower()\n",
    "    code_lower = user_code.lower() # Already lower from outer scope if needed\n",
    "    query_lower = context_query.lower() # Already lower from outer scope if needed\n",
    "\n",
    "    # Rule 1: Check for core docstring/code explanation keywords\n",
    "    core_keywords = [\"docstring\", \"parameter\", \"argument\", \"return\", \"yield\", \"attribute\", \"class\", \"function\", \"method\", \"pep 257\", \"summary\", \"description\", \"example\", \"usage\", \"type hint\"]\n",
    "    if any(keyword in chunk_lower for keyword in core_keywords):\n",
    "        return True\n",
    "\n",
    "    # Rule 2: Check for keywords from the user's context query (if provided)\n",
    "    if query_lower:\n",
    "        # Simple word extraction (can be improved)\n",
    "        query_keywords = re.findall(r'\\b\\w{3,}\\b', query_lower)\n",
    "        stop_words = {\"how\", \"what\", \"the\", \"and\", \"for\", \"does\", \"work\", \"python\", \"use\", \"create\"}\n",
    "        query_keywords = [kw for kw in query_keywords if kw not in stop_words]\n",
    "        if query_keywords and any(keyword in chunk_lower for keyword in query_keywords):\n",
    "             return True\n",
    "\n",
    "    # Rule 3: Check if chunk contains code structure relevant to user code\n",
    "    if (\"def \" in chunk_lower and \"def \" in code_lower) or \\\n",
    "       (\"class \" in chunk_lower and \"class \" in code_lower):\n",
    "         return True\n",
    "    # Check for parameter names from user code (if simple function)\n",
    "    param_match = re.search(r'def\\s+\\w+\\s*\\((.*?)\\):', user_code)\n",
    "    if param_match:\n",
    "        params = [p.strip().split('=')[0].strip() for p in param_match.group(1).split(',') if p.strip()]\n",
    "        if any(f\"parameter {p}\" in chunk_lower or f\"{p}:\" in chunk_lower for p in params if p not in ['self', 'cls']):\n",
    "             return True\n",
    "\n",
    "    return False # Failed grading if no rules passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5cd13e03-63cd-4151-970b-7872d8cf3c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_retrieve(context_query, pinecone_index, embedding_model):\n",
    "    retrieved_matches = []\n",
    "    initial_docs_for_refinement = [] # Store raw docs for Step 4\n",
    "    if context_query.strip():\n",
    "        try:\n",
    "            query_embedding = embedding_model.encode(context_query).tolist()\n",
    "            #print(f\" -> Querying Pinecone (top_k=3)...\")\n",
    "            search_results = pinecone_index.query(\n",
    "                vector=query_embedding,\n",
    "                top_k=3,  # Retrieve candidates\n",
    "                include_metadata=True\n",
    "            )\n",
    "            retrieved_matches = search_results.matches\n",
    "            # Keep the raw text and source for later refinement\n",
    "            initial_docs_for_refinement = [{\"text\": m.metadata.get('text',''), \"source\": m.metadata.get('source','N/A')} for m in retrieved_matches if m.metadata.get('text')]\n",
    "            #print(f\" -> Retrieved {len(initial_docs_for_refinement)} candidate documents from Pinecone for refinement.\")\n",
    "        except Exception as e:\n",
    "            print(f\" -> ERROR querying Pinecone: {e}\")\n",
    "            retrieved_matches = []\n",
    "    else: # No context_query provided\n",
    "         print(\" -> No context query provided by user, skipping Pinecone retrieval.\")\n",
    "    return (retrieved_matches, initial_docs_for_refinement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52f28350-b505-49d3-aabf-bf9860118fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper Function for Rule-Based Grading (Used in Refinement Step Now) ---\n",
    "def grade_chunk_relevance(chunk_text, user_code, context_query):\n",
    "    \"\"\"\n",
    "    Grades a text chunk based on predefined rules (keyword matching).\n",
    "    Returns True if relevant (passes grading), False otherwise.\n",
    "    \"\"\"\n",
    "    if not chunk_text or len(chunk_text.strip()) < 10: # Skip very short/empty chunks\n",
    "        return False\n",
    "\n",
    "    # Normalize text to lower case for matching\n",
    "    chunk_lower = chunk_text.lower()\n",
    "    code_lower = user_code.lower() # Already lower from outer scope if needed\n",
    "    query_lower = context_query.lower() # Already lower from outer scope if needed\n",
    "\n",
    "    # Rule 1: Check for core docstring/code explanation keywords\n",
    "    core_keywords = [\"docstring\", \"parameter\", \"argument\", \"return\", \"yield\", \"attribute\", \"class\", \"function\", \"method\", \"pep 257\", \"summary\", \"description\", \"example\", \"usage\", \"type hint\"]\n",
    "    if any(keyword in chunk_lower for keyword in core_keywords):\n",
    "        return True\n",
    "\n",
    "    # Rule 2: Check for keywords from the user's context query (if provided)\n",
    "    if query_lower:\n",
    "        # Simple word extraction (can be improved)\n",
    "        query_keywords = re.findall(r'\\b\\w{3,}\\b', query_lower)\n",
    "        stop_words = {\"how\", \"what\", \"the\", \"and\", \"for\", \"does\", \"work\", \"python\", \"use\", \"create\"}\n",
    "        query_keywords = [kw for kw in query_keywords if kw not in stop_words]\n",
    "        if query_keywords and any(keyword in chunk_lower for keyword in query_keywords):\n",
    "             return True\n",
    "\n",
    "    # Rule 3: Check if chunk contains code structure relevant to user code\n",
    "    if (\"def \" in chunk_lower and \"def \" in code_lower) or \\\n",
    "       (\"class \" in chunk_lower and \"class \" in code_lower):\n",
    "         return True\n",
    "    # Check for parameter names from user code (if simple function)\n",
    "    param_match = re.search(r'def\\s+\\w+\\s*\\((.*?)\\):', user_code)\n",
    "    if param_match:\n",
    "        params = [p.strip().split('=')[0].strip() for p in param_match.group(1).split(',') if p.strip()]\n",
    "        if any(f\"parameter {p}\" in chunk_lower or f\"{p}:\" in chunk_lower for p in params if p not in ['self', 'cls']):\n",
    "             return True\n",
    "\n",
    "    return False # Failed grading if no rules passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f60987a1-983a-4db6-a54d-eb2572590564",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_relevance(retrieved_matches):\n",
    "    docs_passed_initial_grading = [] # Store docs that pass this initial check\n",
    "    if not retrieved_matches:\n",
    "        print(\" -> No documents retrieved from Pinecone to grade.\")\n",
    "    else:\n",
    "        for i, match in enumerate(retrieved_matches):\n",
    "            # (LLM Grading logic remains the same as previous version)\n",
    "            try:\n",
    "                doc_text = match.metadata.get('text', '')\n",
    "                doc_source = match.metadata.get('source', 'N/A')\n",
    "                doc_score = match.score # Pinecone similarity score\n",
    "    \n",
    "                if not doc_text:\n",
    "                    print(f\" -> Skipping empty document {i+1} from {doc_source}\")\n",
    "                    continue\n",
    "                #print(f\" -> Grading document {i+1} (Source: {doc_source.split('/')[-1]}, Score: {doc_score:.3f})...\")\n",
    "                eval_prompt = f\"\"\"\n",
    "                Task: Evaluate if the following Context is relevant for generating a Python docstring for the provided Code.\n",
    "                Answer ONLY with YES or NO.\n",
    "    \n",
    "                Context (from '{doc_source}'): --- {doc_text[:1500]}... ---\n",
    "                Code: --- {user_code} ---\n",
    "                Is the Context relevant for generating a docstring for the Code (YES or NO):\n",
    "                \"\"\"\n",
    "                eval_response = ollama_client.generate( model=OLLAMA_HELPER_MODEL, prompt=eval_prompt, options={'temperature': 0.5} )\n",
    "                answer = eval_response.get('response', '').strip().upper()\n",
    "                #print(f\"    -> Grading result: {answer}\")\n",
    "                if \"YES\" in answer:\n",
    "                    docs_passed_initial_grading.append({\"text\": doc_text, \"source\": doc_source, \"type\": \"pinecone\"})\n",
    "                    #print(docs_passed_initial_grading)\n",
    "            except Exception as e:\n",
    "                print(f\" -> ERROR grading document {i+1} from {doc_source}: {e}\")\n",
    "    \n",
    "    return docs_passed_initial_grading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b62e8461-2bcc-49fa-bdab-4e7e578c2677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 3: Corrective Action (Web Search) ---\n",
    "def web_search(context_query, user_code):\n",
    "    web_context_docs = [] # Store raw web docs for Step 4\n",
    "    if context_query.strip() and GOOGLE_SEARCH_ENABLED:\n",
    "        try:\n",
    "            code_first_line = user_code.split('\\n', 1)[0].strip()\n",
    "            if len(code_first_line) > 50: code_first_line = code_first_line[:50] + \"...\"\n",
    "            web_query = f\"python {context_query} documentation for `{code_first_line}`\"\n",
    "            #print(f\" -> Web search query: '{web_query}'\")\n",
    "            #print(f\" -> Searching Google (asking for num=3 results)...\")\n",
    "            search_urls = list(google_search_func(web_query, lang=\"en\"))\n",
    "    \n",
    "            if search_urls:\n",
    "                #print(f\" -> Found {len(search_urls)} Google Search URLs. Fetching content...\")\n",
    "                for i, url in enumerate(search_urls[:3]):\n",
    "                    #print(f\"    -> Fetching content from URL {i+1}: {url}\")\n",
    "                    try:\n",
    "                        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
    "                        response = requests.get(url, timeout=15, headers=headers, allow_redirects=True)\n",
    "                        response.raise_for_status()\n",
    "                        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                        main_content = soup.find('main') or soup.find('article') or soup.find('body')\n",
    "                        page_text = \"\"\n",
    "                        if main_content: page_text = main_content.get_text(separator='\\n', strip=True)\n",
    "                        else: page_text = soup.get_text(separator='\\n', strip=True)\n",
    "    \n",
    "                        if page_text:\n",
    "                             #print(f\"       -> Storing snippet (length {len(page_text)}) from: {url} for refinement.\")\n",
    "                             max_snippet_length = 4000 # Allow longer snippets before chunking\n",
    "                             web_context_docs.append({\n",
    "                                 \"text\": page_text[:max_snippet_length],\n",
    "                                 \"source\": url,\n",
    "                                 \"type\": \"web\"\n",
    "                             })\n",
    "                        else: print(f\"       -> No text content extracted from: {url}\")\n",
    "                    except requests.exceptions.Timeout: print(f\"       -> Timeout fetching URL: {url}\")\n",
    "                    except requests.exceptions.RequestException as req_err: print(f\"       -> Failed to fetch/process URL {url}: {req_err}\")\n",
    "                    except Exception as parse_err: print(f\"       -> Failed to parse URL {url}: {parse_err}\")\n",
    "                    time.sleep(1.5)\n",
    "            else: print(\" -> Google Search did not return result URLs.\")\n",
    "        except Exception as e:\n",
    "            print(f\" -> ERROR during Google Search / content fetching process: {e}\")\n",
    "            traceback.print_exc()\n",
    "    elif not context_query.strip(): print(\" -> Skipping web search: No context query was provided.\")\n",
    "    elif not GOOGLE_SEARCH_ENABLED: print(\" -> Skipping web search: 'googlesearch-python' is not installed or failed to import.\")\n",
    "    return web_context_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be58de50-7896-45b0-809a-2b4280677ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 4: Knowledge Refinement (Chunking and Re-Grading) ---\n",
    "def check_and_regrade(initial_docs_for_refinement, web_context_docs, user_code, context_query):\n",
    "    #print(\"\\nStep 4: Refining final knowledge base (Chunking & Re-Grading)...\")\n",
    "    relevant_chunks = []\n",
    "    combined_sources_list = [] # Keep track of original sources\n",
    "    \n",
    "    # Combine all potential documents (initial Pinecone + web)\n",
    "    all_potential_docs = initial_docs_for_refinement + web_context_docs\n",
    "    \n",
    "    if not all_potential_docs:\n",
    "        print(\" -> No documents from Pinecone or Web to refine.\")\n",
    "    else:\n",
    "        #print(f\" -> Processing {len(all_potential_docs)} documents for chunking and refinement...\")\n",
    "        for doc in all_potential_docs:\n",
    "            doc_text = doc.get('text', '')\n",
    "            doc_source = doc.get('source', 'N/A')\n",
    "            doc_type = doc.get('type', 'unknown')\n",
    "    \n",
    "            if not doc_text: continue\n",
    "    \n",
    "            # Simple chunking by paragraph (split by double newline)\n",
    "            # More sophisticated chunking (sentence splitting, recursive) could be used here.\n",
    "            chunks = [chunk.strip() for chunk in doc_text.split('\\n\\n') if chunk.strip()]\n",
    "            print(f\"    -> Document from {doc_source.split('/')[-1]} ({doc_type}) yielded {len(chunks)} chunks.\")\n",
    "    \n",
    "            graded_chunk_count = 0\n",
    "            for chunk in chunks:\n",
    "                # Grade each chunk using the rule-based function\n",
    "                if grade_chunk_relevance(chunk, user_code, context_query):\n",
    "                    relevant_chunks.append(chunk)\n",
    "                    graded_chunk_count += 1\n",
    "                    # Keep track of the source for the final description\n",
    "                    source_tag = f\"{doc_type.capitalize()}:{doc_source.split('/')[-1]}\"\n",
    "                    if source_tag not in combined_sources_list:\n",
    "                        combined_sources_list.append(source_tag)\n",
    "    \n",
    "            #print(f\"       -> Kept {graded_chunk_count} relevant chunks from this document.\")\n",
    "    \n",
    "    # Combine the relevant chunks into the final context\n",
    "    if relevant_chunks:\n",
    "        final_context = \"\\n\\n\".join(relevant_chunks) # Join relevant chunks\n",
    "        final_source_description = \" | \".join(combined_sources_list)\n",
    "        #print(f\" -> Final refined context compiled from {len(relevant_chunks)} relevant chunks.\")\n",
    "        print(f\" -> Sources contributing to final context: {final_source_description}\")\n",
    "        # Optional: Limit total context size\n",
    "        # MAX_TOTAL_CONTEXT = 15000\n",
    "        # if len(final_context) > MAX_TOTAL_CONTEXT:\n",
    "        #    print(f\" -> Warning: Truncating final refined context from {len(final_context)} to {MAX_TOTAL_CONTEXT} chars.\")\n",
    "        #    final_context = final_context[:MAX_TOTAL_CONTEXT]\n",
    "    else:\n",
    "        print(\" -> No relevant chunks found after refinement.\")\n",
    "        final_context = \"\" # Ensure empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c3bcd92e-7c62-4058-8638-e57602bc4cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def revised_prompt(ctx, helper_model_name):\n",
    "    context = ctx\n",
    "    OLLAMA_REWRITER_MODEL = helper_model_name\n",
    "    rewritten_request = None\n",
    "    rewriter_prompt = f\"\"\"\n",
    "    You are an helpful assistant that refines prompts. Given the following context from the RAG knowledge base along with python code: {context}, generate an optimized prompt for another AI whose sole task is to create a Python docstring for the code and your output.\n",
    "    The optimized prompt should clearly state the task, and subtly incorporate hints from the context if relevant, without necessarily repeating the entire context.\n",
    "    Focus on creating a self-contained, clear instruction for the next AI.\n",
    "    \n",
    "    Generate only the optimized context prompt text for the docstring generation AI.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        rewriter_response = ollama_client.generate(\n",
    "            model=OLLAMA_REWRITER_MODEL,\n",
    "            prompt=rewriter_prompt,\n",
    "            #options={'temperature': 0.3} # Lower temperature for more focused rewriting\n",
    "        )\n",
    "        rewritten_request = rewriter_response.get('response', '').strip()\n",
    "    except Exception as e:\n",
    "        rewritten_request = rewriter_prompt # Ensure it's None on error\n",
    "    return rewritten_request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e75b5c06-eb72-4548-89e7-25b6b7df8177",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_content_generation(final_context, user_code, OLLAMA_GENERATOR_MODEL):\n",
    "    messages = [\n",
    "    {'role': 'system', 'content': 'You are an expert Python programmer tasked with generating docstrings. You will receive context (if found) in one message, and the code to document in the final message. Use the context only if it is directly relevant to explaining the provided code. Return only the docstring and dont include the given python code in the output'}\n",
    "]\n",
    "\n",
    "    # Add the retrieved context as a separate user message, if it exists\n",
    "    if final_context:\n",
    "        # Include source information in the context message for clarity\n",
    "        messages.append({'role': 'user', 'content': f\"Here is potentially relevant context retrieved from knowledge base \\n---\\n{final_context} for the python\\n{user_code}\\n :\\n---\"})\n",
    "    else:\n",
    "        # Explicitly state if no context was found or provided\n",
    "        messages.append({'role': 'user', 'content': \"No specific context was retrieved or provided for this request.\"})\n",
    "    \n",
    "    # Add the final user message with the code and the explicit request\n",
    "    messages.append({'role': 'user', 'content': f\"Based on any relevant context provided earlier, generate the Python docstring for the following code:\\n```python\\n{user_code}\\n```\\n\\nOutput *only* the complete docstring content itself, starting with triple quotes. Dont include python codes in the output\"})\n",
    "    \n",
    "    try:\n",
    "        # Use ollama.chat instead of ollama.generate\n",
    "        response = ollama_client.chat(\n",
    "            model=OLLAMA_GENERATOR_MODEL,\n",
    "            messages=messages\n",
    "            # You could add options here if needed, e.g., options={'temperature': 0.5}\n",
    "        )\n",
    "    \n",
    "        # Extract the content from the 'message' dictionary in the response\n",
    "        generated_docstring = response.get('message', {}).get('content', '').strip()\n",
    "    \n",
    "        # --- Print Output ---\n",
    "        print(\"\\n--- Generated Docstring ---\")\n",
    "    \n",
    "        # Basic cleaning: Chat models might add explanatory text or markdown fences.\n",
    "        # Try to remove common prefixes/suffixes if the model didn't follow instructions precisely.\n",
    "        if generated_docstring.startswith(\"```python\"):\n",
    "            generated_docstring = generated_docstring[len(\"```python\"):].strip()\n",
    "        elif generated_docstring.startswith(\"```\"):\n",
    "             generated_docstring = generated_docstring[len(\"```\"):].strip()\n",
    "    \n",
    "        if generated_docstring.endswith(\"```\"):\n",
    "            generated_docstring = generated_docstring[:-len(\"```\")].strip()\n",
    "    \n",
    "        #print(\"---------------------------\\n\")\n",
    "        #print(generated_docstring)\n",
    "        #print(\"---------------------------\\n\")\n",
    "    \n",
    "    except KeyError:\n",
    "         print(f\"Error: Unexpected response structure from Ollama chat: {response}\")\n",
    "         traceback.print_exc()\n",
    "    except Exception as e:\n",
    "        print(f\"Error communicating with Ollama chat endpoint: {e}\")\n",
    "    return (generated_docstring)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6b72ef50-6700-4bd5-9b31-79f81f9a36a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_corrective_rag_pipeline(user_code, pinecone_index, emb_model, ollama_llm_client, helper_model_name, generator_model_name):\n",
    "    user_cd = user_code\n",
    "    context_query = context_qry(user_cd)\n",
    "    matches, initial_docs = initial_retrieve(context_query, pinecone_index, embedding_model)\n",
    "    initial_grading = evaluate_relevance(matches)\n",
    "    if not initial_grading:\n",
    "        web_context_docs = web_search(context_query, user_cd)\n",
    "    else:\n",
    "        web_context_docs = initial_grading\n",
    "    retrieved_contexts_list.append(web_context_docs)\n",
    "    final_context = check_and_regrade(initial_docs, web_context_docs, user_cd, context_query)\n",
    "    generated_docstring = final_content_generation(final_context, user_code, OLLAMA_GENERATOR_MODEL)\n",
    "    return(generated_docstring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5ef5860f-2bcb-4a4f-9d73-b88b1d1e4feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run_corrective_rag_pipeline(user_code, pinecone_index, EMBEDDING_MODEL, ollama_client, OLLAMA_GENERATOR_MODEL, OLLAMA_HELPER_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "742d916a-45bc-45dd-aed8-c164a11f183c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_files_df = pd.read_pickle('class_files_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "38542e8c-e623-4bfd-bf5a-842173d44efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = class_files_df[\"Comments\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8ea0257d-0659-4114-a6d7-a9e573df6b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_docstrings_list = []\n",
    "retrieved_contexts_list = []\n",
    "rewritten_contexts_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e10b03c9-d063-4c5a-b16c-4db548603ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 15 chunks.\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 15 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      " -> Sources contributing to final context: Unknown: | Pinecone:\n",
      "\n",
      "--- Generated Docstring ---\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 15 chunks.\n",
      "    -> Document from  (pinecone) yielded 15 chunks.\n",
      " -> Sources contributing to final context: Unknown: | Pinecone:\n",
      "\n",
      "--- Generated Docstring ---\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 15 chunks.\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      " -> Sources contributing to final context: Unknown: | Pinecone:\n",
      "\n",
      "--- Generated Docstring ---\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 15 chunks.\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 15 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      " -> Sources contributing to final context: Unknown: | Pinecone:\n",
      "\n",
      "--- Generated Docstring ---\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 15 chunks.\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 15 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      " -> Sources contributing to final context: Unknown: | Pinecone:\n",
      "\n",
      "--- Generated Docstring ---\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 15 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 15 chunks.\n",
      " -> Sources contributing to final context: Unknown: | Pinecone:\n",
      "\n",
      "--- Generated Docstring ---\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 15 chunks.\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 15 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      " -> Sources contributing to final context: Unknown: | Pinecone:\n",
      "\n",
      "--- Generated Docstring ---\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 15 chunks.\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 15 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      " -> Sources contributing to final context: Unknown: | Pinecone:\n",
      "\n",
      "--- Generated Docstring ---\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 15 chunks.\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 15 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      " -> Sources contributing to final context: Unknown: | Pinecone:\n",
      "\n",
      "--- Generated Docstring ---\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 15 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      " -> Sources contributing to final context: Unknown: | Pinecone:\n",
      "\n",
      "--- Generated Docstring ---\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 15 chunks.\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 15 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      " -> Sources contributing to final context: Unknown: | Pinecone:\n",
      "\n",
      "--- Generated Docstring ---\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 15 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      " -> Sources contributing to final context: Unknown: | Pinecone:\n",
      "\n",
      "--- Generated Docstring ---\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 15 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      " -> Sources contributing to final context: Unknown: | Pinecone:\n",
      "\n",
      "--- Generated Docstring ---\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 15 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 15 chunks.\n",
      " -> Sources contributing to final context: Unknown: | Pinecone:\n",
      "\n",
      "--- Generated Docstring ---\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 15 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 15 chunks.\n",
      " -> Sources contributing to final context: Unknown: | Pinecone:\n",
      "\n",
      "--- Generated Docstring ---\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 15 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 15 chunks.\n",
      " -> Sources contributing to final context: Unknown: | Pinecone:\n",
      "\n",
      "--- Generated Docstring ---\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 15 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 15 chunks.\n",
      " -> Sources contributing to final context: Unknown: | Pinecone:\n",
      "\n",
      "--- Generated Docstring ---\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 15 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 15 chunks.\n",
      " -> Sources contributing to final context: Unknown: | Pinecone:\n",
      "\n",
      "--- Generated Docstring ---\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 15 chunks.\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 15 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      " -> Sources contributing to final context: Unknown: | Pinecone:\n",
      "\n",
      "--- Generated Docstring ---\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 15 chunks.\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      " -> Sources contributing to final context: Unknown: | Pinecone:\n",
      "\n",
      "--- Generated Docstring ---\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 15 chunks.\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      " -> Sources contributing to final context: Unknown: | Pinecone:\n",
      "\n",
      "--- Generated Docstring ---\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 15 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 15 chunks.\n",
      " -> Sources contributing to final context: Unknown: | Pinecone:\n",
      "\n",
      "--- Generated Docstring ---\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 15 chunks.\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      " -> Sources contributing to final context: Unknown: | Pinecone:\n",
      "\n",
      "--- Generated Docstring ---\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 15 chunks.\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      " -> Sources contributing to final context: Unknown: | Pinecone:\n",
      "\n",
      "--- Generated Docstring ---\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 15 chunks.\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 15 chunks.\n",
      " -> Sources contributing to final context: Unknown: | Pinecone:\n",
      "\n",
      "--- Generated Docstring ---\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 15 chunks.\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 15 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      " -> Sources contributing to final context: Unknown: | Pinecone:\n",
      "\n",
      "--- Generated Docstring ---\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 15 chunks.\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 15 chunks.\n",
      " -> Sources contributing to final context: Unknown: | Pinecone:\n",
      "\n",
      "--- Generated Docstring ---\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 15 chunks.\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 15 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      " -> Sources contributing to final context: Unknown: | Pinecone:\n",
      "\n",
      "--- Generated Docstring ---\n",
      "       -> Failed to fetch/process URL https://www.datacamp.com/tutorial/docstrings-python: 403 Client Error: Forbidden for url: https://www.datacamp.com/tutorial/docstrings-python\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 15 chunks.\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (web) yielded 1 chunks.\n",
      "    -> Document from  (web) yielded 1 chunks.\n",
      " -> Sources contributing to final context: Unknown: | Web:\n",
      "\n",
      "--- Generated Docstring ---\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 15 chunks.\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 15 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      " -> Sources contributing to final context: Unknown: | Pinecone:\n",
      "\n",
      "--- Generated Docstring ---\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 15 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      " -> Sources contributing to final context: Unknown: | Pinecone:\n",
      "\n",
      "--- Generated Docstring ---\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 15 chunks.\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 15 chunks.\n",
      " -> Sources contributing to final context: Unknown: | Pinecone:\n",
      "\n",
      "--- Generated Docstring ---\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 15 chunks.\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 15 chunks.\n",
      " -> Sources contributing to final context: Unknown: | Pinecone:\n",
      "\n",
      "--- Generated Docstring ---\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 15 chunks.\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 15 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      " -> Sources contributing to final context: Unknown: | Pinecone:\n",
      "\n",
      "--- Generated Docstring ---\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 15 chunks.\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 15 chunks.\n",
      " -> Sources contributing to final context: Unknown: | Pinecone:\n",
      "\n",
      "--- Generated Docstring ---\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 15 chunks.\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      " -> Sources contributing to final context: Unknown: | Pinecone:\n",
      "\n",
      "--- Generated Docstring ---\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 15 chunks.\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 15 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      " -> Sources contributing to final context: Unknown: | Pinecone:\n",
      "\n",
      "--- Generated Docstring ---\n",
      "       -> Failed to fetch/process URL https://www.datacamp.com/tutorial/docstrings-python: 403 Client Error: Forbidden for url: https://www.datacamp.com/tutorial/docstrings-python\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 15 chunks.\n",
      "    -> Document from  (web) yielded 1 chunks.\n",
      "    -> Document from  (web) yielded 1 chunks.\n",
      " -> Sources contributing to final context: Unknown: | Web:\n",
      "\n",
      "--- Generated Docstring ---\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 15 chunks.\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      " -> Sources contributing to final context: Unknown: | Pinecone:\n",
      "\n",
      "--- Generated Docstring ---\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 15 chunks.\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 15 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      " -> Sources contributing to final context: Unknown: | Pinecone:\n",
      "\n",
      "--- Generated Docstring ---\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 15 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      " -> Sources contributing to final context: Unknown: | Pinecone:\n",
      "\n",
      "--- Generated Docstring ---\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 15 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      " -> Sources contributing to final context: Unknown: | Pinecone:\n",
      "\n",
      "--- Generated Docstring ---\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from items?itemName=ShanthoshS.docstring-generator-ext (unknown) yielded 2 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      " -> Sources contributing to final context: Unknown: | Unknown:items?itemName=ShanthoshS.docstring-generator-ext | Pinecone:\n",
      "\n",
      "--- Generated Docstring ---\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 15 chunks.\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 15 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      " -> Sources contributing to final context: Unknown: | Pinecone:\n",
      "\n",
      "--- Generated Docstring ---\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 15 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 15 chunks.\n",
      " -> Sources contributing to final context: Unknown: | Pinecone:\n",
      "\n",
      "--- Generated Docstring ---\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 15 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      " -> Sources contributing to final context: Unknown: | Pinecone:\n",
      "\n",
      "--- Generated Docstring ---\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 15 chunks.\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 15 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      " -> Sources contributing to final context: Unknown: | Pinecone:\n",
      "\n",
      "--- Generated Docstring ---\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 15 chunks.\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 15 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      " -> Sources contributing to final context: Unknown: | Pinecone:\n",
      "\n",
      "--- Generated Docstring ---\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 15 chunks.\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 15 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      " -> Sources contributing to final context: Unknown: | Pinecone:\n",
      "\n",
      "--- Generated Docstring ---\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 15 chunks.\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 15 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      " -> Sources contributing to final context: Unknown: | Pinecone:\n",
      "\n",
      "--- Generated Docstring ---\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 15 chunks.\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 15 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      " -> Sources contributing to final context: Unknown: | Pinecone:\n",
      "\n",
      "--- Generated Docstring ---\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 15 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 15 chunks.\n",
      " -> Sources contributing to final context: Unknown: | Pinecone:\n",
      "\n",
      "--- Generated Docstring ---\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 15 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 15 chunks.\n",
      " -> Sources contributing to final context: Unknown: | Pinecone:\n",
      "\n",
      "--- Generated Docstring ---\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 15 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 15 chunks.\n",
      " -> Sources contributing to final context: Unknown: | Pinecone:\n",
      "\n",
      "--- Generated Docstring ---\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 15 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 15 chunks.\n",
      " -> Sources contributing to final context: Unknown: | Pinecone:\n",
      "\n",
      "--- Generated Docstring ---\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 15 chunks.\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 15 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      " -> Sources contributing to final context: Unknown: | Pinecone:\n",
      "\n",
      "--- Generated Docstring ---\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 15 chunks.\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 15 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      " -> Sources contributing to final context: Unknown: | Pinecone:\n",
      "\n",
      "--- Generated Docstring ---\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 15 chunks.\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 15 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      " -> Sources contributing to final context: Unknown: | Pinecone:\n",
      "\n",
      "--- Generated Docstring ---\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 15 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 15 chunks.\n",
      " -> Sources contributing to final context: Unknown: | Pinecone:\n",
      "\n",
      "--- Generated Docstring ---\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 15 chunks.\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 15 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      " -> Sources contributing to final context: Unknown: | Pinecone:\n",
      "\n",
      "--- Generated Docstring ---\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 15 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      " -> Sources contributing to final context: Unknown: | Pinecone:\n",
      "\n",
      "--- Generated Docstring ---\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 15 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 15 chunks.\n",
      " -> Sources contributing to final context: Unknown: | Pinecone:\n",
      "\n",
      "--- Generated Docstring ---\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 15 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 15 chunks.\n",
      " -> Sources contributing to final context: Unknown: | Pinecone:\n",
      "\n",
      "--- Generated Docstring ---\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 15 chunks.\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 15 chunks.\n",
      " -> Sources contributing to final context: Unknown: | Pinecone:\n",
      "\n",
      "--- Generated Docstring ---\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 15 chunks.\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      " -> Sources contributing to final context: Unknown: | Pinecone:\n",
      "\n",
      "--- Generated Docstring ---\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 15 chunks.\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 15 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      " -> Sources contributing to final context: Unknown: | Pinecone:\n",
      "\n",
      "--- Generated Docstring ---\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (unknown) yielded 15 chunks.\n",
      "    -> Document from  (unknown) yielded 1 chunks.\n",
      "    -> Document from  (pinecone) yielded 1 chunks.\n",
      " -> Sources contributing to final context: Unknown: | Pinecone:\n",
      "\n",
      "--- Generated Docstring ---\n"
     ]
    }
   ],
   "source": [
    "for i, row in class_files_df.iterrows():\n",
    "    user_code = row[\"Code_without_comments\"]\n",
    "    output = run_corrective_rag_pipeline(user_code, pinecone_index, EMBEDDING_MODEL, ollama_client, OLLAMA_GENERATOR_MODEL, OLLAMA_HELPER_MODEL)\n",
    "    generated_docstrings_list.append(output)\n",
    "class_files_df[\"RAG_Docstring\"] = generated_docstrings_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6ed19855-0bc6-4069-a06a-ef6b1e0bf6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_rag_docstring(docstring_text):\n",
    "    if not isinstance(docstring_text, str):\n",
    "        return docstring_text\n",
    "\n",
    "    if docstring_text.startswith(\"# ERROR:\") or docstring_text.startswith(\"# SKIPPED:\"):\n",
    "        return docstring_text\n",
    "\n",
    "    text = docstring_text.strip()\n",
    "\n",
    "    if text.startswith(\"```python\"):\n",
    "        text = text[len(\"```python\"):].strip()\n",
    "    elif text.startswith(\"```\"):\n",
    "        text = text[len(\"```\"):].strip()\n",
    "    if text.endswith(\"```\"):\n",
    "        text = text[:-len(\"```\")].strip()\n",
    "\n",
    "    content_inside_quotes = None\n",
    "    first_double_quotes = text.find('\"\"\"')\n",
    "    if first_double_quotes != -1:\n",
    "        last_double_quotes = text.rfind('\"\"\"')\n",
    "        if last_double_quotes > first_double_quotes and (last_double_quotes + 3) <= len(text):\n",
    "            content_inside_quotes = text[first_double_quotes + 3 : last_double_quotes].strip()\n",
    "\n",
    "    if content_inside_quotes is None or not content_inside_quotes.strip():\n",
    "        first_single_quotes = text.find(\"'''\")\n",
    "        if first_single_quotes != -1:\n",
    "            last_single_quotes = text.rfind(\"'''\")\n",
    "            if last_single_quotes > first_single_quotes and (last_single_quotes + 3) <= len(text):\n",
    "                content_inside_quotes = text[first_single_quotes + 3 : last_single_quotes].strip()\n",
    "    \n",
    "    if content_inside_quotes is not None and content_inside_quotes.strip():\n",
    "        final_text_to_clean = content_inside_quotes\n",
    "    else:\n",
    "        final_text_to_clean = text\n",
    "        if final_text_to_clean.startswith('\"\"\"') and final_text_to_clean.endswith('\"\"\"') and len(final_text_to_clean) >= 6:\n",
    "            final_text_to_clean = final_text_to_clean[3:-3].strip()\n",
    "        elif final_text_to_clean.startswith(\"'''\") and final_text_to_clean.endswith(\"'''\") and len(final_text_to_clean) >= 6:\n",
    "            final_text_to_clean = final_text_to_clean[3:-3].strip()\n",
    "\n",
    "    final_text_to_clean = re.sub(r\"(?i)^class\\s+\\w+:\\s*\\n?\", \"\", final_text_to_clean).strip()\n",
    "    \n",
    "    return final_text_to_clean\n",
    "\n",
    "class_files_df[\"RAG_Docstring\"] = class_files_df[\"RAG_Docstring\"].astype(str).apply(clean_rag_docstring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bc166b68-8c0b-4a72-a34a-1568e31dfc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#class_files_df.to_excel('corrective_rag.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6dda6b36-3b02-4b93-8890-8c35f41978d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rouge(df, reference_column, hypothesis_column):\n",
    "    rouge = Rouge()\n",
    "\n",
    "    def calculate_score(row):\n",
    "        scores = rouge.get_scores(row[hypothesis_column].lower(), row[reference_column].lower())\n",
    "        return scores[0]['rouge-1']['f']\n",
    "\n",
    "    df['ROUGE-1 ' + reference_column] = df.apply(calculate_score, axis=1)\n",
    "    return df\n",
    "\n",
    "# Calculate ROUGE-1 scores\n",
    "data_1 = calculate_rouge(class_files_df, 'Comments', 'RAG_Docstring')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "169df403-d829-4cf8-86af-de493f3ed8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bleu(df, reference_column, hypothesis_column):\n",
    "    nltk.download('punkt')\n",
    "\n",
    "    def calculate_score(row):\n",
    "        reference = [row[reference_column].lower().split()]\n",
    "        hypothesis = row[hypothesis_column].lower().split()\n",
    "        score = sentence_bleu(reference, hypothesis, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "        return score\n",
    "\n",
    "    df['BLEU Score ' + reference_column] = df.apply(calculate_score, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "42fa7e5d-4d33-4be6-99dc-7e9f22ba3704",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/balajivenktesh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Calculate BLEU scores\n",
    "data_1 = calculate_bleu(data_1, 'Comments', 'RAG_Docstring')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2fae27b7-9059-43d5-b693-0ea5a07b5132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate BERT encoding score, using cosine similarity\n",
    "def calculate_bert_score(ground_truth, generated):\n",
    "    # Calculate BERT score\n",
    "    _, _, bert_score_f1 = score([ground_truth], [generated], lang='en', model_type='bert-base-uncased')\n",
    "\n",
    "    return bert_score_f1.item()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2e90142e-f2b8-4e45-9f7a-4d198458a65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate BLEU scores\n",
    "list_append_1 = []\n",
    "for index, row in data_1.iterrows():\n",
    "    list_append_1.append(calculate_bert_score(str(row[\"Comments\"]), str(row[\"RAG_Docstring\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5837fdeb-4a68-4b25-bb0c-e39266709951",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1[\"Accuracy\"] = list_append_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a5f66d22-3a90-48f4-a31f-90b9472b8419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate number of syllables in docstring\n",
    "def count_syllables(word):\n",
    "    # Remove punctuation\n",
    "    word = re.sub(r'[^a-zA-Z]', '', word)\n",
    "    \n",
    "    # Vowel count\n",
    "    vowels = 'aeiouy'\n",
    "    syllables = 0\n",
    "    last_was_vowel = False\n",
    "    for char in word:\n",
    "        if char.lower() in vowels:\n",
    "            if not last_was_vowel:\n",
    "                syllables += 1\n",
    "            last_was_vowel = True\n",
    "        else:\n",
    "            last_was_vowel = False\n",
    "    \n",
    "    # Adjust syllable count for words ending in 'e'\n",
    "    if word.endswith(('e', 'es', 'ed')):\n",
    "        syllables -= 1\n",
    "    \n",
    "    # Adjust syllable count for words with no vowels\n",
    "    if syllables == 0:\n",
    "        syllables = 1\n",
    "    \n",
    "    return syllables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7b500c6a-d264-4365-be57-23191a9af4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Flesch reading score\n",
    "def flesch_reading_ease(text):\n",
    "    sentences = text.count('.') + text.count('!') + text.count('?') + 1\n",
    "    words = len(re.findall(r'\\b\\w+\\b', text))\n",
    "    syllables = sum(count_syllables(word) for word in text.split())\n",
    "    \n",
    "    # Calculate Flesch Reading Ease score\n",
    "    score = 206.835 - 1.015 * (words / sentences) - 84.6 * (syllables / words)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ad34e394-9078-4c31-878d-37be548e138a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Easy scores\n",
    "list_append_2 = []\n",
    "for index, row in data_1.iterrows():\n",
    "    list_append_2.append(flesch_reading_ease(str(row[\"RAG_Docstring\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0d672939-ca72-45da-98cc-a92a7153408e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1[\"Ease\"] = list_append_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9e55f3d1-2d11-453b-854f-1448964309d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "def compress(input):\n",
    "\treturn zlib.compress(input.encode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b36ec5af-7052-4cea-99c0-0a47ff16010e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conciness(ground_truth, generated):\n",
    "    comp1 = compress(ground_truth)\n",
    "    comp2 = compress(generated)\n",
    "    return sys.getsizeof(comp2) / sys.getsizeof(comp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "14d5c854-9ce7-43b6-ab88-4c9d2aa72bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Conciseness scores\n",
    "list_append_3 = []\n",
    "for index, row in data_1.iterrows():\n",
    "    list_append_3.append(conciness(str(row[\"Comments\"]), str(row[\"RAG_Docstring\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5c36fba3-3842-4101-9424-574bdb030267",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1[\"Conciseness\"] = list_append_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "21c305cf-7daa-4b01-9bc7-47705554765a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_parameter_coverage(code_str, docstring_str):\n",
    "    \"\"\"\n",
    "    Calculates the proportion of function/method parameters mentioned in the docstring.\n",
    "    Returns a float (0.0 to 1.0) or None if no parameters are found in the code.\n",
    "    \"\"\"        \n",
    "    match = re.search(r\"def\\s+\\w+\\s*\\((.*?)\\):\", code_str)\n",
    "    if not match:\n",
    "        match = re.search(r\"async\\s+def\\s+\\w+\\s*\\((.*?)\\):\", code_str) \n",
    "\n",
    "    if not match:\n",
    "        return None \n",
    "\n",
    "    params_str = match.group(1)\n",
    "    if not params_str.strip(): \n",
    "        return 1.0 \n",
    "\n",
    "    potential_params = [p.strip().split('=')[0].split(':')[0].strip() for p in params_str.split(',')]\n",
    "    actual_params = [p for p in potential_params if p and p not in ('self', 'cls') and not p.startswith('*')]\n",
    "\n",
    "    if not actual_params:\n",
    "        return 1.0 \n",
    "\n",
    "    covered_params = 0\n",
    "    docstring_lower = docstring_str.lower()\n",
    "    for param_name in actual_params:\n",
    "        if re.search(r\"\\b\" + re.escape(param_name.lower()) + r\"\\b\", docstring_lower):\n",
    "            covered_params += 1\n",
    "        elif f\"{param_name.lower()}:\" in docstring_lower or f\"parameter {param_name.lower()}\" in docstring_lower:\n",
    "             covered_params += 1\n",
    "    return covered_params / len(actual_params) if actual_params else 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cb27478e-d654-41ed-b5bb-3df3defead15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Return Value Coverage Calculation Function ---\n",
    "def calculate_return_coverage(code_str, docstring_str):\n",
    "    \"\"\"\n",
    "    Checks if the docstring mentions a return value if the code seems to have one.\n",
    "    Returns 1 if covered/not applicable, 0 if potentially missing, None on error.\n",
    "    \"\"\"\n",
    "    has_return_statement = False\n",
    "    for line in code_str.splitlines():\n",
    "        stripped_line = line.strip()\n",
    "        if stripped_line.startswith(\"return \") and not stripped_line.endswith(\"return None\") and len(stripped_line) > len(\"return \"):\n",
    "            has_return_statement = True\n",
    "            break\n",
    "    \n",
    "    if not has_return_statement:\n",
    "        return 1.0 \n",
    "\n",
    "    docstring_lower = docstring_str.lower()\n",
    "    return_keywords = [\"return\", \"returns\", \"yield\", \"yields\"] \n",
    "    if any(keyword in docstring_lower for keyword in return_keywords):\n",
    "        return 1.0\n",
    "    else:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f3d354a9-b441-49a0-926c-916d43fab2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Basic Faithfulness Metric Function ---\n",
    "def calculate_basic_faithfulness(generated_docstring, retrieved_context_text):\n",
    "    \"\"\"\n",
    "    Calculates a basic faithfulness score based on token overlap.\n",
    "    This is a crude proxy for actual faithfulness.\n",
    "    Returns a float (0.0 to 1.0) or None.\n",
    "    \"\"\"\n",
    "    # Simple tokenization and stopword removal\n",
    "    stop_words = set([\"a\", \"an\", \"the\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"do\", \"does\", \"did\", \"will\", \"would\", \"should\", \"can\", \"could\", \"may\", \"might\", \"must\", \"and\", \"or\", \"but\", \"if\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"to\", \"in\", \"on\", \"this\", \"that\", \"it\", \"its\", \"you\", \"your\", \"i\", \"me\", \"my\", \"he\", \"she\", \"him\", \"her\", \"they\", \"them\", \"their\"])\n",
    "    \n",
    "    try:\n",
    "        gen_tokens = set(token.lower() for token in re.findall(r'\\b\\w+\\b', generated_docstring) if token.lower() not in stop_words)\n",
    "        ctx_tokens = set(token.lower() for token in re.findall(r'\\b\\w+\\b', retrieved_context_text) if token.lower() not in stop_words)\n",
    "    except Exception as e:\n",
    "        print(f\"Error tokenizing for faithfulness: {e}\")\n",
    "        return None\n",
    "\n",
    "    if not gen_tokens: # If generated docstring has no valid tokens after filtering\n",
    "        return 0.0 \n",
    "\n",
    "    overlapping_tokens = gen_tokens.intersection(ctx_tokens)\n",
    "    \n",
    "    return len(overlapping_tokens) / len(gen_tokens) if gen_tokens else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e3881065-dcf4-4b68-b938-63a76efbb8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_exception_coverage(code_str, docstring_str):\n",
    "    if not all(isinstance(s, str) for s in [code_str, docstring_str]) or not docstring_str.strip() or docstring_str.startswith((\"# ERROR:\", \"# SKIPPED:\")): return None\n",
    "    raised_exceptions = set(re.findall(r\"raise\\s+(\\w+)\", code_str)) # Basic: finds exception names\n",
    "    if not raised_exceptions: return 1.0 # No exceptions to cover\n",
    "    \n",
    "    docstring_lower = docstring_str.lower()\n",
    "    mentions_raises_section = \"raises:\" in docstring_lower\n",
    "    covered_exceptions = 0\n",
    "    for exc_name in raised_exceptions:\n",
    "        if re.search(r\"\\b\" + re.escape(exc_name.lower()) + r\"\\b\", docstring_lower):\n",
    "            covered_exceptions += 1\n",
    "            \n",
    "    # If a \"Raises:\" section exists, it's good, even if not all specific exceptions are named (simple check)\n",
    "    if mentions_raises_section and raised_exceptions: return 1.0 \n",
    "    if not raised_exceptions: return 1.0 # Should have been caught above\n",
    "    return covered_exceptions / len(raised_exceptions) if raised_exceptions else 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6714c910-5977-4b0e-957b-f04a6cdbda36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Adherence to Docstring Conventions (Pydocstyle) ---\n",
    "PYDOCSTYLE_ENABLED = True\n",
    "def check_docstring_adherence_pydocstyle(code_str, generated_docstring_content):\n",
    "    \"\"\"\n",
    "    Checks adherence of a generated docstring to PEP 257 using pydocstyle.\n",
    "    The generated_docstring_content should be the *content* of the docstring,\n",
    "    not including the triple quotes.\n",
    "    Returns:\n",
    "        float: A score from 0.0 to 1.0 (1.0 means no errors, 0.0 means many errors).\n",
    "               Returns None if pydocstyle is not enabled or an error occurs.\n",
    "    \"\"\"\n",
    "    # Sanitize content for embedding within triple quotes\n",
    "    safe_content = generated_docstring_content.replace('\\\\', '\\\\\\\\') # Escape backslashes\n",
    "    safe_content = safe_content.replace('\"\"\"', '\\\\\"\\\\\"\\\\\"') # Escape internal triple-double-quotes\n",
    "    safe_content = safe_content.replace(\"'''\", \"\\\\'\\\\'\\\\'\") # Escape internal triple-single-quotes\n",
    "    \n",
    "    # Prepare the content for insertion, ensuring correct indentation for multi-line docstrings\n",
    "    lines = safe_content.split('\\n')\n",
    "    if len(lines) == 1:\n",
    "        # Single line docstring content, no special indentation needed beyond the initial one\n",
    "        indented_docstring_body = lines[0]\n",
    "    else:\n",
    "        # Multi-line: first line as is, subsequent lines indented with 4 spaces\n",
    "        # This assumes the docstring will be placed with an initial 4-space indent.\n",
    "        indented_docstring_body = lines[0] + '\\n' + '\\n'.join(['    ' + line for line in lines[1:]])\n",
    "\n",
    "\n",
    "    # Construct a minimal, valid Python snippet for pydocstyle\n",
    "    # Try to place the docstring correctly within a class or function if identifiable\n",
    "    code_prefix = \"\"\n",
    "    code_suffix = \"\\n    pass\" # Default suffix\n",
    "\n",
    "    class_match = re.search(r\"^(.*\\bclass\\s+\\w+\\s*\\(?.*\\)?:)\", code_str, re.MULTILINE)\n",
    "    func_match = re.search(r\"^(.*\\b(async\\s+)?def\\s+\\w+\\s*\\(?.*\\)?:)\", code_str, re.MULTILINE)\n",
    "\n",
    "    if class_match:\n",
    "        header = class_match.group(1)\n",
    "        # Find the end of the header line to insert the docstring\n",
    "        code_prefix = code_str[:class_match.end()] + f'\\n    \"\"\"{indented_docstring_body}\"\"\"'\n",
    "        code_suffix = code_str[class_match.end():] # The rest of the original class code\n",
    "        # Ensure there's at least a 'pass' or some body if the original was just a header\n",
    "        if not code_suffix.strip() or code_suffix.strip().startswith(\"#\"):\n",
    "            code_suffix = \"\\n    pass\" + code_suffix \n",
    "        code_for_pydocstyle_check = code_prefix + code_suffix\n",
    "\n",
    "    elif func_match:\n",
    "        header = func_match.group(1)\n",
    "        code_prefix = code_str[:func_match.end()] + f'\\n    \"\"\"{indented_docstring_body}\"\"\"'\n",
    "        code_suffix = code_str[func_match.end():]\n",
    "        if not code_suffix.strip() or code_suffix.strip().startswith(\"#\"):\n",
    "            code_suffix = \"\\n    pass\" + code_suffix\n",
    "        code_for_pydocstyle_check = code_prefix + code_suffix\n",
    "    else:\n",
    "        # Fallback: treat as module-level docstring if no class/def found\n",
    "        # This is less ideal as the original code_str might not be a full module\n",
    "        code_for_pydocstyle_check = f'\"\"\"{generated_docstring_content}\"\"\"\\n{code_str}'\n",
    "\n",
    "\n",
    "    errors_count = 0\n",
    "    filtered_errors_count = 0\n",
    "    tmp_file_path = None \n",
    "    try:\n",
    "        with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False, encoding='utf-8') as tmp_file:\n",
    "            tmp_file.write(code_for_pydocstyle_check)\n",
    "            tmp_file_path = tmp_file.name\n",
    "        \n",
    "        command = ['pydocstyle', tmp_file_path]\n",
    "        process = subprocess.run(command, capture_output=True, text=True, encoding='utf-8')\n",
    "        \n",
    "        output = process.stdout.strip()\n",
    "        print\n",
    "        if output:\n",
    "            all_errors = output.splitlines()\n",
    "            errors_count = len(all_errors)\n",
    "            # Filter out D100 (Missing docstring in public module) as it's an artifact\n",
    "            # and D101, D102, D103 if we are only checking the first docstring.\n",
    "            # For now, just D100 as the dummy structure is a module.\n",
    "            filtered_errors = [err for err in all_errors if not err.strip().endswith(\"D100: Missing docstring in public module\")]\n",
    "            filtered_errors_count = len(filtered_errors)\n",
    "        \n",
    "        if process.stderr:\n",
    "            if \"Cannot parse file\" in process.stderr or \"unexpected EOF while parsing\" in process.stderr or \"invalid syntax\" in process.stderr :\n",
    "                 print(f\"Pydocstyle CRITICAL PARSE ERROR for temp file {tmp_file_path}: {process.stderr}\")\n",
    "                 print(\"--- Content written to temp file that failed parsing: ---\")\n",
    "                 print(code_for_pydocstyle_check)\n",
    "                 print(\"--------------------------------------------------------\")\n",
    "                 return 0.0 # Penalize heavily for parse error\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An exception occurred during pydocstyle check: {str(e)}\")\n",
    "        if tmp_file_path and os.path.exists(tmp_file_path): # Check if tmp_file_path was assigned\n",
    "             try:\n",
    "                with open(tmp_file_path, 'r', encoding='utf-8') as f_err:\n",
    "                    print(f\"Content of temp file '{tmp_file_path}' that caused exception:\\n{f_err.read()}\")\n",
    "             except Exception as read_err:\n",
    "                print(f\"Could not read temp file {tmp_file_path}: {read_err}\")\n",
    "        return None # Error during check\n",
    "    finally:\n",
    "        if tmp_file_path and os.path.exists(tmp_file_path):\n",
    "            os.remove(tmp_file_path)\n",
    "    \n",
    "    # Normalize score based on filtered errors.\n",
    "    # Using 10 as the denominator makes the score less harsh than 5.\n",
    "    return max(0.0, 1.0 - (filtered_errors_count / 10.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "361a5c45-8add-433f-a73a-e573d943e222",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_coverage_list = []\n",
    "return_coverage_list = []\n",
    "faithfulness_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "38730b15-7e31-48d3-baa4-37c1e5d49928",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in data_1.iterrows():\n",
    "    param_coverage_list.append(calculate_parameter_coverage(str(row[\"Code_without_comments\"]), str(row[\"RAG_Docstring\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4a056fe4-1d8c-4d77-82c6-e5d6b99d9267",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1[\"Parameter_Coverage\"] = param_coverage_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d9c076b4-ff70-4c9b-b851-e4ef6e87355e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in data_1.iterrows():\n",
    "    return_coverage_list.append(calculate_return_coverage(str(row[\"Code_without_comments\"]), str(row[\"RAG_Docstring\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "495fde10-89c2-42d7-b7a1-ad39429f1ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1[\"Return_Coverage\"] = return_coverage_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "32b3d0bd-73f2-4580-988b-98ee6b9a84a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1[\"Retrieved_Contexts\"] = retrieved_contexts_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d1e39353-380b-4bd9-aae6-fa422f8b25f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in data_1.iterrows():\n",
    "    faithfulness_list.append(calculate_basic_faithfulness(str(row[\"RAG_Docstring\"]), str(row[\"Retrieved_Contexts\"])))\n",
    "    #faithfulness_list.append(faithfulness_score)\n",
    "#if faithfulness_score is not None: print(f\"    -> Basic Faithfulness: {faithfulness_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ef0f41d6-2be3-420b-9bbb-e7590c7cc5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1[\"Faithfulness_Score\"] = faithfulness_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f2034aed-51a7-4a4f-9de2-855e3e2cb9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pydocstyle_adherence_list_1 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "fbf02449-45e4-48da-a236-9ef0fac8ee82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "for index, row in data_1.iterrows():\n",
    "    pydocstyle_adherence_list_1.append(check_docstring_adherence_pydocstyle(str(row[\"Code_without_comments\"]), str(row[\"RAG_Docstring\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "31cccc3d-5bb2-4b20-8502-c0fc74561a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1[\"PythonStyle_Adherence\"] = pydocstyle_adherence_list_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8111acbe-b4ea-4216-b775-f7940c5f5822",
   "metadata": {},
   "outputs": [],
   "source": [
    "exception_coverage_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "49c117c4-c41d-49a1-9005-95a77783046b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in data_1.iterrows():\n",
    "    exception_coverage_list.append(calculate_exception_coverage(str(row[\"Code_without_comments\"]), str(row[\"RAG_Docstring\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9d159e10-b4e6-4a43-aa45-7447c644b35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1[\"Exception_Coverage\"] = exception_coverage_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "79935184-78fc-4ee8-90a1-9a1a2dbec2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1.to_excel('./deepseek/Corrective_RAG.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f4aad25f-ad5b-40fc-a9df-b0d31441dbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1.to_pickle('./deepseek/Corrective_RAG.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d7fed0-1b4d-460a-9066-d588f079627a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
